{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-csxeWlR1kE3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# 获取波士顿房价数据集\n",
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "\n",
        "# 数据（作为 pandas 数据帧）\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 选择需要的特征\n",
        "# 波士顿房价数据集包含以下特征：\n",
        "# ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "#  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "# 您可以选择全部特征或部分特征进行实验\n",
        "features_to_use = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
        "                  'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
        "                  'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "# 处理目标变量\n",
        "# 在波士顿房价数据集中，目标变量为 'MEDV'（Median value of owner-occupied homes in $1000's）\n",
        "y = y.astype(float)  # 确保目标变量为浮点数\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X[features_to_use], y.rename('MEDV')], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['MEDV']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xokKgvJ53WN"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# 获取波士顿房价数据集\n",
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "\n",
        "# 数据（作为 pandas 数据帧）\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 选择需要的特征\n",
        "# 波士顿房价数据集包含以下特征：\n",
        "# ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "#  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "# 您可以选择全部特征或部分特征进行实验\n",
        "features_to_use = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
        "                  'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
        "                  'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "# 处理目标变量\n",
        "# 在波士顿房价数据集中，目标变量为 'MEDV'（Median value of owner-occupied homes in $1000's）\n",
        "y = y.astype(float)  # 确保目标变量为浮点数\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X[features_to_use], y.rename('MEDV')], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['MEDV']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "\n",
        "# 定义扩展后的 FuzzyLayer 类，加入属性注意力机制\n",
        "class AttentionFuzzyLayer(nn.Module):\n",
        "    def __init__(self, input_dim, n_rules):\n",
        "        super(AttentionFuzzyLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        # 初始化中心 c 和宽度 sigma\n",
        "        self.c = nn.Parameter(torch.randn(n_rules, input_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(n_rules, input_dim))\n",
        "\n",
        "        # 初始化属性注意力权重\n",
        "        self.attention_weights = nn.Parameter(torch.randn(n_rules, input_dim))\n",
        "\n",
        "        # 初始化属性掩码（1 表示活跃，0 表示被剪除）\n",
        "        self.register_buffer('attribute_mask', torch.ones(n_rules, input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 计算属性注意力权重（使用 sigmoid 激活函数并应用属性掩码）\n",
        "        attention = torch.sigmoid(self.attention_weights) * self.attribute_mask  # (n_rules, input_dim)\n",
        "\n",
        "        # 扩展维度以进行广播\n",
        "        x_expanded = x.unsqueeze(1)  # (batch_size, 1, input_dim)\n",
        "        c_expanded = self.c.unsqueeze(0)  # (1, n_rules, input_dim)\n",
        "        sigma_expanded = self.sigma.unsqueeze(0)  # (1, n_rules, input_dim)\n",
        "\n",
        "        # 确保 sigma 为正数，避免除以零\n",
        "        sigma_expanded = torch.clamp(sigma_expanded, min=1e-3)\n",
        "\n",
        "        # 计算高斯隶属度函数的对数\n",
        "        diff = x_expanded - c_expanded  # (batch_size, n_rules, input_dim)\n",
        "        exponent = -0.5 * ((diff / sigma_expanded) ** 2)\n",
        "\n",
        "        # 使用属性注意力权重并应用属性掩码\n",
        "        exponent_weighted = exponent * attention.unsqueeze(0)  # (batch_size, n_rules, input_dim)\n",
        "\n",
        "        # 对输入属性维度求和\n",
        "        sum_exponent = exponent_weighted.sum(dim=2)  # (batch_size, n_rules)\n",
        "\n",
        "        # 计算规则激活度\n",
        "        phi = torch.exp(sum_exponent)  # (batch_size, n_rules)\n",
        "\n",
        "        return phi, attention\n",
        "\n",
        "# 定义扩展后的 NormalizedLayer 类，加入规则注意力机制\n",
        "class AttentionNormalizedLayer(nn.Module):\n",
        "    def __init__(self, n_rules):\n",
        "        super(AttentionNormalizedLayer, self).__init__()\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        # 初始化规则注意力权重\n",
        "        self.rule_attention_weights = nn.Parameter(torch.ones(n_rules))\n",
        "\n",
        "    def forward(self, phi):\n",
        "        # phi: (batch_size, n_rules)\n",
        "        # 计算规则注意力权重（使用 sigmoid 激活函数）\n",
        "        rule_attention = torch.sigmoid(self.rule_attention_weights)  # (n_rules,)\n",
        "\n",
        "        # 使用规则注意力权重调整规则激活度\n",
        "        phi_weighted = phi * rule_attention.unsqueeze(0)  # (batch_size, n_rules)\n",
        "\n",
        "        # 计算归一化的激活度\n",
        "        phi_sum = phi_weighted.sum(dim=1, keepdim=True) + 1e-8  # 防止除以零\n",
        "        psi = phi_weighted / phi_sum  # (batch_size, n_rules)\n",
        "        return psi, rule_attention\n",
        "\n",
        "# 定义扩展后的 WeightedLayer 类\n",
        "class AttentionWeightedLayer(nn.Module):\n",
        "    def __init__(self, input_dim, n_rules):\n",
        "        super(AttentionWeightedLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        # 初始化后件参数 a，包括偏置项\n",
        "        self.a = nn.Parameter(torch.randn(n_rules, input_dim + 1))\n",
        "\n",
        "    def forward(self, x, psi):\n",
        "        # x: (batch_size, input_dim)\n",
        "        # psi: (batch_size, n_rules)\n",
        "        batch_size = x.size(0)\n",
        "        # 添加偏置项\n",
        "        ones = torch.ones(batch_size, 1).to(x.device)\n",
        "        x_with_bias = torch.cat([ones, x], dim=1)  # (batch_size, input_dim + 1)\n",
        "\n",
        "        # 扩展 x 和 a 的维度以进行广播\n",
        "        x_expanded = x_with_bias.unsqueeze(1)  # (batch_size, 1, input_dim + 1)\n",
        "        a_expanded = self.a.unsqueeze(0)       # (1, n_rules, input_dim + 1)\n",
        "\n",
        "        # 计算每个规则的输出（元素级乘法后在特征维度上求和）\n",
        "        w = (x_expanded * a_expanded).sum(dim=2)  # (batch_size, n_rules)\n",
        "\n",
        "        f = psi * w  # (batch_size, n_rules)\n",
        "        return f\n",
        "\n",
        "# 定义 OutputLayer 类\n",
        "class OutputLayer(nn.Module):\n",
        "    def forward(self, f):\n",
        "        # f: (batch_size, n_rules)\n",
        "        output = f.sum(dim=1)  # (batch_size,)\n",
        "        return output\n",
        "\n",
        "# 定义扩展后的 SOFENN 模型\n",
        "class AttentionDynamicAttributeAndRuleSOFENN(nn.Module):\n",
        "    def __init__(self, input_dim, n_rules, attention_threshold=0.1):\n",
        "        super(AttentionDynamicAttributeAndRuleSOFENN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_rules = n_rules\n",
        "        self.attention_threshold = attention_threshold\n",
        "\n",
        "        self.fuzzy_layer = AttentionFuzzyLayer(input_dim, n_rules)\n",
        "        self.normalized_layer = AttentionNormalizedLayer(n_rules)\n",
        "        self.weighted_layer = AttentionWeightedLayer(input_dim, n_rules)\n",
        "        self.output_layer = OutputLayer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        phi, attention = self.fuzzy_layer(x)\n",
        "        psi, rule_attention = self.normalized_layer(phi)\n",
        "        f = self.weighted_layer(x, psi)\n",
        "        output = self.output_layer(f)\n",
        "        return output, phi, attention, rule_attention\n",
        "\n",
        "    def train_step(self, x, target, optimizer, lambda_attention=1e-7, lambda_rule_attention=1e-8, lambda_diversity=1e-4):\n",
        "        \"\"\"\n",
        "        执行一次训练步骤。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, input_dim)\n",
        "        - target: 目标数据，形状：(batch_size,)\n",
        "        - optimizer: 优化器实例\n",
        "        - lambda_attention: 属性注意力权重的正则化系数\n",
        "        - lambda_rule_attention: 规则注意力权重的正则化系数\n",
        "        - lambda_diversity: 多样性正则化的系数\n",
        "\n",
        "        返回：\n",
        "        - loss.item(): 当前批次的总损失\n",
        "        - output: 模型的输出\n",
        "        \"\"\"\n",
        "        self.train()\n",
        "        optimizer.zero_grad()\n",
        "        output, _, attention, rule_attention = self.forward(x)\n",
        "        # 计算预测损失（均方误差损失）\n",
        "        loss_pred = nn.functional.mse_loss(output, target)\n",
        "\n",
        "        # 添加属性注意力正则化损失（L1 正则化）\n",
        "        loss_attention = lambda_attention * attention.abs().sum()\n",
        "\n",
        "        # 计算规则注意力正则化损失（L1 正则化）\n",
        "        loss_rule_attention = lambda_rule_attention * rule_attention.abs().sum()\n",
        "\n",
        "        # 添加多样性正则化损失（鼓励不同规则的注意力权重不同）\n",
        "        if self.n_rules > 1:\n",
        "            # 计算注意力权重的余弦相似度矩阵\n",
        "            attention_norm = attention / (attention.norm(dim=1, keepdim=True) + 1e-8)\n",
        "            similarity_matrix = torch.matmul(attention_norm, attention_norm.t())\n",
        "            # 计算非对角线的平均相似度\n",
        "            diversity_loss = torch.sum(similarity_matrix) - torch.diag(similarity_matrix).sum()\n",
        "            diversity_loss = diversity_loss / (self.n_rules * (self.n_rules - 1))\n",
        "        else:\n",
        "            diversity_loss = torch.tensor(0.0).to(attention.device)\n",
        "\n",
        "        loss_diversity = lambda_diversity * diversity_loss\n",
        "\n",
        "        # 总损失\n",
        "        loss = loss_pred + loss_attention + loss_rule_attention + loss_diversity\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Loss is NaN. Stopping training.\")\n",
        "            return loss.item(), output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), output\n",
        "\n",
        "    def prune_attributes_per_rule(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01):\n",
        "        \"\"\"\n",
        "        剪除每个规则中注意力权重低于阈值的属性，并冻结其相关参数。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则撤销剪枝。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认为 0.1\n",
        "        - X_val: 验证集特征，形状：(num_val_samples, input_dim)\n",
        "        - y_val: 验证集目标，形状：(num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为 0.01（即 1%）\n",
        "\n",
        "        返回：\n",
        "        - pruned_dict: 字典，键为规则索引，值为被剪除的属性索引列表\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "\n",
        "        # 保存剪枝前的模型状态和验证损失\n",
        "        original_state = copy.deepcopy(self.state_dict())\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            output_before = self.infer(X_val)\n",
        "            loss_before = nn.functional.mse_loss(output_before, y_val)\n",
        "\n",
        "        # 执行剪枝操作\n",
        "        pruned_dict = {}\n",
        "        with torch.no_grad():\n",
        "            attention = torch.sigmoid(self.fuzzy_layer.attention_weights)  # (n_rules, input_dim)\n",
        "\n",
        "            for rule_idx in range(self.n_rules):\n",
        "                if torch.all(self.fuzzy_layer.attribute_mask[rule_idx] == 0):\n",
        "                    continue  # 跳过已被完全剪除的规则\n",
        "\n",
        "                prune_indices = torch.where(\n",
        "                    (attention[rule_idx] < threshold) & (self.fuzzy_layer.attribute_mask[rule_idx] == 1)\n",
        "                )[0].tolist()\n",
        "\n",
        "                if prune_indices:\n",
        "                    # 更新属性掩码\n",
        "                    self.fuzzy_layer.attribute_mask[rule_idx, prune_indices] = 0.0\n",
        "\n",
        "                    # 冻结被剪除属性的相关参数\n",
        "                    self.fuzzy_layer.attention_weights[rule_idx, prune_indices].requires_grad = False\n",
        "                    self.fuzzy_layer.c[rule_idx, prune_indices].requires_grad = False\n",
        "                    self.fuzzy_layer.sigma[rule_idx, prune_indices].requires_grad = False\n",
        "                    # 对 prune_indices 中的每个索引加 1，因为偏置项占用了第一个位置\n",
        "                    prune_indices_plus_one = [idx + 1 for idx in prune_indices]\n",
        "                    self.weighted_layer.a[rule_idx, prune_indices_plus_one].requires_grad = False  # +1 是因为有偏置项\n",
        "\n",
        "                    pruned_dict[rule_idx] = prune_indices\n",
        "\n",
        "        # 检查是否有规则的所有属性都被剪除，并处理\n",
        "        with torch.no_grad():\n",
        "            all_pruned_rules = torch.where(self.fuzzy_layer.attribute_mask.sum(dim=1) == 0)[0].tolist()\n",
        "\n",
        "        if all_pruned_rules:\n",
        "            print(f\"Rules with all attributes pruned: {all_pruned_rules}\")\n",
        "            # 设置这些规则的规则注意力权重为非常低，确保被 prune_rules 剪除\n",
        "            self.normalized_layer.rule_attention_weights.data[all_pruned_rules] = -1e6  # 安全地进行赋值操作\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = self.infer(X_val)\n",
        "            loss_after = nn.functional.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - loss_before) / loss_before\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，撤销剪枝操作\n",
        "            self.load_state_dict(original_state)\n",
        "            print(f\"Pruning was reverted due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned_dict = {}  # 清空剪枝记录\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能没有显著下降，执行规则剪枝\n",
        "            pruned = self.prune_rules(threshold=0.005)\n",
        "            if pruned:\n",
        "                print(f\"Pruned rules after attribute pruning.\")\n",
        "\n",
        "        return pruned_dict\n",
        "\n",
        "    def prune_rules(self, threshold=0.1):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认值为 0.1\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        pruned = False\n",
        "        with torch.no_grad():\n",
        "            # 获取规则注意力权重\n",
        "            rule_attention = torch.sigmoid(self.normalized_layer.rule_attention_weights)\n",
        "            # 找到需要移除的规则索引（rule_attention < threshold）\n",
        "            low_attention_indices = torch.where(rule_attention < threshold)[0]\n",
        "\n",
        "            # 找到需要移除的规则索引（所有属性已被剪除）\n",
        "            no_active_attrs_indices = torch.where(self.fuzzy_layer.attribute_mask.sum(dim=1) == 0)[0]\n",
        "\n",
        "            # 合并需要移除的规则索引\n",
        "            prune_indices = torch.cat([low_attention_indices, no_active_attrs_indices])\n",
        "            prune_indices = torch.unique(prune_indices)\n",
        "\n",
        "            if len(prune_indices) == 0:\n",
        "                # 没有需要移除的规则\n",
        "                return pruned\n",
        "\n",
        "            # 保留的规则索引（rule_attention >= threshold AND 有活跃属性）\n",
        "            keep_indices = torch.where(\n",
        "                (rule_attention >= threshold) & (self.fuzzy_layer.attribute_mask.sum(dim=1) > 0)\n",
        "            )[0]\n",
        "\n",
        "            # 更新模型参数，移除低重要性的规则\n",
        "            self.fuzzy_layer.c = nn.Parameter(self.fuzzy_layer.c.data[keep_indices])\n",
        "            self.fuzzy_layer.sigma = nn.Parameter(self.fuzzy_layer.sigma.data[keep_indices])\n",
        "            self.fuzzy_layer.attention_weights = nn.Parameter(self.fuzzy_layer.attention_weights.data[keep_indices])\n",
        "            self.normalized_layer.rule_attention_weights = nn.Parameter(\n",
        "                self.normalized_layer.rule_attention_weights.data[keep_indices]\n",
        "            )\n",
        "            self.weighted_layer.a = nn.Parameter(self.weighted_layer.a.data[keep_indices])\n",
        "\n",
        "            # 更新属性掩码，移除被剪除规则的掩码行\n",
        "            self.fuzzy_layer.attribute_mask = self.fuzzy_layer.attribute_mask.data[keep_indices].clone()\n",
        "\n",
        "            # 更新规则数量\n",
        "            self.n_rules = len(keep_indices)\n",
        "            self.fuzzy_layer.n_rules = self.n_rules\n",
        "            self.normalized_layer.n_rules = self.n_rules\n",
        "            self.weighted_layer.n_rules = self.n_rules\n",
        "\n",
        "            pruned = True  # 标记为已剪枝\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.fuzzy_layer.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After pruning, attribute_mask has shape {self.fuzzy_layer.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"After pruning, n_rules: {self.n_rules}, attribute_mask shape: {self.fuzzy_layer.attribute_mask.shape}\")\n",
        "\n",
        "        return pruned\n",
        "\n",
        "    def grow_rule(self, X_new):\n",
        "        \"\"\"\n",
        "        添加一个新的规则。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, input_dim)\n",
        "        \"\"\"\n",
        "        # 获取设备和数据类型\n",
        "        device = self.fuzzy_layer.c.device\n",
        "        dtype = self.fuzzy_layer.c.dtype\n",
        "\n",
        "        # 使用 X_new 计算新的规则中心和标准差\n",
        "        new_c = torch.tensor(X_new.mean(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, input_dim)\n",
        "        new_sigma = torch.tensor(X_new.std(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, input_dim)\n",
        "\n",
        "        # 计算现有规则的属性注意力权重的平均值\n",
        "        if self.n_rules > 0:\n",
        "            existing_attention_weights = torch.sigmoid(self.fuzzy_layer.attention_weights).data  # (n_rules, input_dim)\n",
        "            attention_mean = existing_attention_weights.mean(dim=0, keepdim=True)  # (1, input_dim)\n",
        "        else:\n",
        "            attention_mean = torch.ones(1, self.input_dim, dtype=dtype).to(device)  # 初始化为 1\n",
        "\n",
        "        # 将新规则的属性注意力权重初始化为平均值并加入随机扰动\n",
        "        noise = torch.randn_like(attention_mean) * 0.05  # 调整扰动大小以控制多样性\n",
        "        new_attention_weights = (attention_mean + noise).clamp(0, 1).detach()  # 保持在 [0, 1] 范围内\n",
        "\n",
        "        # 将新规则的规则注意力权重初始化为与现有权重的均值 logit 相同，并加入随机扰动\n",
        "        if self.n_rules > 0:\n",
        "            existing_rule_attention_logits = self.normalized_layer.rule_attention_weights.data  # (n_rules,)\n",
        "            rule_attention_mean_logit = existing_rule_attention_logits.mean().unsqueeze(0)  # (1,)\n",
        "            rule_attention_noise = torch.randn_like(rule_attention_mean_logit) * 0.05  # 调整扰动大小\n",
        "            new_rule_attention_weight = (rule_attention_mean_logit + rule_attention_noise).detach()\n",
        "        else:\n",
        "            rule_attention_mean_logit = torch.tensor([0.0], dtype=dtype).to(device)  # 中性 logit\n",
        "            new_rule_attention_weight = rule_attention_mean_logit.clone().detach()  # (1,)\n",
        "\n",
        "        # 初始化后件参数为小的随机值\n",
        "        new_a = torch.randn(1, self.input_dim + 1, dtype=dtype).to(device) * 0.01  # (1, input_dim + 1)\n",
        "\n",
        "        # 将新的参数添加到模型中\n",
        "        self.fuzzy_layer.c = nn.Parameter(torch.cat([self.fuzzy_layer.c.data, new_c], dim=0))  # (n_rules + 1, input_dim)\n",
        "        self.fuzzy_layer.sigma = nn.Parameter(torch.cat([self.fuzzy_layer.sigma.data, new_sigma], dim=0))  # (n_rules + 1, input_dim)\n",
        "        self.fuzzy_layer.attention_weights = nn.Parameter(torch.cat([self.fuzzy_layer.attention_weights.data, new_attention_weights], dim=0))  # (n_rules + 1, input_dim)\n",
        "        self.normalized_layer.rule_attention_weights = nn.Parameter(torch.cat([self.normalized_layer.rule_attention_weights.data, new_rule_attention_weight], dim=0))  # (n_rules + 1,)\n",
        "        self.weighted_layer.a = nn.Parameter(torch.cat([self.weighted_layer.a.data, new_a], dim=0))  # (n_rules + 1, input_dim + 1)\n",
        "\n",
        "        # 更新属性掩码，添加新规则的掩码行\n",
        "        new_attribute_mask = torch.ones(1, self.input_dim, dtype=self.fuzzy_layer.attribute_mask.dtype).to(device)  # (1, input_dim)\n",
        "        self.fuzzy_layer.attribute_mask = torch.cat([self.fuzzy_layer.attribute_mask, new_attribute_mask], dim=0)  # (n_rules + 1, input_dim)\n",
        "\n",
        "        # 更新规则数量\n",
        "        self.n_rules += 1\n",
        "        self.fuzzy_layer.n_rules = self.n_rules\n",
        "        self.normalized_layer.n_rules = self.n_rules\n",
        "        self.weighted_layer.n_rules = self.n_rules\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.fuzzy_layer.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After growing, attribute_mask has shape {self.fuzzy_layer.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"New rule added. Total rules: {self.n_rules}\")\n",
        "\n",
        "    def infer(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        执行推理。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, input_dim)\n",
        "        - targets: 目标数据，形状：(batch_size,)，可选\n",
        "\n",
        "        返回：\n",
        "        - 如果 targets 为 None，返回模型输出。\n",
        "        - 否则，返回模型输出和损失值。\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            output, _, _, _ = self.forward(x)\n",
        "            if targets is None:\n",
        "                return output\n",
        "            else:\n",
        "                loss = nn.functional.mse_loss(output, targets)\n",
        "                return output, loss.item()\n",
        "\n",
        "    def extract_rules(self, scaler_X, scaler_y, feature_names=None):\n",
        "        \"\"\"\n",
        "        提取模型的模糊规则。\n",
        "\n",
        "        参数：\n",
        "        - scaler_X: 输入数据的标准化器\n",
        "        - scaler_y: 输出数据的标准化器\n",
        "        - feature_names: 特征名称列表\n",
        "\n",
        "        返回：\n",
        "        - rules: 包含规则字符串的列表\n",
        "        \"\"\"\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Input {i+1}' for i in range(self.input_dim)]\n",
        "\n",
        "        rules = []\n",
        "        c = self.fuzzy_layer.c.detach().cpu().numpy()  # (n_rules, input_dim)\n",
        "        sigma = self.fuzzy_layer.sigma.detach().cpu().numpy()  # (n_rules, input_dim)\n",
        "        attention_weights = torch.sigmoid(self.fuzzy_layer.attention_weights).detach().cpu().numpy()  # (n_rules, input_dim)\n",
        "        attribute_mask = self.fuzzy_layer.attribute_mask.detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(self.normalized_layer.rule_attention_weights).detach().cpu().numpy()\n",
        "        a = self.weighted_layer.a.detach().cpu().numpy()  # (n_rules, input_dim + 1)\n",
        "\n",
        "        # 反标准化\n",
        "        c_orig = c * scaler_X.scale_.reshape(1, -1) + scaler_X.mean_.reshape(1, -1)\n",
        "        sigma_orig = sigma * scaler_X.scale_.reshape(1, -1)\n",
        "        a_orig = a.copy()\n",
        "        a_orig[:, 1:] = a[:, 1:] / scaler_X.scale_.reshape(1, -1) * scaler_y.scale_[0]\n",
        "        a_orig[:, 0] = scaler_y.scale_[0] * a[:, 0] + scaler_y.mean_[0] - np.sum(\n",
        "            a[:, 1:] * scaler_X.mean_.reshape(1, -1) / scaler_X.scale_.reshape(1, -1) * scaler_y.scale_[0],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        for j in range(self.n_rules):\n",
        "            # 包含规则注意力权重\n",
        "            rule_str = (f\"Rule {j+1} (Rule Attention: {rule_attention_weights[j]:.4f}): IF \")\n",
        "            antecedents = []\n",
        "            for i in range(self.input_dim):\n",
        "                if attribute_mask[j, i] == 0:\n",
        "                    continue  # 忽略被剪枝的属性\n",
        "                attention_value = attention_weights[j, i]\n",
        "                mu = c_orig[j, i]\n",
        "                sigma_i = sigma_orig[j, i]\n",
        "                antecedents.append(\n",
        "                    f\"[{feature_names[i]} (Attn: {attention_value:.4f}) is Gaussian(c={mu:.4f}, σ={sigma_i:.4f})]\"\n",
        "                )\n",
        "            antecedent_str = \" AND \".join(antecedents) if antecedents else \"True\"\n",
        "            rule_str += antecedent_str + \" THEN Output = \"\n",
        "\n",
        "            # 构建后件部分\n",
        "            a_j = a_orig[j, :]  # (input_dim + 1,)\n",
        "            consequent_terms = [f\"{a_j[0]:.4f}\"]\n",
        "            for idx, coef in enumerate(a_j[1:]):\n",
        "                if attribute_mask[j, idx] == 0:\n",
        "                    continue  # 忽略被剪枝的属性\n",
        "                attention_value = attention_weights[j, idx]\n",
        "                if coef >= 0:\n",
        "                    term = f\"+ {coef:.4f} * {feature_names[idx]} (Attn: {attention_value:.4f})\"\n",
        "                else:\n",
        "                    term = f\"- {abs(coef):.4f} * {feature_names[idx]} (Attn: {attention_value:.4f})\"\n",
        "                consequent_terms.append(term)\n",
        "            consequent_str = \" \".join(consequent_terms)\n",
        "            rule_str += consequent_str\n",
        "            rules.append(rule_str)\n",
        "        return rules\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "from scipy.stats import norm  # 确保导入了 norm 函数\n",
        "\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    \"\"\"\n",
        "    使用解析解计算两个高斯隶属度函数的重叠面积。\n",
        "\n",
        "    参数：\n",
        "    - c1, sigma1: 第一个高斯函数的中心和标准差\n",
        "    - c2, sigma2: 第二个高斯函数的中心和标准差\n",
        "\n",
        "    返回：\n",
        "    - overlap_area: 两个高斯函数的重叠面积\n",
        "    \"\"\"\n",
        "    denominator = np.sqrt(sigma1**2 + sigma2**2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(model):\n",
        "    \"\"\"\n",
        "    计算 Average Overlap Index (Iov)。\n",
        "\n",
        "    参数：\n",
        "    - model: 训练好的 SOFENN 模型\n",
        "\n",
        "    返回：\n",
        "    - average_iov: 平均重叠指数\n",
        "    \"\"\"\n",
        "    c = model.fuzzy_layer.c.detach().cpu().numpy()       # (n_rules, input_dim)\n",
        "    sigma = model.fuzzy_layer.sigma.detach().cpu().numpy() # (n_rules, input_dim)\n",
        "    attribute_mask = model.fuzzy_layer.attribute_mask.detach().cpu().numpy() # (n_rules, input_dim)\n",
        "    n_rules, input_dim = c.shape\n",
        "\n",
        "    total_max_overlap = 0\n",
        "    valid_attributes = 0\n",
        "\n",
        "    for attr in range(input_dim):\n",
        "        # 获取当前属性的活跃规则\n",
        "        active_rules = np.where(attribute_mask[:, attr] == 1)[0]\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算重叠\n",
        "\n",
        "        max_overlap = -np.inf\n",
        "        for i in range(len(active_rules)):\n",
        "            for j in range(i+1, len(active_rules)):\n",
        "                rule_i = active_rules[i]\n",
        "                rule_j = active_rules[j]\n",
        "                c1 = c[rule_i, attr]\n",
        "                sigma1 = sigma[rule_i, attr]\n",
        "                c2 = c[rule_j, attr]\n",
        "                sigma2 = sigma[rule_j, attr]\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "        if max_overlap != -np.inf:\n",
        "            total_max_overlap += max_overlap\n",
        "            valid_attributes += 1\n",
        "\n",
        "    if valid_attributes == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_iov = total_max_overlap / valid_attributes\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(model):\n",
        "    \"\"\"\n",
        "    计算 Average Fuzzy Set Position Index (Ifspe)。\n",
        "\n",
        "    参数：\n",
        "    - model: 训练好的 SOFENN 模型\n",
        "\n",
        "    返回：\n",
        "    - average_ifspe: 平均模糊集位置指数\n",
        "    \"\"\"\n",
        "    c = model.fuzzy_layer.c.detach().cpu().numpy()       # (n_rules, input_dim)\n",
        "    sigma = model.fuzzy_layer.sigma.detach().cpu().numpy() # (n_rules, input_dim)\n",
        "    attribute_mask = model.fuzzy_layer.attribute_mask.detach().cpu().numpy() # (n_rules, input_dim)\n",
        "    n_rules, input_dim = c.shape\n",
        "\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for attr in range(input_dim):\n",
        "        # 获取当前属性的活跃规则\n",
        "        active_rules = np.where(attribute_mask[:, attr] == 1)[0]\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算 Ifspe\n",
        "\n",
        "        # 按中心值排序\n",
        "        sorted_indices = active_rules[np.argsort(c[active_rules, attr])]\n",
        "        sorted_centers = c[sorted_indices, attr]\n",
        "        sorted_sigma = sigma[sorted_indices, attr]\n",
        "\n",
        "        # 计算相邻规则对的 phi 和 psi\n",
        "        for l in range(len(sorted_centers) - 1):\n",
        "            v_l = sorted_centers[l]\n",
        "            v_lp1 = sorted_centers[l + 1]\n",
        "            s_l = sorted_sigma[l]\n",
        "            s_lp1 = sorted_sigma[l + 1]\n",
        "\n",
        "            phi = np.exp(-0.5 * ((v_l + v_lp1) / (s_l + s_lp1))**2)\n",
        "            denominator = s_l - s_lp1\n",
        "            if denominator == 0:\n",
        "                psi = 0\n",
        "            else:\n",
        "                psi = np.exp(-0.5 * ((v_l + v_lp1) / denominator)**2)\n",
        "\n",
        "            # 使用绝对值确保 Ifspe_term 为非负数\n",
        "            ifspe_term = 2 * abs(0.5 - phi) + psi\n",
        "\n",
        "            total_ifspe += ifspe_term\n",
        "            valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_ifspe = total_ifspe / (n_rules * input_dim)\n",
        "    return average_ifspe\n",
        "\n",
        "# 定义训练函数\n",
        "def train_attention_dynamic_attribute_and_rule_sofenn(\n",
        "    X_train_np, y_train_np, X_val_np, y_val_np,\n",
        "    initial_n_rules=3, epochs=1500, batch_size=32, lr=0.01,\n",
        "    prune_frequency=190, prune_threshold=0.1,\n",
        "    best_model_path='best_sofenn_model.pth'\n",
        "):\n",
        "    \"\"\"\n",
        "    训练 AttentionDynamicAttributeAndRuleSOFENN 模型。\n",
        "    并在训练过程中保存验证集上表现最好的模型。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集特征，形状为 (num_samples, input_dim)\n",
        "    - y_train_np: 训练集目标，形状为 (num_samples,)\n",
        "    - X_val_np: 验证集特征，形状为 (num_val_samples, input_dim)\n",
        "    - y_val_np: 验证集目标，形状为 (num_val_samples,)\n",
        "    - initial_n_rules: 初始规则数量，默认值为 3\n",
        "    - epochs: 训练轮数，默认值为 1500\n",
        "    - batch_size: 每批次的样本数量，默认值为 32\n",
        "    - lr: 学习率，默认值为 0.01\n",
        "    - prune_frequency: 进行属性剪枝的频率（每隔多少个 epoch）\n",
        "    - prune_threshold: 属性剪枝的阈值\n",
        "    - best_model_path: 最佳模型保存的文件路径，默认值为 'best_sofenn_model.pth'\n",
        "\n",
        "    返回：\n",
        "    - model: 训练好的 SOFENN 模型（加载了最佳模型状态）\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - scaler_y: 输出数据的标准化器\n",
        "    \"\"\"\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    input_dim = X_train_scaled_tensor.shape[1]\n",
        "    model = AttentionDynamicAttributeAndRuleSOFENN(\n",
        "        input_dim=input_dim,\n",
        "        n_rules=initial_n_rules,\n",
        "        attention_threshold=prune_threshold\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器和调度器\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'active_rules': [],\n",
        "        'total_rules': [],\n",
        "        'val_rmse': [],\n",
        "        'attribute_weights': [],\n",
        "        'rule_attention_weights': [],\n",
        "        'pruned_attributes': [],\n",
        "        'total_active_attributes': []  # 新增\n",
        "    }\n",
        "\n",
        "    # 初始化变量以记录最佳验证损失和最佳模型状态\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # 设置规则生长和剪枝的参数\n",
        "    patience = 25  # 等待多少个 epoch 后触发规则生长\n",
        "    grow_threshold = 0.0001  # 训练损失下降低于该阈值，触发规则生长\n",
        "    no_improve_epochs = 0\n",
        "    prev_val_loss = float('inf')\n",
        "\n",
        "    max_rules = 3  # 设置规则数量上限，防止无限生长\n",
        "    attention_threshold = 0.05  # 定义活跃规则的注意力权重阈值\n",
        "\n",
        "    # 设置特征名称\n",
        "    feature_labels = features_to_use  # 请根据您的数据集调整\n",
        "\n",
        "    # 创建结果保存的目录\n",
        "    os.makedirs('results_sofenn', exist_ok=True)\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            loss_train, _ = model.train_step(\n",
        "                batch_x,\n",
        "                batch_y,\n",
        "                optimizer,\n",
        "                lambda_attention=1e-7,\n",
        "                lambda_rule_attention=1e-8,\n",
        "                lambda_diversity=1e-4\n",
        "            )\n",
        "            epoch_loss += loss_train\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, phi, attention, rule_attention = model.forward(X_val_scaled_tensor)\n",
        "            loss_val = nn.functional.mse_loss(output, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(output.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "\n",
        "        # 调整学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 计算当前活跃规则的数量\n",
        "        rule_attention_np = torch.sigmoid(model.normalized_layer.rule_attention_weights).detach().cpu().numpy()\n",
        "        num_active_rules = np.sum(rule_attention_np >= attention_threshold)\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val.item())\n",
        "        training_info['active_rules'].append(num_active_rules)\n",
        "        training_info['total_rules'].append(model.n_rules)\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "\n",
        "        # 提取注意力权重\n",
        "        attention_weights = torch.sigmoid(model.fuzzy_layer.attention_weights).detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(model.normalized_layer.rule_attention_weights).detach().cpu().numpy()\n",
        "\n",
        "        # 计算平均属性权重\n",
        "        avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "\n",
        "        # 保存注意力权重\n",
        "        training_info['attribute_weights'].append(avg_attribute_weights)\n",
        "        training_info['rule_attention_weights'].append(rule_attention_weights)\n",
        "\n",
        "        # 计算当前所有规则中激活的属性总数\n",
        "        attribute_mask = model.fuzzy_layer.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask)\n",
        "\n",
        "        # 保存激活的属性总数\n",
        "        training_info['total_active_attributes'].append(total_active_attributes)\n",
        "\n",
        "        # 检查是否为最佳验证损失\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Epoch {epoch+1}: New best validation loss: {loss_val.item():.4f}. Model saved.\")\n",
        "\n",
        "        # 显示注意力权重信息\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {loss_val.item():.4f} - Val RMSE: {val_rmse:.4f} - Total Rules: {model.n_rules} - Active Rules: {num_active_rules}\")\n",
        "\n",
        "        # 检查验证损失的改进情况\n",
        "        if loss_val.item() < prev_val_loss - grow_threshold:\n",
        "            no_improve_epochs = 0\n",
        "            prev_val_loss = loss_val.item()\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # 如果验证损失在连续若干个 epoch 中没有显著改进，触发规则生长\n",
        "        if no_improve_epochs >= patience and model.n_rules < max_rules:\n",
        "            print(f\"Epoch {epoch+1}: No significant improvement in validation loss, growing a new rule. Current rules: {model.n_rules}\")\n",
        "            # 找出当前误差较大的数据点，用于初始化新规则\n",
        "            residuals = (y_val_scaled_tensor.cpu().numpy() - output.cpu().numpy())\n",
        "            high_error_indices = np.argsort(np.abs(residuals))[-int(0.1 * len(residuals)):]  # 选取误差最大的 10% 数据\n",
        "            X_new_rule = X_val_np[high_error_indices]\n",
        "            # 添加新规则\n",
        "            model.grow_rule(X_new_rule)\n",
        "            print(f\"Epoch {epoch+1}: Added new rule. Total rules: {model.n_rules}\")\n",
        "            no_improve_epochs = 0  # 重置计数器\n",
        "\n",
        "            # 重新初始化优化器和调度器\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "            print(\"Optimizer and scheduler re-initialized after growing a new rule.\")\n",
        "\n",
        "        # 设置剪枝停止的 epoch 阈值\n",
        "        pruning_stop_epoch = int(epochs * 0.95)  # 在 80% 的训练过程中进行剪枝\n",
        "\n",
        "        # 每隔 prune_frequency 个 epoch 进行属性剪枝\n",
        "        if (epoch + 1) % prune_frequency == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned_dict = model.prune_attributes_per_rule(\n",
        "                threshold=prune_threshold,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01  # 性能下降容忍度，可根据需要调整\n",
        "            )\n",
        "            training_info['pruned_attributes'].append(pruned_dict)\n",
        "            if pruned_dict:\n",
        "                print(f\"Epoch {epoch+1}: Pruned attributes per rule: {pruned_dict}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch:\n",
        "            pass  # 不再进行剪枝操作\n",
        "\n",
        "        # 每隔若干个 epoch 进行规则剪枝\n",
        "        if (epoch + 1) % 50 == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned = model.prune_rules(threshold=attention_threshold)\n",
        "            if pruned:\n",
        "                print(f\"Epoch {epoch+1}: Pruned rules. Total rules: {model.n_rules}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch:\n",
        "            pass  # 不再进行规则剪枝\n",
        "\n",
        "    # 在训练结束后，加载最佳模型的状态字典\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(\"Loaded the best model based on validation loss.\")\n",
        "    else:\n",
        "        print(\"No improvement during training. Using the final model.\")\n",
        "\n",
        "    # 返回最佳模型和标准化器\n",
        "    return model, scaler_X, scaler_y\n",
        "\n",
        "# 现在，我们可以使用这个扩展后的 SOFENN 模型进行训练和测试\n",
        "# 定义实验参数\n",
        "initial_n_rules = 3\n",
        "learning_rate = 0.01\n",
        "epochs = 1500\n",
        "batch_size = 512\n",
        "prune_frequency = 25\n",
        "prune_threshold = 0.25\n",
        "repeats = 5  # 重复次数\n",
        "\n",
        "# 记录实验结果\n",
        "results_sofenn = []\n",
        "test_rmse_list = []\n",
        "time_list = []\n",
        "for repeat in range(repeats):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 进一步将训练集拆分为训练和验证集\n",
        "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "        X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "    )\n",
        "\n",
        "    # 定义最佳模型保存路径\n",
        "    best_model_path = f'results_sofenn/best_sofenn_model_repeat{repeat+1}.pth'\n",
        "\n",
        "    # 训练模型\n",
        "    sofenn_model, scaler_X, scaler_y = train_attention_dynamic_attribute_and_rule_sofenn(\n",
        "        X_train_sub, y_train_sub, X_test_np, y_test_np,\n",
        "        initial_n_rules=initial_n_rules,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        lr=learning_rate,\n",
        "        prune_frequency=prune_frequency,\n",
        "        prune_threshold=prune_threshold,\n",
        "        best_model_path=best_model_path\n",
        "    )\n",
        "\n",
        "    # 在测试集上测试模型\n",
        "    sofenn_model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        y_pred_scaled = sofenn_model.infer(X_test_tensor)\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "        y_true = y_test_np  # 使用原始的 y_test_np\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        test_rmse_list.append(test_rmse)\n",
        "    # 记录时间\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    time_list.append(time_taken)\n",
        "    print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "    # 提取模糊规则\n",
        "    rules = sofenn_model.extract_rules(scaler_X, scaler_y, feature_names=features_to_use)\n",
        "    # 打印模糊规则\n",
        "    print(f\"\\nFuzzy Rules for Repeat={repeat+1}:\")\n",
        "    for rule in rules:\n",
        "        print(rule)\n",
        "        print()\n",
        "\n",
        "    # # 保存规则到文件\n",
        "    # with open(f'results_sofenn/rules_repeat{repeat+1}.txt', 'w') as f:\n",
        "    #     for rule in rules:\n",
        "    #         f.write(rule + '\\n')\n",
        "\n",
        "    # 保存模型\n",
        "    torch.save(sofenn_model.state_dict(), f'results_sofenn/sofenn_model_repeat{repeat+1}.pth')\n",
        "\n",
        "    # 计算 Average Overlap Index (Iov) 和 Average Fuzzy Set Position Index (Ifspe) # 新增\n",
        "    average_iov = compute_iov(sofenn_model)\n",
        "    average_ifspe = compute_ifspe(sofenn_model)\n",
        "    print(f\"Repeat {repeat+1}: Average Overlap Index (Iov)={average_iov:.4f}, Average Fuzzy Set Position Index (Ifspe)={average_ifspe:.4f}\")  # 新增\n",
        "\n",
        "    # 保存结果\n",
        "    result = {\n",
        "        'repeat': repeat + 1,\n",
        "        'test_rmse': test_rmse,\n",
        "        'time_taken': time_taken,\n",
        "        'total_active_attributes': np.sum(sofenn_model.fuzzy_layer.attribute_mask.detach().cpu().numpy()),\n",
        "        'average_iov': average_iov,  # 新增\n",
        "        'average_ifspe': average_ifspe  # 新增\n",
        "    }\n",
        "    results_sofenn.append(result)\n",
        "\n",
        "# 打印所有实验的结果\n",
        "for res in results_sofenn:\n",
        "    print(f\"Repeat {res['repeat']}: Test RMSE={res['test_rmse']:.4f}, Time={res['time_taken']:.2f}s, Total Active Attributes={res['total_active_attributes']}, Average Iov={res['average_iov']:.4f}, Average Ifspe={res['average_ifspe']:.4f}\")  # 修改\n",
        "\n",
        "# 计算平均 RMSE 和时间\n",
        "test_rmse_mean = np.mean(test_rmse_list)\n",
        "test_rmse_std = np.std(test_rmse_list)\n",
        "time_mean = np.mean(time_list)\n",
        "time_std = np.std(time_list)\n",
        "\n",
        "# 计算平均 Iov 和 Ifspe # 新增\n",
        "average_iov_list = [res['average_iov'] for res in results_sofenn]\n",
        "average_ifspe_list = [res['average_ifspe'] for res in results_sofenn]\n",
        "average_iov_mean = np.mean(average_iov_list)\n",
        "average_iov_std = np.std(average_iov_list)\n",
        "average_ifspe_mean = np.mean(average_ifspe_list)\n",
        "average_ifspe_std = np.std(average_ifspe_list)\n",
        "\n",
        "# 打印结果\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "print(f\"Average Overlap Index (Iov): {average_iov_mean:.4f} ± {average_iov_std:.4f}\")  # 新增\n",
        "print(f\"Average Fuzzy Set Position Index (Ifspe): {average_ifspe_mean:.4f} ± {average_ifspe_std:.4f}\")  # 新增\n",
        "\n",
        "# 计算平均的总属性数量\n",
        "total_attributes_list = [res['total_active_attributes'] for res in results_sofenn]\n",
        "average_total_attributes = np.mean(total_attributes_list)\n",
        "print(f\"\\nAverage Total Active Attributes over {repeats} repeats: {average_total_attributes:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZNZEutkxkTD"
      },
      "source": [
        "## ADAR-SOFENN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08OTVst-alqx"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip'\n",
        "\n",
        "# 定义列名称\n",
        "col_names = ['Year'] + [f'Feature_{i}' for i in range(1, 91)]\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(data_url, header=None, names=col_names)\n",
        "\n",
        "# 特征选择\n",
        "X = data.drop('Year', axis=1)\n",
        "y = data['Year']\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = X.columns.tolist()\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "features_to_use=feature_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a3VyLSpY1dF0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import os\n",
        "import time\n",
        "from scipy.stats import norm  # 新增\n",
        "\n",
        "def reinitialize_optimizer(model, old_optimizer, lr=0.01):\n",
        "    \"\"\"\n",
        "    重新初始化优化器，同时尽量保留旧优化器的状态。\n",
        "\n",
        "    参数：\n",
        "    - model: 新的模型实例。\n",
        "    - old_optimizer: 旧的优化器实例。\n",
        "    - lr: 学习率，默认为0.01。\n",
        "\n",
        "    返回：\n",
        "    - new_optimizer: 重新初始化的优化器实例。\n",
        "    \"\"\"\n",
        "    # 创建新的优化器\n",
        "    new_optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    # 如果旧优化器存在状态\n",
        "    if old_optimizer is not None:\n",
        "        old_state_dict = old_optimizer.state_dict()\n",
        "        new_state_dict = new_optimizer.state_dict()\n",
        "\n",
        "        # 创建一个参数名称到参数对象的映射\n",
        "        param_name_to_param = {name: param for name, param in model.named_parameters()}\n",
        "\n",
        "        # 迁移优化器的状态\n",
        "        for group_idx, group in enumerate(old_state_dict['param_groups']):\n",
        "            new_group = new_state_dict['param_groups'][group_idx]\n",
        "            new_group['lr'] = group['lr']  # 保留学习率等其他参数\n",
        "\n",
        "            # 更新每个参数的状态\n",
        "            new_group_params = []\n",
        "            for p in group['params']:\n",
        "                # 找到对应的参数对象\n",
        "                param = None\n",
        "                for name, p_new in model.named_parameters():\n",
        "                    if id(p_new) == p:\n",
        "                        param = p_new\n",
        "                        break\n",
        "                if param is not None and param.requires_grad:\n",
        "                    new_group_params.append(id(param))\n",
        "            new_group['params'] = new_group_params\n",
        "\n",
        "        # 迁移状态\n",
        "        for param_id, state in old_state_dict['state'].items():\n",
        "            # 查找新的参数对象\n",
        "            param = None\n",
        "            for p in model.parameters():\n",
        "                if id(p) == param_id:\n",
        "                    param = p\n",
        "                    break\n",
        "            if param is not None and param.requires_grad:\n",
        "                new_state_dict['state'][id(param)] = state\n",
        "\n",
        "        # 加载更新后的状态字典\n",
        "        try:\n",
        "            new_optimizer.load_state_dict(new_state_dict)\n",
        "            print(\"Optimizer state has been partially loaded.\")\n",
        "        except:\n",
        "            print(\"Warning: Could not fully load optimizer state. Continuing with new optimizer state.\")\n",
        "\n",
        "    return new_optimizer\n",
        "\n",
        "class AttentionDynamicAttributeAndRuleANFIS(nn.Module):\n",
        "    def __init__(self, n_inputs, n_rules, X_train, attention_threshold=0.1):\n",
        "        super(AttentionDynamicAttributeAndRuleANFIS, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.attention_threshold = attention_threshold  # 属性注意力阈值\n",
        "        self.n_rules = n_rules  # 初始规则数量\n",
        "\n",
        "        # 使用 KMeans 聚类初始化隶属函数中心\n",
        "        kmeans = KMeans(n_clusters=n_rules, random_state=42)\n",
        "        kmeans.fit(X_train)\n",
        "        cluster_centers = kmeans.cluster_centers_  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "        # 初始化隶属函数参数\n",
        "        self.mu = nn.Parameter(torch.tensor(cluster_centers, dtype=torch.float32))  # 均值，形状：(n_rules, n_inputs)\n",
        "        self.sigma = nn.Parameter(torch.ones(n_rules, n_inputs))  # 标准差\n",
        "\n",
        "        # 初始化属性注意力权重参数\n",
        "        self.attention_weights = nn.Parameter(torch.randn(n_rules, n_inputs))\n",
        "\n",
        "        # 初始化规则注意力权重参数\n",
        "        self.rule_attention_weights = nn.Parameter(torch.ones(n_rules))\n",
        "\n",
        "        # 初始化后件参数（对于回归任务）\n",
        "        self.consequents = nn.Parameter(torch.randn(n_rules, n_inputs))\n",
        "\n",
        "        # 初始化属性掩码（1表示活跃，0表示被剪除），针对每个规则\n",
        "        self.register_buffer('attribute_mask', torch.ones(n_rules, n_inputs))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 将被剪枝的属性的 attention_weights 设置为一个大负数\n",
        "        masked_attention_weights = self.attention_weights.clone()\n",
        "        masked_attention_weights[self.attribute_mask == 0] = -1e6\n",
        "\n",
        "        # 计算属性注意力权重（使用 sigmoid 激活函数并应用属性掩码）\n",
        "        attention = torch.sigmoid(masked_attention_weights) * self.attribute_mask  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "        # 计算规则注意力权重（使用 sigmoid 激活函数）\n",
        "        rule_attention = torch.sigmoid(self.rule_attention_weights)  # 形状：(n_rules,)\n",
        "\n",
        "        # 扩展维度以匹配批次大小\n",
        "        x_expanded = x.unsqueeze(1)  # 形状：(batch_size, 1, n_inputs)\n",
        "        mu_expanded = self.mu.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "        sigma_expanded = self.sigma.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "\n",
        "        # 确保 sigma 为正数，避免除以零\n",
        "        sigma_expanded = torch.clamp(sigma_expanded, min=1e-3)\n",
        "\n",
        "        # 计算高斯隶属度函数的对数\n",
        "        log_gauss = -0.5 * ((x_expanded - mu_expanded) ** 2) / (sigma_expanded ** 2)\n",
        "\n",
        "        # 使用属性注意力权重并应用属性掩码\n",
        "        log_gauss_weighted = log_gauss * attention.unsqueeze(0)  # 形状：(batch_size, n_rules, n_inputs)\n",
        "\n",
        "        # 对输入属性维度求和\n",
        "        sum_log_gauss = log_gauss_weighted.sum(dim=2)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算规则的激活度\n",
        "        firing_strength = torch.exp(sum_log_gauss)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 使用规则注意力权重调整规则的激活度\n",
        "        firing_strength_weighted = firing_strength * rule_attention.unsqueeze(0)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算归一化的激活度\n",
        "        sum_firing_strength = firing_strength_weighted.sum(dim=1, keepdim=True) + 1e-8\n",
        "        norm_firing_strength = firing_strength_weighted / sum_firing_strength  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算后件部分（使用属性注意力权重）\n",
        "        consequents_weighted = self.consequents * attention  # 形状：(n_rules, n_inputs)\n",
        "        consequents_weighted_expanded = consequents_weighted.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "\n",
        "        # 计算规则的输出（对于每个规则，后件为被选中属性的线性组合）\n",
        "        rule_outputs = torch.sum(consequents_weighted_expanded * x_expanded, dim=2)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算总输出\n",
        "        output = torch.sum(norm_firing_strength * rule_outputs, dim=1)  # 形状：(batch_size,)\n",
        "\n",
        "        return output, firing_strength, attention, rule_attention\n",
        "\n",
        "    def train_step(self, x, target, optimizer, lambda_attention=1e-7, lambda_rule_attention=1e-8, lambda_diversity=1e-4):\n",
        "        \"\"\"\n",
        "        执行一次训练步骤。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, n_inputs)\n",
        "        - target: 目标数据，形状：(batch_size,)\n",
        "        - optimizer: 优化器实例\n",
        "        - lambda_attention: 属性注意力权重的正则化系数\n",
        "        - lambda_rule_attention: 规则注意力权重的正则化系数\n",
        "        - lambda_diversity: 多样性正则化的系数\n",
        "\n",
        "        返回：\n",
        "        - loss.item(): 当前批次的总损失\n",
        "        - output: 模型的输出\n",
        "        \"\"\"\n",
        "        self.train()\n",
        "        optimizer.zero_grad()\n",
        "        output, _, attention, rule_attention = self.forward(x)\n",
        "        # 计算预测损失（均方误差损失）\n",
        "        loss_pred = F.mse_loss(output, target)\n",
        "\n",
        "        # 添加属性注意力正则化损失（L1 正则化）\n",
        "        loss_attention = lambda_attention * attention.abs().sum()\n",
        "\n",
        "        # 计算规则注意力正则化损失（L1 正则化）\n",
        "        loss_rule_attention = lambda_rule_attention * rule_attention.abs().sum()\n",
        "\n",
        "        # 添加多样性正则化损失（鼓励不同规则的注意力权重不同）\n",
        "        if self.n_rules > 1:\n",
        "            # 计算注意力权重的余弦相似度矩阵\n",
        "            attention_norm = attention / (attention.norm(dim=1, keepdim=True) + 1e-8)\n",
        "            similarity_matrix = torch.matmul(attention_norm, attention_norm.t())\n",
        "            # 计算非对角线的平均相似度\n",
        "            diversity_loss = torch.sum(similarity_matrix) - torch.diag(similarity_matrix).sum()\n",
        "            diversity_loss = diversity_loss / (self.n_rules * (self.n_rules - 1))\n",
        "        else:\n",
        "            diversity_loss = torch.tensor(0.0).to(attention.device)\n",
        "\n",
        "        loss_diversity = lambda_diversity * diversity_loss\n",
        "\n",
        "        # 总损失\n",
        "        loss = loss_pred + loss_attention + loss_rule_attention + loss_diversity\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Loss is NaN. Stopping training.\")\n",
        "            return loss.item(), output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # 应用梯度裁剪（如果需要）\n",
        "        # torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), output\n",
        "\n",
        "    def prune_attributes_per_rule(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01):\n",
        "        \"\"\"\n",
        "        剪除每个规则中注意力权重低于阈值的属性，并冻结其相关参数。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则撤销剪枝。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认为0.1\n",
        "        - X_val: 验证集特征，形状：(num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状：(num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "\n",
        "        返回：\n",
        "        - pruned_dict: 字典，键为规则索引，值为被剪除的属性索引列表\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "\n",
        "        # 保存剪枝前的模型状态和验证损失\n",
        "        original_state = copy.deepcopy(self.state_dict())\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            output_before = self.infer(X_val)\n",
        "            loss_before = F.mse_loss(output_before, y_val)\n",
        "\n",
        "        # 执行剪枝操作\n",
        "        pruned_dict = {}\n",
        "        with torch.no_grad():\n",
        "            attention = torch.sigmoid(self.attention_weights)  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "            for rule_idx in range(self.n_rules):\n",
        "                if torch.all(self.attribute_mask[rule_idx] == 0):\n",
        "                    continue  # 跳过已被完全剪除的规则\n",
        "\n",
        "                prune_indices = torch.where((attention[rule_idx] < threshold) & (self.attribute_mask[rule_idx] == 1))[0].tolist()\n",
        "\n",
        "                if prune_indices:\n",
        "                    # 更新属性掩码\n",
        "                    self.attribute_mask[rule_idx, prune_indices] = 0.0\n",
        "\n",
        "                    # 冻结被剪除属性的相关参数\n",
        "                    self.attention_weights[rule_idx, prune_indices].requires_grad = False\n",
        "                    self.consequents[rule_idx, prune_indices].requires_grad = False\n",
        "\n",
        "                    pruned_dict[rule_idx] = prune_indices\n",
        "\n",
        "        # 检查是否有规则的所有属性都被剪除，并处理\n",
        "        with torch.no_grad():\n",
        "            all_pruned_rules = torch.where(self.attribute_mask.sum(dim=1) == 0)[0].tolist()\n",
        "\n",
        "        if all_pruned_rules:\n",
        "            print(f\"Rules with all attributes pruned: {all_pruned_rules}\")\n",
        "            # 设置这些规则的规则注意力权重为非常低，确保被 prune_rules 剪除\n",
        "            self.rule_attention_weights.data[all_pruned_rules] = -1e6  # 安全地进行赋值操作\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = self.infer(X_val)\n",
        "            loss_after = F.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - loss_before) / loss_before\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，撤销剪枝操作\n",
        "            self.load_state_dict(original_state)\n",
        "            print(f\"Pruning was reverted due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned_dict = {}  # 清空剪枝记录\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能没有显著下降，执行规则剪枝\n",
        "            pruned = self.prune_rules(threshold=0.005)\n",
        "            if pruned:\n",
        "                print(f\"Pruned rules after attribute pruning.\")\n",
        "\n",
        "        return pruned_dict\n",
        "\n",
        "    def prune_rules(self, threshold=0.1):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认值为0.1\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        pruned = False\n",
        "        with torch.no_grad():\n",
        "            # 获取规则注意力权重\n",
        "            rule_attention = torch.sigmoid(self.rule_attention_weights)\n",
        "            # 找到需要移除的规则索引（rule_attention < threshold）\n",
        "            low_attention_indices = torch.where(rule_attention < threshold)[0]\n",
        "\n",
        "            # 找到需要移除的规则索引（所有属性已被剪除）\n",
        "            no_active_attrs_indices = torch.where(self.attribute_mask.sum(dim=1) == 0)[0]\n",
        "\n",
        "            # 合并需要移除的规则索引\n",
        "            prune_indices = torch.cat([low_attention_indices, no_active_attrs_indices])\n",
        "            prune_indices = torch.unique(prune_indices)\n",
        "\n",
        "            if len(prune_indices) == 0:\n",
        "                # 没有需要移除的规则\n",
        "                return pruned\n",
        "\n",
        "            # 保留的规则索引（rule_attention >= threshold AND 有活跃属性）\n",
        "            keep_indices = torch.where((rule_attention >= threshold) & (self.attribute_mask.sum(dim=1) > 0))[0]\n",
        "\n",
        "            # 更新模型参数，移除低重要性的规则\n",
        "            self.mu = nn.Parameter(self.mu.data[keep_indices])\n",
        "            self.sigma = nn.Parameter(self.sigma.data[keep_indices])\n",
        "            self.attention_weights = nn.Parameter(self.attention_weights.data[keep_indices])\n",
        "            self.rule_attention_weights = nn.Parameter(self.rule_attention_weights.data[keep_indices])\n",
        "            self.consequents = nn.Parameter(self.consequents.data[keep_indices])\n",
        "\n",
        "            # 更新属性掩码，移除被剪除规则的掩码行\n",
        "            self.attribute_mask = self.attribute_mask.data[keep_indices].clone()\n",
        "\n",
        "            # 更新规则数量\n",
        "            self.n_rules = len(keep_indices)\n",
        "\n",
        "            pruned = True  # 标记为已剪枝\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After pruning, attribute_mask has shape {self.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"After pruning, n_rules: {self.n_rules}, attribute_mask shape: {self.attribute_mask.shape}\")\n",
        "\n",
        "        return pruned\n",
        "\n",
        "    def grow_rule(self, X_new):\n",
        "        \"\"\"\n",
        "        添加一个新的规则。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, n_inputs)\n",
        "        \"\"\"\n",
        "        # 获取设备和数据类型\n",
        "        device = self.mu.device\n",
        "        dtype = self.mu.dtype\n",
        "\n",
        "        # 使用 X_new 计算新的规则中心和标准差\n",
        "        new_mu = torch.tensor(X_new.mean(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, n_inputs)\n",
        "        new_sigma = torch.tensor(X_new.std(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, n_inputs)\n",
        "\n",
        "        # 计算现有规则的属性注意力权重的平均值\n",
        "        if self.n_rules > 0:\n",
        "            existing_attention_weights = torch.sigmoid(self.attention_weights).data  # (n_rules, n_inputs)\n",
        "            attention_mean = existing_attention_weights.mean(dim=0, keepdim=True)  # (1, n_inputs)\n",
        "        else:\n",
        "            attention_mean = torch.ones(1, self.n_inputs, dtype=dtype).to(device)  # 初始化为1\n",
        "\n",
        "        # 将新规则的属性注意力权重初始化为平均值并加入随机扰动\n",
        "        noise = torch.randn_like(attention_mean) * 0.05  # 调整扰动大小以控制多样性\n",
        "        new_attention_weights = (attention_mean + noise).clamp(0, 1).detach()  # 保持在[0,1]范围内\n",
        "\n",
        "        # 将新规则的规则注意力权重初始化为与现有权重的均值 logit 相同，并加入随机扰动\n",
        "        if self.n_rules > 0:\n",
        "            existing_rule_attention_logits = self.rule_attention_weights.data  # (n_rules,)\n",
        "            rule_attention_mean_logit = existing_rule_attention_logits.mean().unsqueeze(0)  # (1,)\n",
        "            rule_attention_noise = torch.randn_like(rule_attention_mean_logit) * 0.05  # 调整扰动大小\n",
        "            new_rule_attention_weight = (rule_attention_mean_logit + rule_attention_noise).detach()\n",
        "        else:\n",
        "            rule_attention_mean_logit = torch.tensor([0.0], dtype=dtype).to(device)  # 中性 logit\n",
        "            new_rule_attention_weight = rule_attention_mean_logit.clone().detach()  # (1,)\n",
        "\n",
        "        # 初始化后件参数为小的随机值\n",
        "        new_consequents = torch.randn(1, self.n_inputs, dtype=dtype).to(device) * 0.01  # (1, n_inputs)\n",
        "\n",
        "        # 将新的参数添加到模型中\n",
        "        self.mu = nn.Parameter(torch.cat([self.mu.data, new_mu], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.sigma = nn.Parameter(torch.cat([self.sigma.data, new_sigma], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.attention_weights = nn.Parameter(torch.cat([self.attention_weights.data, new_attention_weights], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.rule_attention_weights = nn.Parameter(torch.cat([self.rule_attention_weights.data, new_rule_attention_weight], dim=0))  # (n_rules + 1,)\n",
        "        self.consequents = nn.Parameter(torch.cat([self.consequents.data, new_consequents], dim=0))  # (n_rules + 1, n_inputs)\n",
        "\n",
        "        # 更新属性掩码，添加新规则的掩码行\n",
        "        new_attribute_mask = torch.ones(1, self.n_inputs, dtype=self.attribute_mask.dtype).to(device)  # (1, n_inputs)\n",
        "        self.attribute_mask = torch.cat([self.attribute_mask, new_attribute_mask], dim=0)  # (n_rules + 1, n_inputs)\n",
        "\n",
        "        # 更新规则数量\n",
        "        self.n_rules += 1\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After growing, attribute_mask has shape {self.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"New rule added. Total rules: {self.n_rules}\")\n",
        "\n",
        "    def infer(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        执行推理。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, n_inputs)\n",
        "        - targets: 目标数据，形状：(batch_size,)，可选\n",
        "\n",
        "        返回：\n",
        "        - 如果 targets 为 None，返回模型输出。\n",
        "        - 否则，返回模型输出和损失值。\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            output, _, _, _ = self.forward(x)\n",
        "            if targets is None:\n",
        "                return output\n",
        "            else:\n",
        "                loss = F.mse_loss(output, targets)\n",
        "                return output, loss.item()\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "    def plot_membership_functions(self, feature_names=None):\n",
        "        \"\"\"\n",
        "        绘制训练后的隶属函数图像。\n",
        "\n",
        "        参数：\n",
        "        - feature_names: 特征名称列表，默认为 None。\n",
        "        \"\"\"\n",
        "        mus = self.mu.detach().cpu().numpy()\n",
        "        sigmas = self.sigma.detach().cpu().numpy()\n",
        "        attentions = torch.sigmoid(self.attention_weights).detach().cpu().numpy()\n",
        "        rule_attentions = torch.sigmoid(self.rule_attention_weights).detach().cpu().numpy()\n",
        "        xn = np.linspace(-3, 3, 1000)\n",
        "\n",
        "        n_inputs = self.n_inputs\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "        for r in range(self.n_rules):\n",
        "            rule_attention_value = rule_attentions[r]\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.title(f\"Rule {r + 1}, Rule Attention: {rule_attention_value:.4f}\")\n",
        "            for j in range(n_inputs):\n",
        "                # 使用 attribute_mask 确认是否为活跃属性\n",
        "                if self.attribute_mask[r, j] == 0:\n",
        "                    continue  # 跳过被剪除的属性\n",
        "                attention_value = attentions[r, j]\n",
        "                # 绘制带有注意力权重的隶属函数\n",
        "                y = np.exp(-0.5 * ((xn - mus[r, j]) ** 2) / (sigmas[r, j] ** 2 + 1e-8))\n",
        "                plt.plot(xn, y, label=f\"{feature_names[j]} (Attn: {attention_value:.4f})\")\n",
        "            plt.legend()\n",
        "            plt.xlabel('Input')\n",
        "            plt.ylabel('Membership degree')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "def extract_fuzzy_rules(anfis_model, scaler_X, feature_names=None):\n",
        "    \"\"\"\n",
        "    提取 ANFIS 模型的模糊规则，包含所有用于计算的权重。\n",
        "\n",
        "    参数：\n",
        "    - anfis_model: 训练好的 ANFIS 模型\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - feature_names: 特征名称列表\n",
        "\n",
        "    返回：\n",
        "    - rules: 包含规则字符串的列表\n",
        "    \"\"\"\n",
        "    # 获取模型的参数\n",
        "    mus = anfis_model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigmas = anfis_model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "    rule_attention_weights = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "    consequents = anfis_model.consequents.detach().cpu().numpy()\n",
        "\n",
        "    # 反标准化 mu 和 sigma\n",
        "    c_orig = mus * scaler_X.scale_ + scaler_X.mean_  # (n_rules, n_inputs)\n",
        "    sigma_orig = sigmas * scaler_X.scale_  # (n_rules, n_inputs)\n",
        "\n",
        "    # 获取属性掩码\n",
        "    attribute_mask = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "\n",
        "    # 如果未提供特征名称，使用默认名称\n",
        "    n_rules, n_inputs = mus.shape\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "    rules = []\n",
        "\n",
        "    for i in range(n_rules):\n",
        "        # 包含规则注意力权重\n",
        "        rule_str = (f\"Rule {i+1} (Rule Attention: \"\n",
        "                    f\"{rule_attention_weights[i]:.4f}): IF \")\n",
        "        antecedent = []\n",
        "        for j in range(n_inputs):\n",
        "            if attribute_mask[i, j] == 0:\n",
        "                continue  # 忽略被剪枝的属性\n",
        "            attention_value = attention_weights[i, j]\n",
        "            c_val = c_orig[i, j]\n",
        "            sigma_val = sigma_orig[i, j]\n",
        "            antecedent.append(\n",
        "                f\"[{feature_names[j]} (Attn: {attention_value:.4f}) \"\n",
        "                f\"is Gaussian(c={c_val:.4f}, σ={sigma_val:.4f})]\"\n",
        "            )\n",
        "        antecedent_str = \" AND \".join(antecedent) if antecedent else \"True\"\n",
        "        rule_str += antecedent_str + \" THEN Output = \"\n",
        "\n",
        "        consequent_terms = []\n",
        "        for j in range(n_inputs):\n",
        "            if attribute_mask[i, j] == 0:\n",
        "                continue\n",
        "            attention_value = attention_weights[i, j]\n",
        "            coef = consequents[i, j]\n",
        "            consequent_terms.append(\n",
        "                f\"({coef:.4f} * {feature_names[j]} \"\n",
        "                f\"(Attn: {attention_value:.4f}))\"\n",
        "            )\n",
        "        consequent_str = \" + \".join(consequent_terms) if consequent_terms else \"0\"\n",
        "        rule_str += consequent_str\n",
        "        rules.append(rule_str)\n",
        "\n",
        "    return rules\n",
        "\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    \"\"\"\n",
        "    使用解析解计算两个高斯隶属度函数的重叠面积。\n",
        "\n",
        "    参数：\n",
        "    - c1, sigma1: 第一个高斯函数的中心和标准差\n",
        "    - c2, sigma2: 第二个高斯函数的中心和标准差\n",
        "\n",
        "    返回：\n",
        "    - overlap_area: 两个高斯函数的重叠面积\n",
        "    \"\"\"\n",
        "    denominator = np.sqrt(sigma1**2 + sigma2**2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(model):\n",
        "    \"\"\"\n",
        "    计算 Average Overlap Index (Iov)。\n",
        "\n",
        "    参数：\n",
        "    - model: 训练好的 ANFIS 模型\n",
        "\n",
        "    返回：\n",
        "    - average_iov: 平均重叠指数\n",
        "    \"\"\"\n",
        "    mus = model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigmas = model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attribute_mask = model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    n_rules, n_inputs = mus.shape\n",
        "\n",
        "    total_max_overlap = 0\n",
        "    valid_attributes = 0\n",
        "\n",
        "    for j in range(n_inputs):\n",
        "        # 获取当前属性的活跃规则\n",
        "        active_rules = np.where(attribute_mask[:, j] == 1)[0]\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算重叠\n",
        "\n",
        "        max_overlap = -np.inf\n",
        "        for i in range(len(active_rules)):\n",
        "            for k in range(i + 1, len(active_rules)):\n",
        "                rule_i = active_rules[i]\n",
        "                rule_k = active_rules[k]\n",
        "                c1 = mus[rule_i, j]\n",
        "                sigma1 = sigmas[rule_i, j]\n",
        "                c2 = mus[rule_k, j]\n",
        "                sigma2 = sigmas[rule_k, j]\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "        if max_overlap != -np.inf:\n",
        "            total_max_overlap += max_overlap\n",
        "            valid_attributes += 1\n",
        "\n",
        "    if valid_attributes == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_iov = total_max_overlap / valid_attributes\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(model):\n",
        "    \"\"\"\n",
        "    计算 Average Fuzzy Set Position Index (Ifspe)。\n",
        "\n",
        "    参数：\n",
        "    - model: 训练好的 ANFIS 模型\n",
        "\n",
        "    返回：\n",
        "    - average_ifspe: 平均模糊集位置指数（非负数）\n",
        "    \"\"\"\n",
        "    mus = model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigmas = model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attribute_mask = model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    n_rules, n_inputs = mus.shape\n",
        "\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for j in range(n_inputs):\n",
        "        # 获取当前属性的活跃规则\n",
        "        active_rules = np.where(attribute_mask[:, j] == 1)[0]\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算 Ifspe\n",
        "\n",
        "        # 按中心值排序\n",
        "        sorted_indices = active_rules[np.argsort(mus[active_rules, j])]\n",
        "        sorted_centers = mus[sorted_indices, j]\n",
        "        sorted_sigma = sigmas[sorted_indices, j]\n",
        "\n",
        "        # 计算相邻规则对的 phi 和 psi\n",
        "        for l in range(len(sorted_centers) - 1):\n",
        "            v_l = sorted_centers[l]\n",
        "            v_lp1 = sorted_centers[l + 1]\n",
        "            s_l = sorted_sigma[l]\n",
        "            s_lp1 = sorted_sigma[l + 1]\n",
        "\n",
        "            phi = np.exp(-0.5 * ((v_l + v_lp1) / (s_l + s_lp1))**2)\n",
        "            denominator = s_l - s_lp1\n",
        "            if denominator == 0:\n",
        "                psi = 0\n",
        "            else:\n",
        "                psi = np.exp(-0.5 * ((v_l + v_lp1) / denominator)**2)\n",
        "\n",
        "            # 使用绝对值确保 Ifspe_term 为非负数\n",
        "            ifspe_term = 2 * abs(0.5 - phi) + psi\n",
        "\n",
        "            total_ifspe += ifspe_term\n",
        "            valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    # 归一化因子为 L * D\n",
        "    average_ifspe = total_ifspe / (n_inputs * n_rules)\n",
        "    return average_ifspe\n",
        "\n",
        "\n",
        "def plot_attribute_weights(attribute_weights, feature_names):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(feature_names))\n",
        "    plt.bar(x, attribute_weights)\n",
        "    plt.xticks(x, feature_names, rotation=45)\n",
        "    plt.xlabel('Attributes')\n",
        "    plt.ylabel('Average Attribute Weights')\n",
        "    plt.title('Average Attribute Weights over Repeats')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_heatmap(anfis_model, feature_names):\n",
        "    \"\"\"\n",
        "    绘制属性权重的热力图。\n",
        "\n",
        "    参数：\n",
        "    - anfis_model: 训练好的 ANFIS 模型\n",
        "    - feature_names: 特征名称列表\n",
        "    \"\"\"\n",
        "    # 提取属性掩码\n",
        "    attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "\n",
        "    # 将被剪枝的属性的 attention_weights 设置为一个大负数\n",
        "    attention_weights = anfis_model.attention_weights.clone()\n",
        "    attention_weights[anfis_model.attribute_mask == 0] = -1e6\n",
        "\n",
        "    # 计算注意力权重，并应用 attribute_mask\n",
        "    attention = torch.sigmoid(attention_weights) * anfis_model.attribute_mask\n",
        "    attention_np = attention.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "\n",
        "    # 创建注释字符串\n",
        "    annotations = []\n",
        "    for r in range(attention_np.shape[0]):\n",
        "        row = []\n",
        "        for a in range(attention_np.shape[1]):\n",
        "            if attribute_mask_np[r, a] == 0:\n",
        "                row.append(\"0.00\\nX\")  # 被剪枝的属性，值为 0，标记为 X\n",
        "            else:\n",
        "                row.append(f\"{attention_np[r, a]:.2f}\")\n",
        "        annotations.append(row)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(\n",
        "        attention_np,\n",
        "        annot=annotations,\n",
        "        fmt='',\n",
        "        cmap='viridis',\n",
        "        xticklabels=feature_names,\n",
        "        yticklabels=[f'Rule {i+1}' for i in range(anfis_model.n_rules)],\n",
        "        cbar_kws={'label': 'Attention Weight'}\n",
        "    )\n",
        "    plt.title(f'属性注意力权重 (被剪枝的属性标记为 X)')\n",
        "    plt.xlabel('输入特征')\n",
        "    plt.ylabel('规则')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_attention_dynamic_attribute_and_rule_anfis(\n",
        "    X_train_np, y_train_np, X_val_np, y_val_np,\n",
        "    initial_n_rules=3, epochs=1500, batch_size=32, lr=0.01,\n",
        "    prune_frequency=190, prune_threshold=0.0001,\n",
        "    best_model_path='best_model.pth'\n",
        "):\n",
        "    \"\"\"\n",
        "    训练 AttentionDynamicAttributeAndRuleANFIS 模型。\n",
        "    并在训练过程中保存验证集上表现最好的模型。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集特征，形状为 (num_samples, n_inputs)\n",
        "    - y_train_np: 训练集目标，形状为 (num_samples,)\n",
        "    - X_val_np: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "    - y_val_np: 验证集目标，形状为 (num_val_samples,)\n",
        "    - initial_n_rules: 初始规则数量，默认值为 3\n",
        "    - epochs: 训练轮数，默认值为 1500\n",
        "    - batch_size: 每批次的样本数量，默认值为 32\n",
        "    - lr: 学习率，默认值为 0.01\n",
        "    - prune_frequency: 进行属性剪枝的频率（每隔多少个 epoch）\n",
        "    - prune_threshold: 属性剪枝的阈值\n",
        "    - best_model_path: 最佳模型保存的文件路径，默认值为 'best_model.pth'\n",
        "\n",
        "    返回：\n",
        "    - anfis_model: 训练好的 ANFIS 模型（加载了最佳模型状态）\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - scaler_y: 输出数据的标准化器\n",
        "    - total_active_attributes: 最优模型的总活跃属性数量\n",
        "    \"\"\"\n",
        "    # 创建结果保存的目录\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    n_inputs = X_train_scaled_tensor.shape[1]\n",
        "\n",
        "    anfis_model = AttentionDynamicAttributeAndRuleANFIS(\n",
        "        n_inputs=n_inputs,\n",
        "        n_rules=initial_n_rules,\n",
        "        X_train=X_train_scaled,\n",
        "        attention_threshold=prune_threshold\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器和调度器\n",
        "    optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "    # 使用余弦退火学习率调度器\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    # 将优化器赋值给模型\n",
        "    anfis_model.optimizer = optimizer\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'active_rules': [],\n",
        "        'total_rules': [],\n",
        "        'val_rmse': [],\n",
        "        'attribute_weights': [],\n",
        "        'rule_attention_weights': [],\n",
        "        'pruned_attributes': [],\n",
        "        'total_active_attributes': []  # 新增\n",
        "    }\n",
        "\n",
        "    # 初始化变量以记录最佳验证损失和最佳模型状态\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # 设置规则生长和剪枝的参数\n",
        "    patience = 25  # 等待多少个 epoch 后触发规则生长\n",
        "    grow_threshold = 0.0001  # 训练损失下降低于该阈值，触发规则生长\n",
        "    no_improve_epochs = 0\n",
        "    prev_val_loss = float('inf')\n",
        "\n",
        "    max_rules = 9  # 设置规则数量上限，防止无限生长\n",
        "    attention_threshold_final = 0.05  # 定义活跃规则的注意力权重阈值\n",
        "\n",
        "    # 设置特征名称（请根据您的数据集进行调整）\n",
        "    feature_labels = [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "    # 初始化列表用于统计总属性数量\n",
        "    total_attributes_list = []\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
        "        anfis_model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            loss_train, _ = anfis_model.train_step(\n",
        "                batch_x,\n",
        "                batch_y,\n",
        "                optimizer,\n",
        "                lambda_attention=1e-7,\n",
        "                lambda_rule_attention=1e-8,\n",
        "                lambda_diversity=1e-4\n",
        "            )\n",
        "            epoch_loss += loss_train\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        anfis_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, firing_strength, attention, rule_attention = anfis_model.forward(X_val_scaled_tensor)\n",
        "            loss_val = F.mse_loss(output, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(output.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "\n",
        "        # 调整学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 计算当前活跃规则的数量\n",
        "        rule_attention_np = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "        num_active_rules = np.sum(rule_attention_np >= attention_threshold_final)\n",
        "\n",
        "        # 计算总活跃属性数量（仅在当前 epoch 使用）\n",
        "        attribute_mask = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        num_active_attributes_per_rule = np.sum(attribute_mask, axis=1)  # 每个规则中活跃的属性数量\n",
        "        total_active_attributes = np.sum(num_active_attributes_per_rule)  # 该模型中总的活跃属性数量\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val.item())\n",
        "        training_info['active_rules'].append(num_active_rules)\n",
        "        training_info['total_rules'].append(anfis_model.n_rules)\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "        training_info['total_active_attributes'].append(total_active_attributes)  # 新增\n",
        "\n",
        "        # 提取注意力权重\n",
        "        attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "\n",
        "        # 计算平均属性权重\n",
        "        avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "\n",
        "        # 保存注意力权重\n",
        "        training_info['attribute_weights'].append(avg_attribute_weights)\n",
        "        training_info['rule_attention_weights'].append(rule_attention_weights)\n",
        "\n",
        "        # 保存总活跃属性数量\n",
        "        total_attributes_list.append(total_active_attributes)\n",
        "\n",
        "        # Check for best validation loss\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            best_model_state = copy.deepcopy(anfis_model.state_dict())\n",
        "            torch.save(anfis_model.state_dict(), best_model_path)\n",
        "            print(f\"Epoch {epoch+1}: New best validation loss: {loss_val.item():.4f}. Model saved.\")\n",
        "\n",
        "        # 显示注意力权重信息\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {loss_val.item():.4f} - Val RMSE: {val_rmse:.4f} - Total Rules: {anfis_model.n_rules} - Active Rules: {num_active_rules}\")\n",
        "\n",
        "        # 检查验证损失的改进情况\n",
        "        if loss_val.item() < prev_val_loss - grow_threshold:\n",
        "            no_improve_epochs = 0\n",
        "            prev_val_loss = loss_val.item()\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # 如果验证损失在连续若干个 epoch 中没有显著改进，触发规则生长\n",
        "        if no_improve_epochs >= patience and anfis_model.n_rules < max_rules:\n",
        "            print(f\"Epoch {epoch+1}: No significant improvement in validation loss, growing a new rule. Current rules: {anfis_model.n_rules}\")\n",
        "            # 找出当前误差较大的数据点，用于初始化新规则\n",
        "            residuals = (y_val_scaled_tensor.cpu().numpy() - output.cpu().numpy())\n",
        "            high_error_indices = np.argsort(np.abs(residuals))[-int(0.1 * len(residuals)):]  # 选取误差最大的 10% 数据\n",
        "            X_new_rule = X_val_scaled[high_error_indices]\n",
        "            # 添加新规则\n",
        "            anfis_model.grow_rule(X_new_rule)\n",
        "            print(f\"Epoch {epoch+1}: Added new rule. Total rules: {anfis_model.n_rules}\")\n",
        "            no_improve_epochs = 0  # 重置计数器\n",
        "\n",
        "            # 重新初始化优化器和调度器\n",
        "            optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "            # 将优化器赋值给模型\n",
        "            anfis_model.optimizer = optimizer\n",
        "            print(\"Optimizer and scheduler re-initialized after growing a new rule.\")\n",
        "\n",
        "        # 设置剪枝停止的 epoch 阈值\n",
        "        pruning_stop_epoch = int(epochs * 0.8)  # 在 80% 的训练过程中进行剪枝\n",
        "\n",
        "        # 每隔 prune_frequency 个 epoch 进行属性剪枝\n",
        "        if (epoch + 1) % prune_frequency == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned_dict = anfis_model.prune_attributes_per_rule(\n",
        "                threshold=prune_threshold,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01  # 性能下降容忍度，可根据需要调整\n",
        "            )\n",
        "            training_info['pruned_attributes'].append(pruned_dict)\n",
        "            if pruned_dict:\n",
        "                print(f\"Epoch {epoch+1}: Pruned attributes per rule: {pruned_dict}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                # 将优化器赋值给模型\n",
        "                anfis_model.optimizer = optimizer\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning attributes.\")\n",
        "        elif epoch >= pruning_stop_epoch and (epoch + 1) % prune_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}: Pruning has been stopped to stabilize the model structure.\")\n",
        "\n",
        "        # 每隔若干个 epoch 进行规则剪枝\n",
        "        if (epoch + 1) % 50 == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned = anfis_model.prune_rules(threshold=attention_threshold_final)\n",
        "            if pruned:\n",
        "                print(f\"Epoch {epoch+1}: Pruned rules. Total rules: {anfis_model.n_rules}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                # 将优化器赋值给模型\n",
        "                anfis_model.optimizer = optimizer\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch and (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Rule pruning has been stopped to stabilize the model structure.\")\n",
        "\n",
        "    # 在训练结束后，加载最佳模型的状态字典\n",
        "    if best_model_state is not None:\n",
        "        anfis_model.load_state_dict(best_model_state)\n",
        "        print(\"Loaded the best model based on validation loss.\")\n",
        "\n",
        "        # 显示训练后的隶属函数图像\n",
        "        anfis_model.plot_membership_functions(feature_names=feature_labels)\n",
        "\n",
        "        # 绘制属性权重的热力图\n",
        "        plot_heatmap(anfis_model, feature_names=feature_labels)\n",
        "\n",
        "        # 提取并保存规则\n",
        "        rules = extract_fuzzy_rules(anfis_model, scaler_X, feature_names=feature_labels)\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "        # 保存规则到文件\n",
        "        # with open(f'results/rules_nrules{anfis_model.n_rules}_lr{lr}_final.txt', 'w') as f:\n",
        "        #     for rule in rules:\n",
        "        #         f.write(rule + '\\n')\n",
        "\n",
        "        # 计算总活跃属性数量\n",
        "        attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask_np)\n",
        "        print(f\"Total Number of Attributes Included in All Rules: {total_active_attributes:.2f}\")\n",
        "    else:\n",
        "        print(\"No improvement during training. Using the final model.\")\n",
        "\n",
        "        # 计算总活跃属性数量\n",
        "        attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask_np)\n",
        "        print(f\"Total Number of Attributes Included in All Rules: {total_active_attributes:.2f}\")\n",
        "\n",
        "    # 可视化训练过程\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_info['epoch'], training_info['train_loss'], label='Train Loss')\n",
        "    plt.plot(training_info['epoch'], training_info['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 可视化验证集上的 RMSE\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_info['epoch'], training_info['val_rmse'], label='Validation RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Validation RMSE over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 返回最佳模型和标准化器及总属性数量\n",
        "    return anfis_model, scaler_X, scaler_y, total_active_attributes\n",
        "\n",
        "# 实验参数\n",
        "learning_rates = [0.01]\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results = []\n",
        "\n",
        "# 假设您已经有 X 和 y 数据\n",
        "# 请确保 X 和 y 是 pandas DataFrame/Series 或 numpy arrays\n",
        "# 这里假设 X 和 y 已经定义\n",
        "# 例如：\n",
        "# import pandas as pd\n",
        "# X = pd.read_csv('features.csv')\n",
        "# y = pd.read_csv('targets.csv').values.flatten()\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "n_rules = 9\n",
        "for lr in learning_rates:\n",
        "    test_rmse_list = []  # List to store test set RMSEs for each repeat\n",
        "    val_rmse_list = []   # List to store validation set RMSEs for each repeat\n",
        "    time_list = []\n",
        "    attribute_weights_list = []\n",
        "    overlap_indices_list = []\n",
        "    position_indices_list = []\n",
        "    total_attributes_list_experiment = []\n",
        "    print(f\"\\nStarting experiments for n_rules={n_rules}, learning_rate={lr}\")\n",
        "    for repeat in range(repeats):\n",
        "        # Record start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Further split the training set into training and validation sets\n",
        "        X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "            X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "        )\n",
        "\n",
        "        # Define a unique path for saving the best model for this repeat\n",
        "        best_model_path = f'results/best_model_nrules{n_rules}_lr{lr}_repeat{repeat+1}.pth'\n",
        "\n",
        "        # Train the model\n",
        "        anfis_model, scaler_X, scaler_y, total_active_attributes = train_attention_dynamic_attribute_and_rule_anfis(\n",
        "            X_train_sub, y_train_sub, X_val_sub, y_val_sub,\n",
        "            initial_n_rules=n_rules,\n",
        "            epochs=1500,\n",
        "            batch_size=512,\n",
        "            lr=lr,\n",
        "            prune_frequency=25,\n",
        "            prune_threshold=0.25,\n",
        "            best_model_path=best_model_path  # 传递最佳模型保存路径\n",
        "        )\n",
        "\n",
        "        # 加载最佳模型的状态字典（确保使用的是最佳模型）\n",
        "        anfis_model.eval()  # 设置为评估模式\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        anfis_model.to(device)\n",
        "\n",
        "        # Test the model on the test set using the best model\n",
        "        X_test_scaled = scaler_X.transform(X_test_np)  # 使用 X_test_np 作为测试集\n",
        "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "        y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            y_pred_scaled = anfis_model.infer(X_test_tensor)\n",
        "            y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_true = y_test_np  # Original unstandardized y_test\n",
        "            test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "            test_rmse_list.append(test_rmse)\n",
        "\n",
        "        # Compute RMSE on the validation set\n",
        "        X_val_scaled = scaler_X.transform(X_val_sub)\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        y_val_tensor = torch.tensor(y_val_sub, dtype=torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            y_val_pred_scaled = anfis_model.infer(X_val_tensor)\n",
        "            y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = y_val_sub  # 使用原始的 y_val_sub 作为真实值\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "            val_rmse_list.append(val_rmse)\n",
        "\n",
        "        # Record end time\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        time_list.append(time_taken)\n",
        "\n",
        "        # Extract attribute weights\n",
        "        attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "        avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "        attribute_weights_list.append(avg_attribute_weights)\n",
        "\n",
        "        # Compute interpretability indices\n",
        "        overlap_index = compute_iov(anfis_model)\n",
        "        position_index = compute_ifspe(anfis_model)\n",
        "        overlap_indices_list.append(overlap_index)\n",
        "        position_indices_list.append(position_index)\n",
        "\n",
        "        # Collect total active attributes from best model\n",
        "        total_attributes_list_experiment.append(total_active_attributes)\n",
        "\n",
        "        print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Val RMSE={val_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "        # Extract and save fuzzy rules\n",
        "        rules = extract_fuzzy_rules(anfis_model, scaler_X, feature_names=features_to_use)\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "        # # Save rules to file\n",
        "        # with open(f'results/rules_nrules{n_rules}_lr{lr}_repeat{repeat+1}.txt', 'w') as f:\n",
        "        #     for rule in rules:\n",
        "        #         f.write(rule + '\\n')\n",
        "\n",
        "    # Compute RMSE mean and std for test set and validation set\n",
        "    test_rmse_mean = np.mean(test_rmse_list)\n",
        "    test_rmse_std = np.std(test_rmse_list)\n",
        "    val_rmse_mean = np.mean(val_rmse_list)\n",
        "    val_rmse_std = np.std(val_rmse_list)\n",
        "\n",
        "    # Compute average attribute weights over repeats\n",
        "    avg_attribute_weights_over_repeats = np.mean(attribute_weights_list, axis=0)\n",
        "\n",
        "    # Compute average total number of attributes included in all rules\n",
        "    average_total_attributes = np.mean(total_attributes_list_experiment)\n",
        "\n",
        "    # Compute average interpretability indices\n",
        "    avg_overlap_index = np.mean(overlap_indices_list)\n",
        "    avg_position_index = np.mean(position_indices_list)\n",
        "    std_overlap_index = np.std(overlap_indices_list)\n",
        "    std_position_index = np.std(position_indices_list)\n",
        "    # Print the results\n",
        "    print(f\"\\nResults for n_rules={n_rules}, learning_rate={lr}:\")\n",
        "    print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "    print(f\"Validation RMSE: {val_rmse_mean:.4f} ± {val_rmse_std:.4f}\")\n",
        "    print(f\"Time: {np.mean(time_list):.2f}s ± {np.std(time_list):.2f}s\")\n",
        "    print(f\"Average Overlap Index (Iov): {avg_overlap_index:.4f} ± {std_overlap_index:.4f}\")\n",
        "    print(f\"Average Fuzzy Set Position Index (Ifspe): {avg_position_index:.4f} ± {std_position_index:.4f} \")\n",
        "    print(f\"Average Attribute Weights over Repeats: {avg_attribute_weights_over_repeats}\")\n",
        "    print(f\"Average Total Number of Attributes Included in All Rules: {average_total_attributes:.2f}\")\n",
        "\n",
        "    # Save results\n",
        "    result = {\n",
        "        'n_rules': n_rules,\n",
        "        'learning_rate': lr,\n",
        "        'test_rmse_mean': test_rmse_mean,\n",
        "        'test_rmse_std': test_rmse_std,\n",
        "        'val_rmse_mean': val_rmse_mean,\n",
        "        'val_rmse_std': val_rmse_std,\n",
        "        'time_mean': np.mean(time_list),\n",
        "        'time_std': np.std(time_list),\n",
        "        'attribute_weights': avg_attribute_weights_over_repeats,\n",
        "        'overlap_index': avg_overlap_index,\n",
        "        'position_index': avg_position_index,\n",
        "        'average_total_attributes': average_total_attributes\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "    # Save result data\n",
        "    np.save(f'results/attribute_weights_nrules{n_rules}_lr{lr}.npy', avg_attribute_weights_over_repeats)\n",
        "    np.save(f'results/overlap_index_nrules{n_rules}_lr{lr}.npy', avg_overlap_index)\n",
        "    np.save(f'results/position_index_nrules{n_rules}_lr{lr}.npy', avg_position_index)\n",
        "    np.save(f'results/average_total_attributes_nrules{n_rules}_lr{lr}.npy', average_total_attributes)\n",
        "\n",
        "    # 可视化重叠指数和位置指数\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(['Overlap Index (Iov)', 'Position Index (Ifspe)'], [avg_overlap_index, avg_position_index], color=['skyblue', 'salmon'])\n",
        "    plt.ylabel('Index Value')\n",
        "    plt.title('Average Interpretability Indices')\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()\n",
        "\n",
        "    # 可视化平均总属性数量\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(['Average Total Attributes'], [average_total_attributes], color=['lightgreen'])\n",
        "    plt.ylabel('Number of Attributes')\n",
        "    plt.title('Average Total Number of Attributes Included in All Rules')\n",
        "    plt.grid(axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 注意：\n",
        "# - 请确保您已经定义了 X 和 y 数据。\n",
        "# - X 应该是一个包含特征的 DataFrame 或 numpy 数组。\n",
        "# - y 应该是一个包含目标变量的 Series 或 numpy 数组。\n",
        "# - 例如：\n",
        "# import pandas as pd\n",
        "# X = pd.read_csv('features.csv')\n",
        "# y = pd.read_csv('targets.csv').values.flatten()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIsSiX9WuWLQ"
      },
      "source": [
        "# ADAR-ANFIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMbV9jvGhAyp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 从 UCI ML Repo 导入 Auto MPG 数据集\n",
        "auto_mpg_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "\n",
        "# 定义列名称\n",
        "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "                'acceleration', 'model_year', 'origin', 'car_name']\n",
        "\n",
        "# 读取数据集，处理缺失值\n",
        "data = pd.read_csv(auto_mpg_url, delim_whitespace=True, names=column_names, na_values='?')\n",
        "\n",
        "# 删除含有缺失值的样本\n",
        "data = data.dropna()\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'mpg' 和 'car_name'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "    'acceleration', 'model_year', 'origin'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['mpg']\n",
        "\n",
        "# 将类别变量 'origin' 进行独热编码（如果需要，可以选择保留为数值型）\n",
        "# 这里保留为数值型，以简化 ANFIS 模型的处理\n",
        "# 如果希望进行独热编码，请取消下方代码的注释\n",
        "# X = pd.get_dummies(X, columns=['origin'], drop_first=True)\n",
        "# features_to_use = X.columns.tolist()\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本（已在读取时完成）\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJiiQuYI6m2V"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from tqdm import tqdm, trange\n",
        "from scipy.stats import norm  # 新增\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "from sklearn.datasets import fetch_openml\n",
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip'\n",
        "\n",
        "# 定义列名称\n",
        "col_names = ['Year'] + [f'Feature_{i}' for i in range(1, 91)]\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(data_url, header=None, names=col_names)\n",
        "\n",
        "# 特征选择\n",
        "X = data.drop('Year', axis=1)\n",
        "y = data['Year']\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = X.columns.tolist()\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "features_to_use=feature_labels\n",
        "\n",
        "# ============================\n",
        "# 定义 ADAR-ANFIS 模型\n",
        "# ============================\n",
        "\n",
        "class AttentionDynamicAttributeAndRuleANFIS(nn.Module):\n",
        "    def __init__(self, n_inputs, n_rules, X_train=None, attention_threshold=0.1, init_from_checkpoint=False):\n",
        "        super(AttentionDynamicAttributeAndRuleANFIS, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.attention_threshold = attention_threshold  # 属性注意力阈值\n",
        "        self.n_rules = n_rules  # 初始规则数量\n",
        "\n",
        "        if not init_from_checkpoint:\n",
        "            if X_train is None:\n",
        "                raise ValueError(\"X_train must be provided for initializing rules.\")\n",
        "\n",
        "            # 使用 KMeans 聚类初始化隶属函数中心\n",
        "            kmeans = KMeans(n_clusters=n_rules, random_state=42)\n",
        "            kmeans.fit(X_train)\n",
        "            cluster_centers = kmeans.cluster_centers_  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "            # 初始化隶属函数参数\n",
        "            self.mu = nn.Parameter(torch.tensor(cluster_centers, dtype=torch.float32))  # 均值，形状：(n_rules, n_inputs)\n",
        "            self.sigma = nn.Parameter(torch.ones(n_rules, n_inputs))  # 标准差\n",
        "\n",
        "            # 初始化属性注意力权重参数\n",
        "            self.attention_weights = nn.Parameter(torch.randn(n_rules, n_inputs))\n",
        "\n",
        "            # 初始化规则注意力权重参数\n",
        "            self.rule_attention_weights = nn.Parameter(torch.ones(n_rules))\n",
        "\n",
        "            # 初始化后件参数（对于回归任务）\n",
        "            self.consequents = nn.Parameter(torch.randn(n_rules, n_inputs))\n",
        "\n",
        "            # 初始化属性掩码（1表示活跃，0表示被剪除），针对每个规则\n",
        "            device = self.mu.device  # 获取设备\n",
        "            self.register_buffer('attribute_mask', torch.ones(n_rules, n_inputs, device=device))\n",
        "        else:\n",
        "            # 初始化占位参数，实际参数将在加载后设置\n",
        "            self.mu = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.sigma = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.attention_weights = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.rule_attention_weights = nn.Parameter(torch.empty(n_rules))\n",
        "            self.consequents = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.register_buffer('attribute_mask', torch.empty(n_rules, n_inputs))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 将被剪枝的属性的 attention_weights 设置为一个大负数\n",
        "        masked_attention_weights = self.attention_weights.clone()\n",
        "        masked_attention_weights[self.attribute_mask == 0] = -1e6\n",
        "\n",
        "        # 计算属性注意力权重（使用 sigmoid 激活函数并应用属性掩码）\n",
        "        attention = torch.sigmoid(masked_attention_weights) * self.attribute_mask  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "        # 计算规则注意力权重（使用 sigmoid 激活函数）\n",
        "        rule_attention = torch.sigmoid(self.rule_attention_weights)  # 形状：(n_rules,)\n",
        "\n",
        "        # 扩展维度以匹配批次大小\n",
        "        x_expanded = x.unsqueeze(1)  # 形状：(batch_size, 1, n_inputs)\n",
        "        mu_expanded = self.mu.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "        sigma_expanded = self.sigma.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "\n",
        "        # 确保 sigma 为正数，避免除以零\n",
        "        sigma_expanded = torch.clamp(sigma_expanded, min=1e-3)\n",
        "\n",
        "        # 计算高斯隶属度函数的对数\n",
        "        log_gauss = -0.5 * ((x_expanded - mu_expanded) ** 2) / (sigma_expanded ** 2)\n",
        "\n",
        "        # 使用属性注意力权重并应用属性掩码\n",
        "        log_gauss_weighted = log_gauss * attention.unsqueeze(0)  # 形状：(batch_size, n_rules, n_inputs)\n",
        "\n",
        "        # 对输入属性维度求和\n",
        "        sum_log_gauss = log_gauss_weighted.sum(dim=2)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算规则的激活度\n",
        "        firing_strength = torch.exp(sum_log_gauss)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 使用规则注意力权重调整规则的激活度\n",
        "        firing_strength_weighted = firing_strength * rule_attention.unsqueeze(0)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算归一化的激活度\n",
        "        sum_firing_strength = firing_strength_weighted.sum(dim=1, keepdim=True) + 1e-8\n",
        "        norm_firing_strength = firing_strength_weighted / sum_firing_strength  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算后件部分（使用属性注意力权重）\n",
        "        consequents_weighted = self.consequents * attention  # 形状：(n_rules, n_inputs)\n",
        "        consequents_weighted_expanded = consequents_weighted.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "\n",
        "        # 计算规则的输出（对于每个规则，后件为被选中属性的线性组合）\n",
        "        rule_outputs = torch.sum(consequents_weighted_expanded * x_expanded, dim=2)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算总输出\n",
        "        output = torch.sum(norm_firing_strength * rule_outputs, dim=1)  # 形状：(batch_size,)\n",
        "\n",
        "        return output, firing_strength, attention, rule_attention\n",
        "\n",
        "    def train_step(self, x, target, optimizer, lambda_attention=1e-7, lambda_rule_attention=1e-8, lambda_diversity=1e-4):\n",
        "        \"\"\"\n",
        "        执行一次训练步骤。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, n_inputs)\n",
        "        - target: 目标数据，形状：(batch_size,)\n",
        "        - optimizer: 优化器实例\n",
        "        - lambda_attention: 属性注意力权重的正则化系数\n",
        "        - lambda_rule_attention: 规则注意力权重的正则化系数\n",
        "        - lambda_diversity: 多样性正则化的系数\n",
        "\n",
        "        返回：\n",
        "        - loss.item(): 当前批次的总损失\n",
        "        - output: 模型的输出\n",
        "        \"\"\"\n",
        "        self.train()\n",
        "        optimizer.zero_grad()\n",
        "        output, firing_strength, attention, rule_attention = self.forward(x)\n",
        "        # 计算预测损失（均方误差损失）\n",
        "        loss_pred = F.mse_loss(output, target)\n",
        "\n",
        "        # 添加属性注意力正则化损失（L1 正则化）\n",
        "        loss_attention = lambda_attention * attention.abs().sum()\n",
        "\n",
        "        # 计算规则注意力正则化损失（L1 正则化）\n",
        "        loss_rule_attention = lambda_rule_attention * rule_attention.abs().sum()\n",
        "\n",
        "        # 添加多样性正则化损失（鼓励不同规则的注意力权重不同）\n",
        "        if self.n_rules > 1:\n",
        "            # 计算注意力权重的余弦相似度矩阵\n",
        "            attention_norm = attention / (attention.norm(dim=1, keepdim=True) + 1e-8)\n",
        "            similarity_matrix = torch.matmul(attention_norm, attention_norm.t())\n",
        "            # 计算非对角线的平均相似度\n",
        "            diversity_loss = torch.sum(similarity_matrix) - torch.diag(similarity_matrix).sum()\n",
        "            diversity_loss = diversity_loss / (self.n_rules * (self.n_rules - 1))\n",
        "        else:\n",
        "            diversity_loss = torch.tensor(0.0).to(attention.device)\n",
        "\n",
        "        loss_diversity = lambda_diversity * diversity_loss\n",
        "\n",
        "        # 总损失\n",
        "        loss = loss_pred + loss_attention + loss_rule_attention + loss_diversity\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Loss is NaN. Stopping training.\")\n",
        "            return loss.item(), output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), output\n",
        "\n",
        "    def prune_attributes_per_rule(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除每个规则中注意力权重低于阈值的属性，并冻结其相关参数。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则不执行剪枝。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认为0.1\n",
        "        - X_val: 验证集特征，形状：(num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状：(num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "        - best_val_loss: 之前的最佳验证损失\n",
        "\n",
        "        返回：\n",
        "        - pruned_dict: 字典，键为规则索引，值为被剪除的属性索引列表\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self)\n",
        "\n",
        "        # 执行剪枝操作\n",
        "        pruned_dict = {}\n",
        "        with torch.no_grad():\n",
        "            attention = torch.sigmoid(model_copy.attention_weights)  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "            for rule_idx in range(model_copy.n_rules):\n",
        "                if torch.all(model_copy.attribute_mask[rule_idx] == 0):\n",
        "                    continue  # 跳过已被完全剪除的规则\n",
        "\n",
        "                prune_indices = torch.where((attention[rule_idx] < threshold) & (model_copy.attribute_mask[rule_idx] == 1))[0].tolist()\n",
        "\n",
        "                if prune_indices:\n",
        "                    # 更新属性掩码\n",
        "                    model_copy.attribute_mask[rule_idx, prune_indices] = 0.0\n",
        "\n",
        "                    # 冻结被剪除属性的相关参数\n",
        "                    model_copy.attention_weights[rule_idx, prune_indices].requires_grad = False\n",
        "                    model_copy.consequents[rule_idx, prune_indices].requires_grad = False\n",
        "\n",
        "                    pruned_dict[rule_idx] = prune_indices\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = model_copy.infer(X_val)\n",
        "            loss_after = F.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - best_val_loss) / best_val_loss\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，不执行剪枝\n",
        "            print(f\"Attribute pruning was not performed due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned_dict = {}  # 清空剪枝记录\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能未下降，更新原始模型的参数\n",
        "            self.attribute_mask = model_copy.attribute_mask.clone()\n",
        "            self.attention_weights.data = model_copy.attention_weights.data.clone()\n",
        "            self.consequents.data = model_copy.consequents.data.clone()\n",
        "            print(f\"Attribute pruning successful. Performance drop: {performance_drop * 100:.2f}%\")\n",
        "            pruned = True\n",
        "\n",
        "        return pruned_dict\n",
        "\n",
        "    def prune_rules_with_recovery(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则不执行剪枝。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认值为0.1\n",
        "        - X_val: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状为 (num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "        - best_val_loss: 之前的最佳验证损失\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self)\n",
        "\n",
        "        # 执行规则剪枝操作\n",
        "        pruned = False\n",
        "        with torch.no_grad():\n",
        "            # 获取规则注意力权重\n",
        "            rule_attention = torch.sigmoid(model_copy.rule_attention_weights)\n",
        "            # 找到需要移除的规则索引（rule_attention < threshold）\n",
        "            low_attention_indices = torch.where(rule_attention < threshold)[0]\n",
        "\n",
        "            if len(low_attention_indices) == 0:\n",
        "                return pruned  # 没有需要移除的规则\n",
        "\n",
        "            # 保留的规则索引（rule_attention >= threshold）\n",
        "            keep_indices = torch.where(rule_attention >= threshold)[0]\n",
        "\n",
        "            # 更新模型参数，移除低重要性的规则\n",
        "            model_copy.mu = nn.Parameter(model_copy.mu.data[keep_indices])\n",
        "            model_copy.sigma = nn.Parameter(model_copy.sigma.data[keep_indices])\n",
        "            model_copy.attention_weights = nn.Parameter(model_copy.attention_weights.data[keep_indices])\n",
        "            model_copy.rule_attention_weights = nn.Parameter(model_copy.rule_attention_weights.data[keep_indices])\n",
        "            model_copy.consequents = nn.Parameter(model_copy.consequents.data[keep_indices])\n",
        "            model_copy.attribute_mask = model_copy.attribute_mask.data[keep_indices].clone()\n",
        "\n",
        "            # 更新规则数量\n",
        "            model_copy.n_rules = len(keep_indices)\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = model_copy.infer(X_val)\n",
        "            loss_after = F.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - best_val_loss) / best_val_loss\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，不执行剪枝\n",
        "            print(f\"Rule pruning was not performed due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能未下降，更新原始模型的参数\n",
        "            self.mu = nn.Parameter(model_copy.mu.data.clone())\n",
        "            self.sigma = nn.Parameter(model_copy.sigma.data.clone())\n",
        "            self.attention_weights = nn.Parameter(model_copy.attention_weights.data.clone())\n",
        "            self.rule_attention_weights = nn.Parameter(model_copy.rule_attention_weights.data.clone())\n",
        "            self.consequents = nn.Parameter(model_copy.consequents.data.clone())\n",
        "            self.attribute_mask = model_copy.attribute_mask.clone()\n",
        "            self.n_rules = model_copy.n_rules  # 更新规则数量\n",
        "            print(f\"Rules pruned successfully. Performance drop: {performance_drop * 100:.2f}%\")\n",
        "            pruned = True\n",
        "\n",
        "        return pruned\n",
        "\n",
        "    def prune_rules(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则撤销剪枝操作。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认值为0.1\n",
        "        - X_val: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状为 (num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "        - best_val_loss: 之前的最佳验证损失\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        return self.prune_rules_with_recovery(threshold, X_val, y_val, performance_drop_tolerance, best_val_loss)\n",
        "\n",
        "    def grow_rule_with_performance_check(self, X_new, X_train, y_train, X_val, y_val, best_val_loss, device, lr, grow_epochs=10):\n",
        "        \"\"\"\n",
        "        添加一个新的规则，并进行性能检查。如果性能没有提升，则撤销规则生长。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, n_inputs)\n",
        "        - X_train: 训练集特征，形状：(num_train_samples, n_inputs)\n",
        "        - y_train: 训练集目标，形状：(num_train_samples,)\n",
        "        - X_val: 验证集特征，形状：(num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状：(num_val_samples,)\n",
        "        - best_val_loss: 当前最佳验证损失\n",
        "        - device: 设备\n",
        "        - lr: 学习率\n",
        "        - grow_epochs: 在规则生长后训练的 epoch 数，默认值为10\n",
        "\n",
        "        返回：\n",
        "        - improved: 布尔值，指示是否保留了新规则\n",
        "        \"\"\"\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self).to(device)\n",
        "\n",
        "        # 添加新规则到副本\n",
        "        model_copy.grow_rule(X_new)\n",
        "\n",
        "        # 初始化优化器和调度器\n",
        "        optimizer_copy = optim.AdamW(model_copy.parameters(), lr=lr)\n",
        "        scheduler_copy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_copy, T_max=grow_epochs)\n",
        "\n",
        "        # 将训练数据转换为张量\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
        "\n",
        "        # 训练模型副本\n",
        "        for epoch in range(grow_epochs):\n",
        "            model_copy.train()\n",
        "            optimizer_copy.zero_grad()\n",
        "            output, _, _, _ = model_copy.forward(X_train_tensor)\n",
        "            loss = F.mse_loss(output, y_train_tensor)\n",
        "            loss.backward()\n",
        "            optimizer_copy.step()\n",
        "            scheduler_copy.step()\n",
        "\n",
        "        # 评估副本模型的性能\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model_copy.forward(X_val_tensor)[0]\n",
        "            loss = F.mse_loss(output, y_val_tensor).item()\n",
        "\n",
        "        print(f\"After growing rule and training for {grow_epochs} epochs: Validation Loss = {loss:.4f}\")\n",
        "\n",
        "        if loss < best_val_loss:\n",
        "            # 性能有所提升，保留新规则\n",
        "            # 手动更新原始模型的参数\n",
        "            self.mu = nn.Parameter(model_copy.mu.data.clone())\n",
        "            self.sigma = nn.Parameter(model_copy.sigma.data.clone())\n",
        "            self.attention_weights = nn.Parameter(model_copy.attention_weights.data.clone())\n",
        "            self.rule_attention_weights = nn.Parameter(model_copy.rule_attention_weights.data.clone())\n",
        "            self.consequents = nn.Parameter(model_copy.consequents.data.clone())\n",
        "            self.attribute_mask = model_copy.attribute_mask.clone()\n",
        "            self.n_rules = model_copy.n_rules  # 更新规则数量\n",
        "            print(\"Performance improved after growing rule. New rule retained.\")\n",
        "            return True\n",
        "        else:\n",
        "            # 性能未提升，丢弃副本，保持主模型不变\n",
        "            print(\"Performance did not improve after growing rule. Rule growth reverted.\")\n",
        "            return False\n",
        "\n",
        "    def grow_rule(self, X_new):\n",
        "        \"\"\"\n",
        "        添加一个新的规则。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, n_inputs)\n",
        "        \"\"\"\n",
        "        # 获取设备和数据类型\n",
        "        device = self.mu.device\n",
        "        dtype = self.mu.dtype\n",
        "\n",
        "        # 使用 X_new 计算新的规则中心和标准差\n",
        "        new_mu = torch.tensor(X_new.mean(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, n_inputs)\n",
        "        new_sigma = torch.tensor(X_new.std(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, n_inputs)\n",
        "\n",
        "        # 计算现有规则的属性注意力权重的平均值\n",
        "        if self.n_rules > 0:\n",
        "            existing_attention_weights = torch.sigmoid(self.attention_weights).data  # (n_rules, n_inputs)\n",
        "            attention_mean = existing_attention_weights.mean(dim=0, keepdim=True)  # (1, n_inputs)\n",
        "        else:\n",
        "            attention_mean = torch.ones(1, self.n_inputs, dtype=dtype).to(device)  # 初始化为1\n",
        "\n",
        "        # 将新规则的属性注意力权重初始化为平均值并加入随机扰动\n",
        "        noise = torch.randn_like(attention_mean) * 0.05  # 调整扰动大小以控制多样性\n",
        "        new_attention_weights = (attention_mean + noise).clamp(0, 1).detach()  # 保持在[0,1]范围内\n",
        "\n",
        "        # 将新规则的规则注意力权重初始化为与现有权重的均值 logit 相同，并加入随机扰动\n",
        "        if self.n_rules > 0:\n",
        "            existing_rule_attention_logits = self.rule_attention_weights.data  # (n_rules,)\n",
        "            rule_attention_mean_logit = existing_rule_attention_logits.mean().unsqueeze(0)  # (1,)\n",
        "            rule_attention_noise = torch.randn_like(rule_attention_mean_logit) * 0.05  # 调整扰动大小\n",
        "            new_rule_attention_weight = (rule_attention_mean_logit + rule_attention_noise).detach()\n",
        "        else:\n",
        "            rule_attention_mean_logit = torch.tensor([0.0], dtype=dtype).to(device)  # 中性 logit\n",
        "            new_rule_attention_weight = rule_attention_mean_logit.clone().detach()  # (1,)\n",
        "\n",
        "        # 初始化后件参数为小的随机值\n",
        "        new_consequents = torch.randn(1, self.n_inputs, dtype=dtype).to(device) * 0.01  # (1, n_inputs)\n",
        "\n",
        "        # 将新的参数添加到模型中\n",
        "        self.mu = nn.Parameter(torch.cat([self.mu.data, new_mu], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.sigma = nn.Parameter(torch.cat([self.sigma.data, new_sigma], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.attention_weights = nn.Parameter(torch.cat([self.attention_weights.data, new_attention_weights], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.rule_attention_weights = nn.Parameter(torch.cat([self.rule_attention_weights.data, new_rule_attention_weight], dim=0))  # (n_rules + 1,)\n",
        "        self.consequents = nn.Parameter(torch.cat([self.consequents.data, new_consequents], dim=0))  # (n_rules + 1, n_inputs)\n",
        "\n",
        "        # 更新属性掩码，添加新规则的掩码行\n",
        "        new_attribute_mask = torch.ones(1, self.n_inputs, dtype=self.attribute_mask.dtype).to(device)  # (1, n_inputs)\n",
        "        self.attribute_mask = torch.cat([self.attribute_mask, new_attribute_mask], dim=0)  # (n_rules + 1, n_inputs)\n",
        "\n",
        "        # 更新规则数量\n",
        "        self.n_rules += 1\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After growing, attribute_mask has shape {self.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"New rule added. Total rules: {self.n_rules}\")\n",
        "\n",
        "    def infer(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        执行推理。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, n_inputs)\n",
        "        - targets: 目标数据，形状：(batch_size,)，可选\n",
        "\n",
        "        返回：\n",
        "        - 如果 targets 为 None，返回模型输出。\n",
        "        - 否则，返回模型输出和损失值。\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            output, _, _, _ = self.forward(x)\n",
        "            if targets is None:\n",
        "                return output\n",
        "            else:\n",
        "                loss = F.mse_loss(output, targets)\n",
        "                return output, loss.item()\n",
        "\n",
        "    def save_model_with_architecture(self, scaler_X, scaler_y, path):\n",
        "        \"\"\"\n",
        "        保存模型的状态字典和架构信息。\n",
        "\n",
        "        参数：\n",
        "        - scaler_X: 输入数据的标准化器\n",
        "        - scaler_y: 输出数据的标准化器\n",
        "        - path: 保存路径\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'n_rules': self.n_rules,\n",
        "            'mu': self.mu.detach().cpu().numpy(),\n",
        "            'sigma': self.sigma.detach().cpu().numpy(),\n",
        "            'attention_weights': self.attention_weights.detach().cpu().numpy(),\n",
        "            'rule_attention_weights': self.rule_attention_weights.detach().cpu().numpy(),\n",
        "            'consequents': self.consequents.detach().cpu().numpy(),\n",
        "            'attribute_mask': self.attribute_mask.detach().cpu().numpy(),\n",
        "            'scaler_X_mean': scaler_X.mean_,\n",
        "            'scaler_X_scale': scaler_X.scale_,\n",
        "            'scaler_y_mean': scaler_y.mean_,\n",
        "            'scaler_y_scale': scaler_y.scale_\n",
        "        }, path)\n",
        "        print(f\"Model and architecture saved to {path}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model_with_architecture(path, device='cpu'):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        n_rules = checkpoint['n_rules']\n",
        "        n_inputs = checkpoint['mu'].shape[1]\n",
        "\n",
        "        # 初始化模型，设置 init_from_checkpoint=True 以跳过 KMeans 初始化\n",
        "        model = AttentionDynamicAttributeAndRuleANFIS(\n",
        "            n_inputs=n_inputs,\n",
        "            n_rules=n_rules,\n",
        "            X_train=None,  # 跳过 KMeans 初始化\n",
        "            attention_threshold=0.1,\n",
        "            init_from_checkpoint=True\n",
        "        ).to(device)\n",
        "\n",
        "        # 手动设置参数\n",
        "        with torch.no_grad():\n",
        "            model.mu = nn.Parameter(torch.tensor(checkpoint['mu'], dtype=torch.float32, device=device))\n",
        "            model.sigma = nn.Parameter(torch.tensor(checkpoint['sigma'], dtype=torch.float32, device=device))\n",
        "            model.attention_weights = nn.Parameter(torch.tensor(checkpoint['attention_weights'], dtype=torch.float32, device=device))\n",
        "            model.rule_attention_weights = nn.Parameter(torch.tensor(checkpoint['rule_attention_weights'], dtype=torch.float32, device=device))\n",
        "            model.consequents = nn.Parameter(torch.tensor(checkpoint['consequents'], dtype=torch.float32, device=device))\n",
        "\n",
        "            # 使用 copy_ 复制到 attribute_mask buffer，确保在正确的设备上\n",
        "            model.attribute_mask.copy_(torch.tensor(checkpoint['attribute_mask'], dtype=torch.float32, device=device))\n",
        "\n",
        "        # 加载标准化器参数\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_X.mean_ = checkpoint['scaler_X_mean']\n",
        "        scaler_X.scale_ = checkpoint['scaler_X_scale']\n",
        "\n",
        "        scaler_y = StandardScaler()\n",
        "        scaler_y.mean_ = checkpoint['scaler_y_mean']\n",
        "        scaler_y.scale_ = checkpoint['scaler_y_scale']\n",
        "\n",
        "        print(f\"Model and architecture loaded from {path} with n_rules={n_rules}\")\n",
        "\n",
        "        return model, scaler_X, scaler_y\n",
        "\n",
        "    def plot_membership_functions(self, feature_names=None):\n",
        "        \"\"\"\n",
        "        绘制训练后的隶属函数图像。\n",
        "\n",
        "        参数：\n",
        "        - feature_names: 特征名称列表，默认为 None。\n",
        "        \"\"\"\n",
        "        mus = self.mu.detach().cpu().numpy()\n",
        "        sigmas = self.sigma.detach().cpu().numpy()\n",
        "        attentions = torch.sigmoid(self.attention_weights).detach().cpu().numpy()\n",
        "        rule_attentions = torch.sigmoid(self.rule_attention_weights).detach().cpu().numpy()\n",
        "        xn = np.linspace(-3, 3, 1000)\n",
        "\n",
        "        n_inputs = self.n_inputs\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "        for r in range(self.n_rules):\n",
        "            rule_attention_value = rule_attentions[r]\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.title(f\"Rule {r + 1}, Rule Attention: {rule_attention_value:.4f}\")\n",
        "            for j in range(n_inputs):\n",
        "                # 使用 attribute_mask 确认是否为活跃属性\n",
        "                if self.attribute_mask[r, j] == 0:\n",
        "                    continue  # 跳过被剪除的属性\n",
        "                attention_value = attentions[r, j]\n",
        "                c_val = mus[r, j]\n",
        "                sigma_val = sigmas[r, j]\n",
        "                # 绘制带有注意力权重的隶属函数\n",
        "                y = np.exp(-0.5 * ((xn - c_val) ** 2) / (sigma_val ** 2 + 1e-8))\n",
        "                plt.plot(xn, y, label=f\"{feature_names[j]} (Attn: {attention_value:.4f})\")\n",
        "            plt.legend()\n",
        "            plt.xlabel('Input')\n",
        "            plt.ylabel('Membership degree')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# ============================\n",
        "# 定义辅助函数\n",
        "# ============================\n",
        "\n",
        "def extract_fuzzy_rules(anfis_model, scaler_X, feature_names=None):\n",
        "    \"\"\"\n",
        "    提取 ANFIS 模型的模糊规则，包含所有用于计算的权重。\n",
        "\n",
        "    参数：\n",
        "    - anfis_model: 训练好的 ANFIS 模型\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - feature_names: 特征名称列表\n",
        "\n",
        "    返回：\n",
        "    - rules: 包含规则字符串的列表\n",
        "    \"\"\"\n",
        "    # 获取模型的参数\n",
        "    mus = anfis_model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigmas = anfis_model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "    rule_attention_weights = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "    consequents = anfis_model.consequents.detach().cpu().numpy()\n",
        "\n",
        "    # 反标准化 mu 和 sigma\n",
        "    c_orig = mus * scaler_X.scale_ + scaler_X.mean_  # (n_rules, n_inputs)\n",
        "    sigma_orig = sigmas * scaler_X.scale_  # (n_rules, n_inputs)\n",
        "\n",
        "    # 获取属性掩码\n",
        "    attribute_mask = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "\n",
        "    # 如果未提供特征名称，使用默认名称\n",
        "    n_rules, n_inputs = mus.shape\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "    rules = []\n",
        "\n",
        "    for i in range(n_rules):\n",
        "        # 包含规则注意力权重\n",
        "        rule_str = (f\"Rule {i+1} (Rule Attention: \"\n",
        "                    f\"{rule_attention_weights[i]:.4f}): IF \")\n",
        "        antecedent = []\n",
        "        for j in range(n_inputs):\n",
        "            if attribute_mask[i, j] == 0:\n",
        "                continue  # 忽略被剪枝的属性\n",
        "            attention_value = attention_weights[i, j]\n",
        "            c_val = c_orig[i, j]\n",
        "            sigma_val = sigma_orig[i, j]\n",
        "            antecedent.append(\n",
        "                f\"[{feature_names[j]} (Attn: {attention_value:.4f}) \"\n",
        "                f\"is Gaussian(c={c_val:.4f}, σ={sigma_val:.4f})]\"\n",
        "            )\n",
        "        antecedent_str = \" AND \".join(antecedent) if antecedent else \"True\"\n",
        "        rule_str += antecedent_str + \" THEN Output = \"\n",
        "\n",
        "        consequent_terms = []\n",
        "        for j in range(n_inputs):\n",
        "            if attribute_mask[i, j] == 0:\n",
        "                continue\n",
        "            attention_value = attention_weights[i, j]\n",
        "            coef = consequents[i, j]\n",
        "            consequent_terms.append(\n",
        "                f\"({coef:.4f} * {feature_names[j]} \"\n",
        "                f\"(Attn: {attention_value:.4f}))\"\n",
        "            )\n",
        "        consequent_str = \" + \".join(consequent_terms) if consequent_terms else \"0\"\n",
        "        rule_str += consequent_str\n",
        "        rules.append(rule_str)\n",
        "\n",
        "    return rules\n",
        "\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    \"\"\"\n",
        "    使用解析解计算两个高斯隶属度函数的重叠面积。\n",
        "\n",
        "    参数：\n",
        "    - c1, sigma1: 第一个高斯函数的中心和标准差\n",
        "    - c2, sigma2: 第二个高斯函数的中心和标准差\n",
        "\n",
        "    返回：\n",
        "    - overlap_area: 两个高斯函数的重叠面积\n",
        "    \"\"\"\n",
        "    denominator = np.sqrt(sigma1**2 + sigma2**2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(model):\n",
        "    \"\"\"\n",
        "    计算 Average Overlap Index (Iov)。\n",
        "\n",
        "    参数：\n",
        "    - model: 训练好的 ANFIS 模型\n",
        "\n",
        "    返回：\n",
        "    - average_iov: 平均重叠指数\n",
        "    \"\"\"\n",
        "    mus = model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigmas = model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attribute_mask = model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    n_rules, n_inputs = mus.shape\n",
        "\n",
        "    total_max_overlap = 0\n",
        "    valid_attributes = 0\n",
        "\n",
        "    for j in range(n_inputs):\n",
        "        # 获取当前属性的活跃规则\n",
        "        active_rules = np.where(attribute_mask[:, j] == 1)[0]\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算重叠\n",
        "\n",
        "        max_overlap = -np.inf\n",
        "        for i in range(len(active_rules)):\n",
        "            for k in range(i + 1, len(active_rules)):\n",
        "                rule_i = active_rules[i]\n",
        "                rule_k = active_rules[k]\n",
        "                c1 = mus[rule_i, j]\n",
        "                sigma1 = sigmas[rule_i, j]\n",
        "                c2 = mus[rule_k, j]\n",
        "                sigma2 = sigmas[rule_k, j]\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "        if max_overlap != -np.inf:\n",
        "            total_max_overlap += max_overlap\n",
        "            valid_attributes += 1\n",
        "\n",
        "    if valid_attributes == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_iov = total_max_overlap / valid_attributes\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(model):\n",
        "    \"\"\"\n",
        "    计算 Average Fuzzy Set Position Index (Ifspe)。\n",
        "\n",
        "    参数：\n",
        "    - model: 训练好的 ANFIS 模型\n",
        "\n",
        "    返回：\n",
        "    - average_ifspe: 平均模糊集位置指数（非负数）\n",
        "    \"\"\"\n",
        "    mus = model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigmas = model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attribute_mask = model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    n_rules, n_inputs = mus.shape\n",
        "\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for j in range(n_inputs):\n",
        "        # 获取当前属性的活跃规则\n",
        "        active_rules = np.where(attribute_mask[:, j] == 1)[0]\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算 Ifspe\n",
        "\n",
        "        # 按中心值排序\n",
        "        sorted_indices = active_rules[np.argsort(mus[active_rules, j])]\n",
        "        sorted_centers = mus[sorted_indices, j]\n",
        "        sorted_sigma = sigmas[sorted_indices, j]\n",
        "\n",
        "        # 计算相邻规则对的 phi 和 psi\n",
        "        for l in range(len(sorted_centers) - 1):\n",
        "            v_l = sorted_centers[l]\n",
        "            v_lp1 = sorted_centers[l + 1]\n",
        "            s_l = sorted_sigma[l]\n",
        "            s_lp1 = sorted_sigma[l + 1]\n",
        "\n",
        "            phi = np.exp(-0.5 * ((v_l + v_lp1) / (s_l + s_lp1))**2)\n",
        "            denominator = s_l - s_lp1\n",
        "            if denominator == 0:\n",
        "                psi = 0\n",
        "            else:\n",
        "                psi = np.exp(-0.5 * ((v_l + v_lp1) / denominator)**2)\n",
        "\n",
        "            # 使用绝对值确保 Ifspe_term 为非负数\n",
        "            ifspe_term = 2 * abs(0.5 - phi) + psi\n",
        "\n",
        "            total_ifspe += ifspe_term\n",
        "            valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    # 归一化因子为 L * D\n",
        "    average_ifspe = total_ifspe / (n_inputs * n_rules)\n",
        "    return average_ifspe\n",
        "\n",
        "def plot_attribute_weights(attribute_weights, feature_names):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(feature_names))\n",
        "    plt.bar(x, attribute_weights)\n",
        "    plt.xticks(x, feature_names, rotation=45)\n",
        "    plt.xlabel('Attributes')\n",
        "    plt.ylabel('Average Attribute Weights')\n",
        "    plt.title('Average Attribute Weights over Repeats')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_heatmap(anfis_model, feature_names):\n",
        "    \"\"\"\n",
        "    绘制属性权重的热力图。\n",
        "\n",
        "    参数：\n",
        "    - anfis_model: 训练好的 ANFIS 模型\n",
        "    - feature_names: 特征名称列表\n",
        "    \"\"\"\n",
        "    # 提取属性掩码\n",
        "    attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "\n",
        "    # 将被剪枝的属性的 attention_weights 设置为一个大负数\n",
        "    attention_weights = torch.sigmoid(anfis_model.attention_weights).clone()\n",
        "    attention_weights[anfis_model.attribute_mask == 0] = -1e6\n",
        "\n",
        "    # 计算注意力权重，并应用 attribute_mask\n",
        "    attention = torch.sigmoid(attention_weights) * anfis_model.attribute_mask\n",
        "    attention_np = attention.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "\n",
        "    # 创建注释字符串\n",
        "    annotations = []\n",
        "    for r in range(attention_np.shape[0]):\n",
        "        row = []\n",
        "        for a in range(attention_np.shape[1]):\n",
        "            if attribute_mask_np[r, a] == 0:\n",
        "                row.append(\"0.00\\nX\")  # 被剪枝的属性，值为 0，标记为 X\n",
        "            else:\n",
        "                row.append(f\"{attention_np[r, a]:.2f}\")\n",
        "        annotations.append(row)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(\n",
        "        attention_np,\n",
        "        annot=annotations,\n",
        "        fmt='',\n",
        "        cmap='viridis',\n",
        "        xticklabels=feature_names,\n",
        "        yticklabels=[f'Rule {i+1}' for i in range(anfis_model.n_rules)],\n",
        "        cbar_kws={'label': 'Attention Weight'}\n",
        "    )\n",
        "    plt.title(f'属性注意力权重 (被剪枝的属性标记为 X)')\n",
        "    plt.xlabel('输入特征')\n",
        "    plt.ylabel('规则')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ============================\n",
        "# 定义训练函数\n",
        "# ============================\n",
        "\n",
        "def train_attention_dynamic_attribute_and_rule_anfis(\n",
        "    X_train_np, y_train_np, X_val_np, y_val_np,\n",
        "    initial_n_rules=3, epochs=1500, batch_size=32, lr=0.01,\n",
        "    prune_frequency=25, prune_threshold=0.1,\n",
        "    best_model_path='best_model.pth'\n",
        "):\n",
        "    \"\"\"\n",
        "    训练 AttentionDynamicAttributeAndRuleANFIS 模型。\n",
        "    并在训练过程中保存验证集上表现最好的模型。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集特征，形状为 (num_samples, n_inputs)\n",
        "    - y_train_np: 训练集目标，形状为 (num_samples,)\n",
        "    - X_val_np: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "    - y_val_np: 验证集目标，形状为 (num_val_samples,)\n",
        "    - initial_n_rules: 初始规则数量，默认值为 3\n",
        "    - epochs: 训练轮数，默认值为 1500\n",
        "    - batch_size: 每批次的样本数量，默认值为 32\n",
        "    - lr: 学习率，默认值为 0.01\n",
        "    - prune_frequency: 进行属性剪枝的频率（每隔多少个 epoch）\n",
        "    - prune_threshold: 属性剪枝的阈值\n",
        "    - best_model_path: 最佳模型保存的文件路径，默认值为 'best_model.pth'\n",
        "\n",
        "    返回:\n",
        "    - anfis_model: 训练好的 ANFIS 模型（加载了最佳模型状态）\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - scaler_y: 输出数据的标准化器\n",
        "    - total_active_attributes: 最优模型的总活跃属性数量\n",
        "    - training_info: 训练过程中的信息\n",
        "    \"\"\"\n",
        "    # 创建结果保存的目录\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    n_inputs = X_train_scaled_tensor.shape[1]\n",
        "\n",
        "    anfis_model = AttentionDynamicAttributeAndRuleANFIS(\n",
        "        n_inputs=n_inputs,\n",
        "        n_rules=initial_n_rules,\n",
        "        X_train=X_train_scaled,\n",
        "        attention_threshold=prune_threshold\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器和调度器\n",
        "    optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "    # 使用余弦退火学习率调度器\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    # 将优化器赋值给模型\n",
        "    anfis_model.optimizer = optimizer\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'active_rules': [],\n",
        "        'total_rules': [],\n",
        "        'val_rmse': [],\n",
        "        'attribute_weights': [],\n",
        "        'rule_attention_weights': [],\n",
        "        'pruned_attributes': [],\n",
        "        'total_active_attributes': []  # 新增\n",
        "    }\n",
        "\n",
        "    # 初始化变量以记录最佳验证损失和最佳模型状态\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None  # 用于保存最佳模型状态\n",
        "\n",
        "    # 设置规则生长和剪枝的参数\n",
        "    patience = 25  # 等待多少个 epoch 后触发规则生长\n",
        "    grow_threshold = 0.0001  # 训练损失下降低于该阈值，触发规则生长\n",
        "    no_improve_epochs = 0\n",
        "    prev_val_loss = float('inf')\n",
        "\n",
        "    max_rules = 3  # 设置规则数量上限，防止无限生长\n",
        "    attention_threshold_final = 0.05  # 定义活跃规则的注意力权重阈值\n",
        "\n",
        "    # 设置特征名称\n",
        "    feature_names = feature_labels if feature_labels else [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "    # 初始化列表用于统计总属性数量\n",
        "    total_attributes_list = []\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in trange(epochs, desc=\"Training\"):\n",
        "        anfis_model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            loss_train, _ = anfis_model.train_step(\n",
        "                batch_x,\n",
        "                batch_y,\n",
        "                optimizer,\n",
        "                lambda_attention=1e-7,\n",
        "                lambda_rule_attention=1e-8,\n",
        "                lambda_diversity=1e-4\n",
        "            )\n",
        "            epoch_loss += loss_train\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        anfis_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, firing_strength, attention, rule_attention = anfis_model.forward(X_val_scaled_tensor)\n",
        "            loss_val = F.mse_loss(output, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(output.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "\n",
        "        # 调整学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 计算当前活跃规则的数量\n",
        "        rule_attention_np = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "        num_active_rules = np.sum(rule_attention_np >= attention_threshold_final)\n",
        "\n",
        "        # 计算总活跃属性数量（仅在当前 epoch 使用）\n",
        "        attribute_mask = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        num_active_attributes_per_rule = np.sum(attribute_mask, axis=1)  # 每个规则中活跃的属性数量\n",
        "        total_active_attributes = np.sum(num_active_attributes_per_rule)  # 该模型中总的活跃属性数量\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val.item())\n",
        "        training_info['active_rules'].append(num_active_rules)\n",
        "        training_info['total_rules'].append(anfis_model.n_rules)\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "        training_info['total_active_attributes'].append(total_active_attributes)\n",
        "\n",
        "        # 提取注意力权重\n",
        "        attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "\n",
        "        # 计算平均属性权重\n",
        "        avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "\n",
        "        # 保存注意力权重\n",
        "        training_info['attribute_weights'].append(avg_attribute_weights)\n",
        "        training_info['rule_attention_weights'].append(rule_attention_weights)\n",
        "\n",
        "        # 保存总活跃属性数量\n",
        "        total_attributes_list.append(total_active_attributes)\n",
        "\n",
        "        # Check for best validation loss\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            # 保存最佳模型状态\n",
        "            best_model_state = copy.deepcopy(anfis_model.state_dict())\n",
        "            # 保存最佳模型\n",
        "            anfis_model.save_model_with_architecture(scaler_X, scaler_y, best_model_path)\n",
        "            print(f\"\\nEpoch {epoch+1}: New best validation loss: {loss_val.item():.6f}. Model saved.\")\n",
        "\n",
        "        # 显示训练进度\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.6f} - Val Loss: {loss_val.item():.6f} - Val RMSE: {val_rmse:.6f} - Total Rules: {anfis_model.n_rules} - Active Rules: {num_active_rules}\")\n",
        "\n",
        "        # 检查验证损失的改进情况\n",
        "        if loss_val.item() < prev_val_loss - grow_threshold:\n",
        "            no_improve_epochs = 0\n",
        "            prev_val_loss = loss_val.item()\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # 如果验证损失在连续若干个 epoch 中没有显著改进，触发规则生长\n",
        "        if no_improve_epochs >= patience and anfis_model.n_rules < max_rules:\n",
        "            print(f\"\\nEpoch {epoch+1}: No significant improvement in validation loss, growing a new rule. Current rules: {anfis_model.n_rules}\")\n",
        "            # 找出当前误差较大的数据点，用于初始化新规则\n",
        "            residuals = (y_val_scaled_tensor.cpu().numpy() - output.cpu().numpy())\n",
        "            high_error_indices = np.argsort(np.abs(residuals))[-int(0.1 * len(residuals)):]  # 选取误差最大的 10% 数据\n",
        "            X_new_rule = X_val_scaled[high_error_indices]\n",
        "            # 添加新规则并进行性能检查\n",
        "            improved = anfis_model.grow_rule_with_performance_check(\n",
        "                X_new_rule,\n",
        "                X_train_scaled,  # 传入训练集特征\n",
        "                y_train_scaled_tensor.cpu().numpy(),  # 传入训练集目标\n",
        "                X_val_scaled,\n",
        "                y_val_scaled_tensor.cpu().numpy(),\n",
        "                best_val_loss,\n",
        "                device,\n",
        "                lr,\n",
        "                grow_epochs=100  # 设定在规则生长后训练的轮次\n",
        "            )\n",
        "\n",
        "            if improved:\n",
        "                # 如果性能有所改善，更新最佳验证损失\n",
        "                prev_val_loss = best_val_loss  # 更新之前的验证损失\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "                print(\"Optimizer and scheduler re-initialized after growing a new rule.\")\n",
        "            else:\n",
        "                # 如果性能未改善，撤销规则生长操作（已在方法内完成）\n",
        "                pass\n",
        "\n",
        "            no_improve_epochs = 0  # 重置计数器\n",
        "\n",
        "        # 设置剪枝停止的 epoch 阈值\n",
        "        pruning_stop_epoch = int(epochs * 0.8)  # 在 80% 的训练过程中进行剪枝\n",
        "\n",
        "        # 每隔 prune_frequency 个 epoch 进行属性剪枝\n",
        "        if (epoch + 1) % prune_frequency == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned_dict = anfis_model.prune_attributes_per_rule(\n",
        "                threshold=prune_threshold,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01,  # 性能下降容忍度，可根据需要调整\n",
        "                best_val_loss=best_val_loss\n",
        "            )\n",
        "            training_info['pruned_attributes'].append(pruned_dict)\n",
        "            if pruned_dict:\n",
        "                print(f\"Epoch {epoch+1}: Pruned attributes per rule: {pruned_dict}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning attributes.\")\n",
        "        elif epoch >= pruning_stop_epoch and (epoch + 1) % prune_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}: Pruning has been stopped to stabilize the model structure.\")\n",
        "\n",
        "        # 每隔若干个 epoch 进行规则剪枝\n",
        "        if (epoch + 1) % 50 == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned = anfis_model.prune_rules(\n",
        "                threshold=attention_threshold_final,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01,\n",
        "                best_val_loss=best_val_loss\n",
        "            )\n",
        "            if pruned:\n",
        "                print(f\"Epoch {epoch+1}: Pruned rules. Total rules: {anfis_model.n_rules}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch and (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Rule pruning has been stopped to stabilize the model structure.\")\n",
        "\n",
        "    # 在训练结束后，加载最佳模型\n",
        "    if os.path.exists(best_model_path):\n",
        "        model_loaded, scaler_X_loaded, scaler_y_loaded = AttentionDynamicAttributeAndRuleANFIS.load_model_with_architecture(best_model_path, device=device)\n",
        "        anfis_model = model_loaded  # 更新模型为加载的最佳模型\n",
        "        scaler_X = scaler_X_loaded\n",
        "        scaler_y = scaler_y_loaded\n",
        "        print(\"\\nLoaded the best model based on validation loss.\")\n",
        "\n",
        "        # 显示训练后的隶属函数图像（可选）\n",
        "        # anfis_model.plot_membership_functions(feature_names=feature_labels)\n",
        "\n",
        "        # 绘制属性权重的热力图（可选）\n",
        "        # plot_heatmap(anfis_model, feature_labels)\n",
        "\n",
        "        # 提取并保存规则\n",
        "        rules = extract_fuzzy_rules(anfis_model, scaler_X, feature_names=feature_labels)\n",
        "        print(f\"\\n=== ADAR-ANFIS Extracted Fuzzy Rules ===\")\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "            print()\n",
        "        # # 保存规则到文件\n",
        "        # with open(f'results/rules_lr{lr}_final.txt', 'w') as f:\n",
        "        #     for rule in rules:\n",
        "        #         f.write(rule + '\\n')\n",
        "\n",
        "        # 计算总活跃属性数量\n",
        "        attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask_np)\n",
        "        print(f\"Total Number of Attributes Included in All Rules: {total_active_attributes:.2f}\")\n",
        "    else:\n",
        "        print(\"No improvement during training. Using the final model.\")\n",
        "\n",
        "        # 计算总活跃属性数量\n",
        "        attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask_np)\n",
        "        print(f\"Total Number of Attributes Included in All Rules: {total_active_attributes:.2f}\")\n",
        "\n",
        "    # 可视化训练过程\n",
        "    # 合并验证损失和规则数量曲线\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Validation Loss', color=color)\n",
        "    ax1.plot(training_info['epoch'], training_info['val_loss'], color=color, label='Validation Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()  # 共享 x 轴\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Number of Rules', color=color)\n",
        "    ax2.plot(training_info['epoch'], training_info['total_rules'], color=color, label='Total Rules', linestyle='--')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.title('Validation Loss and Number of Rules over Epochs')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return anfis_model, scaler_X, scaler_y, total_active_attributes, training_info\n",
        "\n",
        "# ============================\n",
        "# 定义实验函数\n",
        "# ============================\n",
        "\n",
        "def run_experiment(\n",
        "    X, y, feature_names, n_rules=9, learning_rates=[0.01], repeats=5\n",
        "):\n",
        "    \"\"\"\n",
        "    执行多次实验，计算 RMSE、Iov 和 Ifspe。\n",
        "\n",
        "    参数：\n",
        "    - X: 特征数据，pandas DataFrame 或 numpy array\n",
        "    - y: 目标数据，pandas Series 或 numpy array\n",
        "    - feature_names: 特征名称列表\n",
        "    - n_rules: 规则数量\n",
        "    - learning_rates: 学习率列表\n",
        "    - repeats: 每种配置的重复次数\n",
        "\n",
        "    返回：\n",
        "    - results: 实验结果的列表\n",
        "    \"\"\"\n",
        "    # 将数据拆分为训练集和测试集\n",
        "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 创建结果保存的目录\n",
        "    os.makedirs('results_sofenn', exist_ok=True)\n",
        "\n",
        "    # 记录实验结果\n",
        "    results_sofenn = []\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        test_rmse_list = []  # List to store test set RMSEs for each repeat\n",
        "        val_rmse_list = []   # List to store validation set RMSEs for each repeat\n",
        "        time_list = []\n",
        "        attribute_weights_list = []\n",
        "        overlap_indices_list = []\n",
        "        position_indices_list = []\n",
        "        total_attributes_list_experiment = []\n",
        "        print(f\"\\nStarting experiments for n_rules={n_rules}, learning_rate={lr}\")\n",
        "        for repeat in range(repeats):\n",
        "            # Record start time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Further split the training set into training and validation sets\n",
        "            X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "                X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "            )\n",
        "\n",
        "            # Define a unique path for saving the best model for this repeat\n",
        "            best_model_path = f'results_sofenn/best_model_nrules{n_rules}_lr{lr}_repeat{repeat+1}.pth'\n",
        "\n",
        "            # Train the model\n",
        "            anfis_model, scaler_X, scaler_y, total_active_attributes, training_info = train_attention_dynamic_attribute_and_rule_anfis(\n",
        "                X_train_sub, y_train_sub, X_test_np, y_test_np,\n",
        "                initial_n_rules=n_rules,\n",
        "                epochs=1500,\n",
        "                batch_size=512,\n",
        "                lr=lr,\n",
        "                prune_frequency=25,\n",
        "                prune_threshold=0.1,\n",
        "                best_model_path=best_model_path  # 传递最佳模型保存路径\n",
        "            )\n",
        "\n",
        "            # 加载最佳模型的状态字典（确保使用的是最佳模型）\n",
        "            anfis_model.eval()  # 设置为评估模式\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            anfis_model.to(device)\n",
        "\n",
        "            # Test the model on the test set using the best model\n",
        "            X_test_scaled = scaler_X.transform(X_test_np)  # 使用 X_test_np 作为测试集\n",
        "            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "            y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "            with torch.no_grad():\n",
        "                y_pred_scaled = anfis_model.infer(X_test_tensor)\n",
        "                y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                y_true = y_test_np  # Original unstandardized y_test\n",
        "                test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "                test_rmse_list.append(test_rmse)\n",
        "\n",
        "            # Compute RMSE on the validation set\n",
        "            X_val_scaled = scaler_X.transform(X_val_sub)\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "            y_val_tensor = torch.tensor(y_val_sub, dtype=torch.float32).to(device)\n",
        "            with torch.no_grad():\n",
        "                y_val_pred_scaled = anfis_model.infer(X_val_tensor)\n",
        "                y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                y_val_true = y_val_sub  # 使用原始的 y_val_sub 作为真实值\n",
        "                val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "                val_rmse_list.append(val_rmse)\n",
        "\n",
        "            # Record end time\n",
        "            end_time = time.time()\n",
        "            time_taken = end_time - start_time\n",
        "            time_list.append(time_taken)\n",
        "\n",
        "            # Extract attribute weights\n",
        "            attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "            avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "            attribute_weights_list.append(avg_attribute_weights)\n",
        "\n",
        "            # Compute interpretability indices\n",
        "            overlap_index = compute_iov(anfis_model)\n",
        "            position_index = compute_ifspe(anfis_model)\n",
        "            overlap_indices_list.append(overlap_index)\n",
        "            position_indices_list.append(position_index)\n",
        "\n",
        "            # Collect total active attributes from best model\n",
        "            total_attributes_list_experiment.append(total_active_attributes)\n",
        "\n",
        "            print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Val RMSE={val_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "            # Extract and save fuzzy rules\n",
        "            rules = extract_fuzzy_rules(anfis_model, scaler_X, feature_names=feature_names)\n",
        "            print(f\"\\n=== Fuzzy Rules for Repeat {repeat+1} ===\")\n",
        "            for rule in rules:\n",
        "                print(rule)\n",
        "                print()\n",
        "            # # Save rules to file\n",
        "            # with open(f'results_sofenn/rules_nrules{n_rules}_lr{lr}_repeat{repeat+1}.txt', 'w') as f:\n",
        "            #     for rule in rules:\n",
        "            #         f.write(rule + '\\n')\n",
        "\n",
        "        # Compute RMSE mean and std for test set and validation set\n",
        "        test_rmse_mean = np.mean(test_rmse_list)\n",
        "        test_rmse_std = np.std(test_rmse_list)\n",
        "        val_rmse_mean = np.mean(val_rmse_list)\n",
        "        val_rmse_std = np.std(val_rmse_list)\n",
        "\n",
        "        # Compute average attribute weights over repeats\n",
        "        avg_attribute_weights_over_repeats = np.mean(attribute_weights_list, axis=0)\n",
        "\n",
        "        # Compute average total number of attributes included in all rules\n",
        "        average_total_attributes = np.mean(total_attributes_list_experiment)\n",
        "\n",
        "        # Compute average interpretability indices\n",
        "        avg_overlap_index = np.mean(overlap_indices_list)\n",
        "        avg_position_index = np.mean(position_indices_list)\n",
        "        std_overlap_index = np.std(overlap_indices_list)\n",
        "        std_position_index = np.std(position_indices_list)\n",
        "        # Print the results\n",
        "        print(f\"\\nResults for n_rules={n_rules}, learning_rate={lr}:\")\n",
        "        print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "        print(f\"Validation RMSE: {val_rmse_mean:.4f} ± {val_rmse_std:.4f}\")\n",
        "        print(f\"Time: {np.mean(time_list):.2f}s ± {np.std(time_list):.2f}s\")\n",
        "        print(f\"Average Overlap Index (Iov): {avg_overlap_index:.4f} ± {std_overlap_index:.4f}\")\n",
        "        print(f\"Average Fuzzy Set Position Index (Ifspe): {avg_position_index:.4f} ± {std_position_index:.4f} \")\n",
        "        print(f\"Average Attribute Weights over Repeats: {avg_attribute_weights_over_repeats}\")\n",
        "        print(f\"Average Total Number of Attributes Included in All Rules: {average_total_attributes:.2f}\")\n",
        "\n",
        "        # Save results\n",
        "        result = {\n",
        "            'n_rules': n_rules,\n",
        "            'learning_rate': lr,\n",
        "            'test_rmse_mean': test_rmse_mean,\n",
        "            'test_rmse_std': test_rmse_std,\n",
        "            'val_rmse_mean': val_rmse_mean,\n",
        "            'val_rmse_std': val_rmse_std,\n",
        "            'time_mean': np.mean(time_list),\n",
        "            'time_std': np.std(time_list),\n",
        "            'attribute_weights': avg_attribute_weights_over_repeats,\n",
        "            'overlap_index': avg_overlap_index,\n",
        "            'position_index': avg_position_index,\n",
        "            'average_total_attributes': average_total_attributes\n",
        "        }\n",
        "        results_sofenn.append(result)\n",
        "\n",
        "        # Save result data\n",
        "        np.save(f'results_sofenn/attribute_weights_nrules{n_rules}_lr{lr}.npy', avg_attribute_weights_over_repeats)\n",
        "        np.save(f'results_sofenn/overlap_index_nrules{n_rules}_lr{lr}.npy', avg_overlap_index)\n",
        "        np.save(f'results_sofenn/position_index_nrules{n_rules}_lr{lr}.npy', avg_position_index)\n",
        "        np.save(f'results_sofenn/average_total_attributes_nrules{n_rules}_lr{lr}.npy', average_total_attributes)\n",
        "\n",
        "        # 可视化重叠指数和位置指数\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(['Overlap Index (Iov)', 'Position Index (Ifspe)'], [avg_overlap_index, avg_position_index], color=['skyblue', 'salmon'])\n",
        "        plt.ylabel('Index Value')\n",
        "        plt.title('Average Interpretability Indices')\n",
        "        plt.grid(axis='y')\n",
        "        plt.show()\n",
        "\n",
        "        # 可视化平均总属性数量\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(['Average Total Attributes'], [average_total_attributes], color=['lightgreen'])\n",
        "        plt.ylabel('Number of Attributes')\n",
        "        plt.title('Average Total Number of Attributes Included in All Rules')\n",
        "        plt.grid(axis='y')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 将结果保存为 CSV 文件\n",
        "    results_df = pd.DataFrame(results_sofenn)\n",
        "    results_df.to_csv('results_sofenn/experiment_results_boston.csv', index=False)\n",
        "    print(\"\\n所有实验结果已保存到 'results_sofenn/experiment_results_boston.csv' 文件中。\")\n",
        "\n",
        "    return results_sofenn\n",
        "\n",
        "# ============================\n",
        "# 执行实验\n",
        "# ============================\n",
        "\n",
        "# 将训练集进一步拆分为训练和验证集\n",
        "X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "    X_train_np, y_train_np, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 设置实验参数\n",
        "n_rules = 3\n",
        "learning_rates = [0.01]\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 运行实验\n",
        "results = run_experiment(\n",
        "    X=X_train_np,\n",
        "    y=y_train_np,\n",
        "    feature_names=feature_labels,\n",
        "    n_rules=n_rules,\n",
        "    learning_rates=learning_rates,\n",
        "    repeats=repeats\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahDynRgqoInL"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# from interpret.glassbox import ExplainableBoostingRegressor, APLRRegressor\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "\n",
        "# 从 UCI ML Repo 下载 Appliances Energy Prediction 数据集\n",
        "energy_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv'\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(energy_url)\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'date' 和 'Appliances'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4',\n",
        "    'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9',\n",
        "    'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
        "    'Tdewpoint', 'rv1', 'rv2'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# feature_names = features_to_use\n",
        "\n",
        "# ============================\n",
        "# 定义 ADAR-ANFIS 模型\n",
        "# ============================\n",
        "\n",
        "class AttentionDynamicAttributeAndRuleANFIS(nn.Module):\n",
        "    def __init__(self, n_inputs, n_rules, X_train=None, attention_threshold=0.1, init_from_checkpoint=False):\n",
        "        super(AttentionDynamicAttributeAndRuleANFIS, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.attention_threshold = attention_threshold  # 属性注意力阈值\n",
        "        self.n_rules = n_rules  # 初始规则数量\n",
        "\n",
        "        if not init_from_checkpoint:\n",
        "            if X_train is None:\n",
        "                raise ValueError(\"X_train must be provided for initializing rules.\")\n",
        "\n",
        "            # 使用 KMeans 聚类初始化隶属函数中心\n",
        "            kmeans = KMeans(n_clusters=n_rules, random_state=42)\n",
        "            kmeans.fit(X_train)\n",
        "            cluster_centers = kmeans.cluster_centers_  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "            # 初始化隶属函数参数\n",
        "            self.mu = nn.Parameter(torch.tensor(cluster_centers, dtype=torch.float32))  # 均值，形状：(n_rules, n_inputs)\n",
        "            self.sigma = nn.Parameter(torch.ones(n_rules, n_inputs))  # 标准差\n",
        "\n",
        "            # 初始化属性注意力权重参数\n",
        "            self.attention_weights = nn.Parameter(torch.randn(n_rules, n_inputs))\n",
        "\n",
        "            # 初始化规则注意力权重参数\n",
        "            self.rule_attention_weights = nn.Parameter(torch.ones(n_rules))\n",
        "\n",
        "            # 初始化后件参数（对于回归任务）\n",
        "            self.consequents = nn.Parameter(torch.randn(n_rules, n_inputs))\n",
        "\n",
        "            # 初始化属性掩码（1表示活跃，0表示被剪除），针对每个规则\n",
        "            device = self.mu.device  # 获取设备\n",
        "            self.register_buffer('attribute_mask', torch.ones(n_rules, n_inputs, device=device))\n",
        "        else:\n",
        "            # 初始化占位参数，实际参数将在加载后设置\n",
        "            self.mu = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.sigma = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.attention_weights = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.rule_attention_weights = nn.Parameter(torch.empty(n_rules))\n",
        "            self.consequents = nn.Parameter(torch.empty(n_rules, n_inputs))\n",
        "            self.register_buffer('attribute_mask', torch.empty(n_rules, n_inputs))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 将被剪枝的属性的 attention_weights 设置为一个大负数\n",
        "        masked_attention_weights = self.attention_weights.clone()\n",
        "        masked_attention_weights[self.attribute_mask == 0] = -1e6\n",
        "\n",
        "        # 计算属性注意力权重（使用 sigmoid 激活函数并应用属性掩码）\n",
        "        attention = torch.sigmoid(masked_attention_weights) * self.attribute_mask  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "        # 计算规则注意力权重（使用 sigmoid 激活函数）\n",
        "        rule_attention = torch.sigmoid(self.rule_attention_weights)  # 形状：(n_rules,)\n",
        "\n",
        "        # 扩展维度以匹配批次大小\n",
        "        x_expanded = x.unsqueeze(1)  # 形状：(batch_size, 1, n_inputs)\n",
        "        mu_expanded = self.mu.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "        sigma_expanded = self.sigma.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "\n",
        "        # 确保 sigma 为正数，避免除以零\n",
        "        sigma_expanded = torch.clamp(sigma_expanded, min=1e-3)\n",
        "\n",
        "        # 计算高斯隶属度函数的对数\n",
        "        log_gauss = -0.5 * ((x_expanded - mu_expanded) ** 2) / (sigma_expanded ** 2)\n",
        "\n",
        "        # 使用属性注意力权重并应用属性掩码\n",
        "        log_gauss_weighted = log_gauss * attention.unsqueeze(0)  # 形状：(batch_size, n_rules, n_inputs)\n",
        "\n",
        "        # 对输入属性维度求和\n",
        "        sum_log_gauss = log_gauss_weighted.sum(dim=2)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算规则的激活度\n",
        "        firing_strength = torch.exp(sum_log_gauss)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 使用规则注意力权重调整规则的激活度\n",
        "        firing_strength_weighted = firing_strength * rule_attention.unsqueeze(0)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算归一化的激活度\n",
        "        sum_firing_strength = firing_strength_weighted.sum(dim=1, keepdim=True) + 1e-8\n",
        "        norm_firing_strength = firing_strength_weighted / sum_firing_strength  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算后件部分（使用属性注意力权重）\n",
        "        consequents_weighted = self.consequents * attention  # 形状：(n_rules, n_inputs)\n",
        "        consequents_weighted_expanded = consequents_weighted.unsqueeze(0)  # 形状：(1, n_rules, n_inputs)\n",
        "\n",
        "        # 计算规则的输出（对于每个规则，后件为被选中属性的线性组合）\n",
        "        rule_outputs = torch.sum(consequents_weighted_expanded * x_expanded, dim=2)  # 形状：(batch_size, n_rules)\n",
        "\n",
        "        # 计算总输出\n",
        "        output = torch.sum(norm_firing_strength * rule_outputs, dim=1)  # 形状：(batch_size,)\n",
        "\n",
        "        return output, firing_strength, attention, rule_attention\n",
        "\n",
        "    def train_step(self, x, target, optimizer, lambda_attention=1e-7, lambda_rule_attention=1e-8, lambda_diversity=1e-4):\n",
        "        \"\"\"\n",
        "        执行一次训练步骤。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, n_inputs)\n",
        "        - target: 目标数据，形状：(batch_size,)\n",
        "        - optimizer: 优化器实例\n",
        "        - lambda_attention: 属性注意力权重的正则化系数\n",
        "        - lambda_rule_attention: 规则注意力权重的正则化系数\n",
        "        - lambda_diversity: 多样性正则化的系数\n",
        "\n",
        "        返回：\n",
        "        - loss.item(): 当前批次的总损失\n",
        "        - output: 模型的输出\n",
        "        \"\"\"\n",
        "        self.train()\n",
        "        optimizer.zero_grad()\n",
        "        output, firing_strength, attention, rule_attention = self.forward(x)\n",
        "        # 计算预测损失（均方误差损失）\n",
        "        loss_pred = F.mse_loss(output, target)\n",
        "\n",
        "        # 添加属性注意力正则化损失（L1 正则化）\n",
        "        loss_attention = lambda_attention * attention.abs().sum()\n",
        "\n",
        "        # 计算规则注意力正则化损失（L1 正则化）\n",
        "        loss_rule_attention = lambda_rule_attention * rule_attention.abs().sum()\n",
        "\n",
        "        # 添加多样性正则化损失（鼓励不同规则的注意力权重不同）\n",
        "        if self.n_rules > 1:\n",
        "            # 计算注意力权重的余弦相似度矩阵\n",
        "            attention_norm = attention / (attention.norm(dim=1, keepdim=True) + 1e-8)\n",
        "            similarity_matrix = torch.matmul(attention_norm, attention_norm.t())\n",
        "            # 计算非对角线的平均相似度\n",
        "            diversity_loss = torch.sum(similarity_matrix) - torch.diag(similarity_matrix).sum()\n",
        "            diversity_loss = diversity_loss / (self.n_rules * (self.n_rules - 1))\n",
        "        else:\n",
        "            diversity_loss = torch.tensor(0.0).to(attention.device)\n",
        "\n",
        "        loss_diversity = lambda_diversity * diversity_loss\n",
        "\n",
        "        # 总损失\n",
        "        loss = loss_pred + loss_attention + loss_rule_attention + loss_diversity\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Loss is NaN. Stopping training.\")\n",
        "            return loss.item(), output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), output\n",
        "\n",
        "    def prune_attributes_per_rule(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除每个规则中注意力权重低于阈值的属性，并冻结其相关参数。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则不执行剪枝。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认为0.1\n",
        "        - X_val: 验证集特征，形状：(num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状：(num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "        - best_val_loss: 之前的最佳验证损失\n",
        "\n",
        "        返回：\n",
        "        - pruned_dict: 字典，键为规则索引，值为被剪除的属性索引列表\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self)\n",
        "\n",
        "        # 执行剪枝操作\n",
        "        pruned_dict = {}\n",
        "        with torch.no_grad():\n",
        "            attention = torch.sigmoid(model_copy.attention_weights)  # 形状：(n_rules, n_inputs)\n",
        "\n",
        "            for rule_idx in range(model_copy.n_rules):\n",
        "                if torch.all(model_copy.attribute_mask[rule_idx] == 0):\n",
        "                    continue  # 跳过已被完全剪除的规则\n",
        "\n",
        "                prune_indices = torch.where((attention[rule_idx] < threshold) & (model_copy.attribute_mask[rule_idx] == 1))[0].tolist()\n",
        "\n",
        "                if prune_indices:\n",
        "                    # 更新属性掩码\n",
        "                    model_copy.attribute_mask[rule_idx, prune_indices] = 0.0\n",
        "\n",
        "                    # 冻结被剪除属性的相关参数\n",
        "                    model_copy.attention_weights[rule_idx, prune_indices].requires_grad = False\n",
        "                    model_copy.consequents[rule_idx, prune_indices].requires_grad = False\n",
        "\n",
        "                    pruned_dict[rule_idx] = prune_indices\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after, _, _, _ = model_copy.forward(X_val)\n",
        "            loss_after = F.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - best_val_loss) / best_val_loss\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，不执行剪枝\n",
        "            print(f\"Attribute pruning was not performed due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned_dict = {}  # 清空剪枝记录\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能未下降，更新原始模型的参数\n",
        "            self.attribute_mask = model_copy.attribute_mask.clone()\n",
        "            self.attention_weights.data = model_copy.attention_weights.data.clone()\n",
        "            self.consequents.data = model_copy.consequents.data.clone()\n",
        "            print(f\"Attribute pruning successful. Performance drop: {performance_drop * 100:.2f}%\")\n",
        "            pruned = True\n",
        "\n",
        "        return pruned_dict\n",
        "\n",
        "    def prune_rules_with_recovery(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则不执行剪枝。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认值为0.1\n",
        "        - X_val: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状为 (num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "        - best_val_loss: 之前的最佳验证损失\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self)\n",
        "\n",
        "        # 执行规则剪枝操作\n",
        "        pruned = False\n",
        "        with torch.no_grad():\n",
        "            # 获取规则注意力权重\n",
        "            rule_attention = torch.sigmoid(model_copy.rule_attention_weights)\n",
        "            # 找到需要移除的规则索引（rule_attention < threshold）\n",
        "            low_attention_indices = torch.where(rule_attention < threshold)[0]\n",
        "\n",
        "            if len(low_attention_indices) == 0:\n",
        "                return pruned  # 没有需要移除的规则\n",
        "\n",
        "            # 保留的规则索引（rule_attention >= threshold）\n",
        "            keep_indices = torch.where(rule_attention >= threshold)[0]\n",
        "\n",
        "            # 更新模型参数，移除低重要性的规则\n",
        "            model_copy.mu = nn.Parameter(model_copy.mu.data[keep_indices])\n",
        "            model_copy.sigma = nn.Parameter(model_copy.sigma.data[keep_indices])\n",
        "            model_copy.attention_weights = nn.Parameter(model_copy.attention_weights.data[keep_indices])\n",
        "            model_copy.rule_attention_weights = nn.Parameter(model_copy.rule_attention_weights.data[keep_indices])\n",
        "            model_copy.consequents = nn.Parameter(model_copy.consequents.data[keep_indices])\n",
        "            model_copy.attribute_mask = model_copy.attribute_mask.data[keep_indices].clone()\n",
        "\n",
        "            # 更新规则数量\n",
        "            model_copy.n_rules = len(keep_indices)\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after, _, _, _ = model_copy.forward(X_val)\n",
        "            loss_after = F.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - best_val_loss) / best_val_loss\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，不执行剪枝\n",
        "            print(f\"Rule pruning was not performed due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能未下降，更新原始模型的参数\n",
        "            self.mu = nn.Parameter(model_copy.mu.data.clone())\n",
        "            self.sigma = nn.Parameter(model_copy.sigma.data.clone())\n",
        "            self.attention_weights = nn.Parameter(model_copy.attention_weights.data.clone())\n",
        "            self.rule_attention_weights = nn.Parameter(model_copy.rule_attention_weights.data.clone())\n",
        "            self.consequents = nn.Parameter(model_copy.consequents.data.clone())\n",
        "            self.attribute_mask = model_copy.attribute_mask.clone()\n",
        "            self.n_rules = model_copy.n_rules  # 更新规则数量\n",
        "            print(f\"Rules pruned successfully. Performance drop: {performance_drop * 100:.2f}%\")\n",
        "            pruned = True\n",
        "\n",
        "        return pruned\n",
        "\n",
        "    def prune_rules(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则撤销剪枝操作。\n",
        "\n",
        "        参数：\n",
        "        - threshold: 剪枝阈值，默认值为0.1\n",
        "        - X_val: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状为 (num_val_samples,)\n",
        "        - performance_drop_tolerance: 性能下降容忍度，默认为0.01（即1%）\n",
        "        - best_val_loss: 之前的最佳验证损失\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        return self.prune_rules_with_recovery(threshold, X_val, y_val, performance_drop_tolerance, best_val_loss)\n",
        "\n",
        "    def grow_rule_with_performance_check(self, X_new, X_train, y_train, X_val, y_val, best_val_loss, device, lr, grow_epochs=10):\n",
        "        \"\"\"\n",
        "        添加一个新的规则，并进行性能检查。如果性能没有提升，则撤销规则生长。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, n_inputs)\n",
        "        - X_train: 训练集特征，形状：(num_train_samples, n_inputs)\n",
        "        - y_train: 训练集目标，形状：(num_train_samples,)\n",
        "        - X_val: 验证集特征，形状：(num_val_samples, n_inputs)\n",
        "        - y_val: 验证集目标，形状：(num_val_samples,)\n",
        "        - best_val_loss: 当前最佳验证损失\n",
        "        - device: 设备\n",
        "        - lr: 学习率\n",
        "        - grow_epochs: 在规则生长后训练的 epoch 数，默认值为10\n",
        "\n",
        "        返回：\n",
        "        - improved: 布尔值，指示是否保留了新规则\n",
        "        \"\"\"\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self).to(device)\n",
        "\n",
        "        # 添加新规则到副本\n",
        "        model_copy.grow_rule(X_new)\n",
        "\n",
        "        # 初始化优化器和调度器\n",
        "        optimizer_copy = optim.AdamW(model_copy.parameters(), lr=lr)\n",
        "        scheduler_copy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_copy, T_max=grow_epochs)\n",
        "\n",
        "        # 将训练数据转换为张量\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
        "\n",
        "        # 训练模型副本\n",
        "        for epoch in range(grow_epochs):\n",
        "            model_copy.train()\n",
        "            optimizer_copy.zero_grad()\n",
        "            output, _, _, _ = model_copy.forward(X_train_tensor)\n",
        "            loss = F.mse_loss(output, y_train_tensor)\n",
        "            loss.backward()\n",
        "            optimizer_copy.step()\n",
        "            scheduler_copy.step()\n",
        "\n",
        "        # 评估副本模型的性能\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after, _, _, _ = model_copy.forward(X_val_tensor)\n",
        "            loss_after = F.mse_loss(output_after, y_val_tensor).item()\n",
        "\n",
        "        print(f\"After growing rule and training for {grow_epochs} epochs: Validation Loss = {loss_after:.4f}\")\n",
        "\n",
        "        if loss_after < best_val_loss:\n",
        "            # 性能有所提升，保留新规则\n",
        "            # 手动更新原始模型的参数\n",
        "            self.mu = nn.Parameter(model_copy.mu.data.clone())\n",
        "            self.sigma = nn.Parameter(model_copy.sigma.data.clone())\n",
        "            self.attention_weights = nn.Parameter(model_copy.attention_weights.data.clone())\n",
        "            self.rule_attention_weights = nn.Parameter(model_copy.rule_attention_weights.data.clone())\n",
        "            self.consequents = nn.Parameter(model_copy.consequents.data.clone())\n",
        "            self.attribute_mask = model_copy.attribute_mask.clone()\n",
        "            self.n_rules = model_copy.n_rules  # 更新规则数量\n",
        "            print(\"Performance improved after growing rule. New rule retained.\")\n",
        "            return True\n",
        "        else:\n",
        "            # 性能未提升，丢弃副本，保持主模型不变\n",
        "            print(\"Performance did not improve after growing rule. Rule growth reverted.\")\n",
        "            return False\n",
        "\n",
        "    def grow_rule(self, X_new):\n",
        "        \"\"\"\n",
        "        添加一个新的规则。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, n_inputs)\n",
        "        \"\"\"\n",
        "        # 获取设备和数据类型\n",
        "        device = self.mu.device\n",
        "        dtype = self.mu.dtype\n",
        "\n",
        "        # 使用 X_new 计算新的规则中心和标准差\n",
        "        new_mu = torch.tensor(X_new.mean(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, n_inputs)\n",
        "        new_sigma = torch.tensor(X_new.std(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, n_inputs)\n",
        "\n",
        "        # 计算现有规则的属性注意力权重的平均值\n",
        "        if self.n_rules > 0:\n",
        "            existing_attention_weights = torch.sigmoid(self.attention_weights).data  # (n_rules, n_inputs)\n",
        "            attention_mean = existing_attention_weights.mean(dim=0, keepdim=True)  # (1, n_inputs)\n",
        "        else:\n",
        "            attention_mean = torch.ones(1, self.n_inputs, dtype=dtype).to(device)  # 初始化为1\n",
        "\n",
        "        # 将新规则的属性注意力权重初始化为平均值并加入随机扰动\n",
        "        noise = torch.randn_like(attention_mean) * 0.05  # 调整扰动大小以控制多样性\n",
        "        new_attention_weights = (attention_mean + noise).clamp(0, 1).detach()  # 保持在[0,1]范围内\n",
        "\n",
        "        # 将新规则的规则注意力权重初始化为与现有权重的均值 logit 相同，并加入随机扰动\n",
        "        if self.n_rules > 0:\n",
        "            existing_rule_attention_logits = self.rule_attention_weights.data  # (n_rules,)\n",
        "            rule_attention_mean_logit = existing_rule_attention_logits.mean().unsqueeze(0)  # (1,)\n",
        "            rule_attention_noise = torch.randn_like(rule_attention_mean_logit) * 0.05  # 调整扰动大小\n",
        "            new_rule_attention_weight = (rule_attention_mean_logit + rule_attention_noise).detach()\n",
        "        else:\n",
        "            rule_attention_mean_logit = torch.tensor([0.0], dtype=dtype).to(device)  # 中性 logit\n",
        "            new_rule_attention_weight = rule_attention_mean_logit.clone().detach()  # (1,)\n",
        "\n",
        "        # 初始化后件参数为小的随机值\n",
        "        new_consequents = torch.randn(1, self.n_inputs, dtype=dtype).to(device) * 0.01  # (1, n_inputs)\n",
        "\n",
        "        # 将新的参数添加到模型中\n",
        "        self.mu = nn.Parameter(torch.cat([self.mu.data, new_mu], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.sigma = nn.Parameter(torch.cat([self.sigma.data, new_sigma], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.attention_weights = nn.Parameter(torch.cat([self.attention_weights.data, new_attention_weights], dim=0))  # (n_rules + 1, n_inputs)\n",
        "        self.rule_attention_weights = nn.Parameter(torch.cat([self.rule_attention_weights.data, new_rule_attention_weight], dim=0))  # (n_rules + 1,)\n",
        "        self.consequents = nn.Parameter(torch.cat([self.consequents.data, new_consequents], dim=0))  # (n_rules + 1, n_inputs)\n",
        "\n",
        "        # 更新属性掩码，添加新规则的掩码行\n",
        "        new_attribute_mask = torch.ones(1, self.n_inputs, dtype=self.attribute_mask.dtype).to(device)  # (1, n_inputs)\n",
        "        self.attribute_mask = torch.cat([self.attribute_mask, new_attribute_mask], dim=0)  # (n_rules + 1, n_inputs)\n",
        "\n",
        "        # 更新规则数量\n",
        "        self.n_rules += 1\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After growing, attribute_mask has shape {self.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"New rule added. Total rules: {self.n_rules}\")\n",
        "\n",
        "    def infer(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        执行推理。\n",
        "\n",
        "        参数：\n",
        "        - x: 输入数据，形状：(batch_size, n_inputs)\n",
        "        - targets: 目标数据，形状：(batch_size,)，可选\n",
        "\n",
        "        返回：\n",
        "        - 如果 targets 为 None，返回模型输出。\n",
        "        - 否则，返回模型输出和损失值。\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            output, _, _, _ = self.forward(x)\n",
        "            if targets is None:\n",
        "                return output\n",
        "            else:\n",
        "                loss = F.mse_loss(output, targets)\n",
        "                return output, loss.item()\n",
        "\n",
        "    def save_model_with_architecture(self, scaler_X, scaler_y, path):\n",
        "        \"\"\"\n",
        "        保存模型的状态字典和架构信息。\n",
        "\n",
        "        参数：\n",
        "        - scaler_X: 输入数据的标准化器\n",
        "        - scaler_y: 输出数据的标准化器\n",
        "        - path: 保存路径\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'n_rules': self.n_rules,\n",
        "            'mu': self.mu.detach().cpu().numpy(),\n",
        "            'sigma': self.sigma.detach().cpu().numpy(),\n",
        "            'attention_weights': self.attention_weights.detach().cpu().numpy(),\n",
        "            'rule_attention_weights': self.rule_attention_weights.detach().cpu().numpy(),\n",
        "            'consequents': self.consequents.detach().cpu().numpy(),\n",
        "            'attribute_mask': self.attribute_mask.detach().cpu().numpy(),\n",
        "            'scaler_X_mean': scaler_X.mean_,\n",
        "            'scaler_X_scale': scaler_X.scale_,\n",
        "            'scaler_y_mean': scaler_y.mean_,\n",
        "            'scaler_y_scale': scaler_y.scale_\n",
        "        }, path)\n",
        "        print(f\"Model and architecture saved to {path}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model_with_architecture(path, device='cpu'):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        n_rules = checkpoint['n_rules']\n",
        "        n_inputs = checkpoint['mu'].shape[1]\n",
        "\n",
        "        # 初始化模型，设置 init_from_checkpoint=True 以跳过 KMeans 初始化\n",
        "        model = AttentionDynamicAttributeAndRuleANFIS(\n",
        "            n_inputs=n_inputs,\n",
        "            n_rules=n_rules,\n",
        "            X_train=None,  # 跳过 KMeans 初始化\n",
        "            attention_threshold=0.1,\n",
        "            init_from_checkpoint=True\n",
        "        ).to(device)\n",
        "\n",
        "        # 手动设置参数\n",
        "        with torch.no_grad():\n",
        "            model.mu = nn.Parameter(torch.tensor(checkpoint['mu'], dtype=torch.float32, device=device))\n",
        "            model.sigma = nn.Parameter(torch.tensor(checkpoint['sigma'], dtype=torch.float32, device=device))\n",
        "            model.attention_weights = nn.Parameter(torch.tensor(checkpoint['attention_weights'], dtype=torch.float32, device=device))\n",
        "            model.rule_attention_weights = nn.Parameter(torch.tensor(checkpoint['rule_attention_weights'], dtype=torch.float32, device=device))\n",
        "            model.consequents = nn.Parameter(torch.tensor(checkpoint['consequents'], dtype=torch.float32, device=device))\n",
        "\n",
        "            # 使用 copy_ 复制到 attribute_mask buffer，确保在正确的设备上\n",
        "            model.attribute_mask.copy_(torch.tensor(checkpoint['attribute_mask'], dtype=torch.float32, device=device))\n",
        "\n",
        "\n",
        "        # 加载标准化器参数\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_X.mean_ = checkpoint['scaler_X_mean']\n",
        "        scaler_X.scale_ = checkpoint['scaler_X_scale']\n",
        "\n",
        "        scaler_y = StandardScaler()\n",
        "        scaler_y.mean_ = checkpoint['scaler_y_mean']\n",
        "        scaler_y.scale_ = checkpoint['scaler_y_scale']\n",
        "\n",
        "        print(f\"Model and architecture loaded from {path} with n_rules={n_rules}\")\n",
        "\n",
        "        return model, scaler_X, scaler_y\n",
        "\n",
        "    def plot_membership_functions(self, feature_names=None):\n",
        "        \"\"\"\n",
        "        绘制训练后的隶属函数图像。\n",
        "\n",
        "        参数：\n",
        "        - feature_names: 特征名称列表，默认为 None。\n",
        "        \"\"\"\n",
        "        mus = self.mu.detach().cpu().numpy()\n",
        "        sigmas = self.sigma.detach().cpu().numpy()\n",
        "        attentions = torch.sigmoid(self.attention_weights).detach().cpu().numpy()\n",
        "        rule_attentions = torch.sigmoid(self.rule_attention_weights).detach().cpu().numpy()\n",
        "        xn = np.linspace(-3, 3, 1000)\n",
        "\n",
        "        n_inputs = self.n_inputs\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "        for r in range(self.n_rules):\n",
        "            rule_attention_value = rule_attentions[r]\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.title(f\"Rule {r + 1}, Rule Attention: {rule_attention_value:.4f}\")\n",
        "            for j in range(n_inputs):\n",
        "                # 使用 attribute_mask 确认是否为活跃属性\n",
        "                if self.attribute_mask[r, j] == 0:\n",
        "                    continue  # 跳过被剪除的属性\n",
        "                attention_value = attentions[r, j]\n",
        "                c_val = mus[r, j]\n",
        "                sigma_val = sigmas[r, j]\n",
        "                # 绘制带有注意力权重的隶属函数\n",
        "                y = np.exp(-0.5 * ((xn - c_val) ** 2) / (sigma_val ** 2 + 1e-8))\n",
        "                plt.plot(xn, y, label=f\"{feature_names[j]} (Attn: {attention_value:.4f})\")\n",
        "            plt.legend()\n",
        "            plt.xlabel('Input')\n",
        "            plt.ylabel('Membership degree')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# ============================\n",
        "# 定义辅助函数\n",
        "# ============================\n",
        "\n",
        "def extract_fuzzy_rules(anfis_model, scaler_X, feature_names=None):\n",
        "    \"\"\"\n",
        "    提取 ANFIS 模型的模糊规则，包含所有用于计算的权重。\n",
        "\n",
        "    参数：\n",
        "    - anfis_model: 训练好的 ANFIS 模型\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - feature_names: 特征名称列表\n",
        "\n",
        "    返回：\n",
        "    - rules: 包含规则字符串的列表\n",
        "    \"\"\"\n",
        "    # 获取模型的参数\n",
        "    mu = anfis_model.mu.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    sigma = anfis_model.sigma.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "    attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "    rule_attention_weights = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "    consequents = anfis_model.consequents.detach().cpu().numpy()\n",
        "\n",
        "    # 反标准化 mu 和 sigma\n",
        "    c_orig = mu * scaler_X.scale_ + scaler_X.mean_  # (n_rules, n_inputs)\n",
        "    sigma_orig = sigma * scaler_X.scale_  # (n_rules, n_inputs)\n",
        "\n",
        "    # 获取属性掩码\n",
        "    attribute_mask = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "\n",
        "    # 如果未提供特征名称，使用默认名称\n",
        "    input_dim = c_orig.shape[1]\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'Input {i+1}' for i in range(input_dim)]\n",
        "\n",
        "    rules = []\n",
        "    n_rules = c_orig.shape[0]\n",
        "\n",
        "    for i in range(n_rules):\n",
        "        # 包含规则注意力权重\n",
        "        rule_str = (f\"Rule {i+1} (Rule Attention: \"\n",
        "                    f\"{rule_attention_weights[i]:.4f}): IF \")\n",
        "        antecedent = []\n",
        "        for j in range(input_dim):\n",
        "            if attribute_mask[i, j] == 0:\n",
        "                continue  # 忽略被剪枝的属性\n",
        "            attention_value = attention_weights[i, j]\n",
        "            c_val = c_orig[i, j]\n",
        "            sigma_val = sigma_orig[i, j]\n",
        "            antecedent.append(\n",
        "                f\"[{feature_names[j]} (Attn: {attention_value:.4f}) \"\n",
        "                f\"is Gaussian(c={c_val:.4f}, σ={sigma_val:.4f})]\"\n",
        "            )\n",
        "        antecedent_str = \" AND \".join(antecedent) if antecedent else \"True\"\n",
        "        rule_str += antecedent_str + \" THEN Output = \"\n",
        "\n",
        "        consequent_terms = []\n",
        "        for j in range(input_dim):\n",
        "            if attribute_mask[i, j] == 0:\n",
        "                continue\n",
        "            attention_value = attention_weights[i, j]\n",
        "            coef = consequents[i, j]\n",
        "            consequent_terms.append(\n",
        "                f\"({coef:.4f} * {feature_names[j]} \"\n",
        "                f\"(Attn: {attention_value:.4f}))\"\n",
        "            )\n",
        "        consequent_str = \" + \".join(consequent_terms) if consequent_terms else \"0\"\n",
        "        rule_str += consequent_str\n",
        "        rules.append(rule_str)\n",
        "\n",
        "    return rules\n",
        "\n",
        "def compute_overlap_index(mus, sigmas):\n",
        "    \"\"\"\n",
        "    计算模糊集之间的重叠指数。\n",
        "\n",
        "    参数：\n",
        "    - mus: 形状为 (n_rules, n_inputs) 的数组，表示隶属函数的中心。\n",
        "    - sigmas: 形状为 (n_rules, n_inputs) 的数组，表示隶属函数的标准差。\n",
        "\n",
        "    返回：\n",
        "    - overlap_index: 浮点数，表示所有输入特征上的平均重叠指数。\n",
        "    \"\"\"\n",
        "    n_rules, n_inputs = mus.shape\n",
        "    overlap_indices = []\n",
        "    for j in range(n_inputs):\n",
        "        overlaps = []\n",
        "        for i in range(n_rules):\n",
        "            for k in range(i + 1, n_rules):\n",
        "                # 计算第 j 个输入特征上规则 i 和规则 k 的隶属函数之间的重叠程度\n",
        "                distance = abs(mus[i, j] - mus[k, j])\n",
        "                sigma_sum = sigmas[i, j] + sigmas[k, j]\n",
        "                overlap = max(0, (sigma_sum - distance) / sigma_sum)\n",
        "                overlaps.append(overlap)\n",
        "        # 计算第 j 个输入特征上的平均重叠指数\n",
        "        if overlaps:\n",
        "            overlap_indices.append(np.mean(overlaps))\n",
        "        else:\n",
        "            overlap_indices.append(0)\n",
        "    # 返回所有输入特征上的平均重叠指数\n",
        "    overlap_index = np.mean(overlap_indices)\n",
        "    return overlap_index\n",
        "\n",
        "def compute_fuzzy_set_position_index(mus):\n",
        "    \"\"\"\n",
        "    计算模糊集的位置指数。\n",
        "\n",
        "    参数：\n",
        "    - mus: 形状为 (n_rules, n_inputs) 的数组，表示隶属函数的中心。\n",
        "\n",
        "    返回：\n",
        "    - position_index: 浮点数，表示所有输入特征上的平均标准差。\n",
        "    \"\"\"\n",
        "    n_rules, n_inputs = mus.shape\n",
        "    position_indices = []\n",
        "    for j in range(n_inputs):\n",
        "        centers = mus[:, j]\n",
        "        std_dev = np.std(centers)\n",
        "        position_indices.append(std_dev)\n",
        "    # 返回所有输入特征上的平均位置指数\n",
        "    position_index = np.mean(position_indices)\n",
        "    return position_index\n",
        "\n",
        "def plot_attribute_weights(attribute_weights, feature_names):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(feature_names))\n",
        "    plt.bar(x, attribute_weights)\n",
        "    plt.xticks(x, feature_names, rotation=45)\n",
        "    plt.xlabel('Attributes')\n",
        "    plt.ylabel('Average Attribute Weights')\n",
        "    plt.title('Average Attribute Weights over Repeats')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_heatmap(anfis_model, feature_names):\n",
        "    \"\"\"\n",
        "    绘制属性权重的热力图。\n",
        "\n",
        "    参数：\n",
        "    - anfis_model: 训练好的 ANFIS 模型\n",
        "    - feature_names: 特征名称列表\n",
        "    \"\"\"\n",
        "    # 提取属性掩码\n",
        "    attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "\n",
        "    # 将被剪枝的属性的 attention_weights 设置为一个大负数\n",
        "    attention_weights = torch.sigmoid(anfis_model.attention_weights).clone()\n",
        "    attention_weights[anfis_model.attribute_mask == 0] = -1e6\n",
        "\n",
        "    # 计算注意力权重，并应用 attribute_mask\n",
        "    attention = torch.sigmoid(attention_weights) * anfis_model.attribute_mask\n",
        "    attention_np = attention.detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "\n",
        "    # 创建注释字符串\n",
        "    annotations = []\n",
        "    for r in range(attention_np.shape[0]):\n",
        "        row = []\n",
        "        for a in range(attention_np.shape[1]):\n",
        "            if attribute_mask_np[r, a] == 0:\n",
        "                row.append(\"0.00\\nX\")  # 被剪枝的属性，值为 0，标记为 X\n",
        "            else:\n",
        "                row.append(f\"{attention_np[r, a]:.2f}\")\n",
        "        annotations.append(row)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(\n",
        "        attention_np,\n",
        "        annot=annotations,\n",
        "        fmt='',\n",
        "        cmap='viridis',\n",
        "        xticklabels=feature_names,\n",
        "        yticklabels=[f'Rule {i+1}' for i in range(anfis_model.n_rules)],\n",
        "        cbar_kws={'label': 'Attention Weight'}\n",
        "    )\n",
        "    plt.title(f'属性注意力权重 (被剪枝的属性标记为 X)')\n",
        "    plt.xlabel('输入特征')\n",
        "    plt.ylabel('规则')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ============================\n",
        "# 定义训练函数\n",
        "# ============================\n",
        "\n",
        "def train_attention_dynamic_attribute_and_rule_anfis(\n",
        "    X_train_np, y_train_np, X_val_np, y_val_np,\n",
        "    initial_n_rules=3, epochs=1500, batch_size=32, lr=0.01,\n",
        "    prune_frequency=25, prune_threshold=0.1,\n",
        "    best_model_path='best_model.pth'\n",
        "):\n",
        "    \"\"\"\n",
        "    训练 AttentionDynamicAttributeAndRuleANFIS 模型。\n",
        "    并在训练过程中保存验证集上表现最好的模型。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集特征，形状为 (num_samples, n_inputs)\n",
        "    - y_train_np: 训练集目标，形状为 (num_samples,)\n",
        "    - X_val_np: 验证集特征，形状为 (num_val_samples, n_inputs)\n",
        "    - y_val_np: 验证集目标，形状为 (num_val_samples,)\n",
        "    - initial_n_rules: 初始规则数量，默认值为 3\n",
        "    - epochs: 训练轮数，默认值为 1500\n",
        "    - batch_size: 每批次的样本数量，默认值为 32\n",
        "    - lr: 学习率，默认值为 0.01\n",
        "    - prune_frequency: 进行属性剪枝的频率（每隔多少个 epoch）\n",
        "    - prune_threshold: 属性剪枝的阈值\n",
        "    - best_model_path: 最佳模型保存的文件路径，默认值为 'best_model.pth'\n",
        "\n",
        "    返回:\n",
        "    - anfis_model: 训练好的 ANFIS 模型（加载了最佳模型状态）\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - scaler_y: 输出数据的标准化器\n",
        "    - total_active_attributes: 最优模型的总活跃属性数量\n",
        "    - training_info: 训练过程中的信息\n",
        "    \"\"\"\n",
        "    # 创建结果保存的目录\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    n_inputs = X_train_scaled_tensor.shape[1]\n",
        "\n",
        "    anfis_model = AttentionDynamicAttributeAndRuleANFIS(\n",
        "        n_inputs=n_inputs,\n",
        "        n_rules=initial_n_rules,\n",
        "        X_train=X_train_scaled,\n",
        "        attention_threshold=prune_threshold\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器和调度器\n",
        "    optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "    # 使用余弦退火学习率调度器\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    # 将优化器赋值给模型\n",
        "    anfis_model.optimizer = optimizer\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'active_rules': [],\n",
        "        'total_rules': [],\n",
        "        'val_rmse': [],\n",
        "        'attribute_weights': [],\n",
        "        'rule_attention_weights': [],\n",
        "        'pruned_attributes': [],\n",
        "        'total_active_attributes': []\n",
        "    }\n",
        "\n",
        "    # 初始化变量以记录最佳验证损失和最佳模型状态\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None  # 用于保存最佳模型状态\n",
        "\n",
        "    # 设置规则生长和剪枝的参数\n",
        "    patience = 5  # 等待多少个 epoch 后触发规则生长\n",
        "    grow_threshold = 0.0001  # 训练损失下降低于该阈值，触发规则生长\n",
        "    no_improve_epochs = 0\n",
        "    prev_val_loss = float('inf')\n",
        "\n",
        "    max_rules = 50  # 设置规则数量上限，防止无限生长（根据需求调整）\n",
        "    attention_threshold_final = 0.25  # 定义活跃规则的注意力权重阈值\n",
        "\n",
        "    # 设置特征名称\n",
        "    feature_names = feature_labels if feature_labels else [f'Input {i+1}' for i in range(n_inputs)]\n",
        "\n",
        "    # 初始化列表用于统计总属性数量\n",
        "    total_attributes_list = []\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in trange(epochs, desc=\"Training\"):\n",
        "        anfis_model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            loss_train, _ = anfis_model.train_step(\n",
        "                batch_x,\n",
        "                batch_y,\n",
        "                optimizer,\n",
        "                lambda_attention=1e-7,\n",
        "                lambda_rule_attention=1e-8,\n",
        "                lambda_diversity=1e-4\n",
        "            )\n",
        "            epoch_loss += loss_train\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        anfis_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, firing_strength, attention, rule_attention = anfis_model.forward(X_val_scaled_tensor)\n",
        "            loss_val = F.mse_loss(output, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(output.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "\n",
        "        # 调整学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 计算当前活跃规则的数量\n",
        "        rule_attention_np = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "        num_active_rules = np.sum(rule_attention_np >= attention_threshold_final)\n",
        "\n",
        "        # 计算总活跃属性数量（仅在当前 epoch 使用）\n",
        "        attribute_mask = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        num_active_attributes_per_rule = np.sum(attribute_mask, axis=1)  # 每个规则中活跃的属性数量\n",
        "        total_active_attributes = np.sum(num_active_attributes_per_rule)  # 该模型中总的活跃属性数量\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val.item())\n",
        "        training_info['active_rules'].append(num_active_rules)\n",
        "        training_info['total_rules'].append(anfis_model.n_rules)\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "        training_info['total_active_attributes'].append(total_active_attributes)\n",
        "\n",
        "        # 提取注意力权重\n",
        "        attention_weights = torch.sigmoid(anfis_model.attention_weights).detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(anfis_model.rule_attention_weights).detach().cpu().numpy()\n",
        "\n",
        "        # 计算平均属性权重\n",
        "        avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "\n",
        "        # 保存注意力权重\n",
        "        training_info['attribute_weights'].append(avg_attribute_weights)\n",
        "        training_info['rule_attention_weights'].append(rule_attention_weights)\n",
        "\n",
        "        # 保存总活跃属性数量\n",
        "        total_attributes_list.append(total_active_attributes)\n",
        "\n",
        "        # Check for best validation loss\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            # 保存最佳模型状态\n",
        "            best_model_state = copy.deepcopy(anfis_model.state_dict())\n",
        "            # 保存最佳模型\n",
        "            anfis_model.save_model_with_architecture(scaler_X, scaler_y, best_model_path)\n",
        "            print(f\"\\nEpoch {epoch+1}: New best validation loss: {loss_val.item():.6f}. Model saved.\")\n",
        "\n",
        "        # 显示训练进度\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.6f} - Val Loss: {loss_val.item():.6f} - Val RMSE: {val_rmse:.6f} - Total Rules: {anfis_model.n_rules} - Active Rules: {num_active_rules}\")\n",
        "\n",
        "        # 检查验证损失的改进情况\n",
        "        if loss_val.item() < prev_val_loss - grow_threshold:\n",
        "            no_improve_epochs = 0\n",
        "            prev_val_loss = loss_val.item()\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # 如果验证损失在连续若干个 epoch 中没有显著改进，触发规则生长\n",
        "        if no_improve_epochs >= patience and anfis_model.n_rules < max_rules:\n",
        "            print(f\"\\nEpoch {epoch+1}: No significant improvement in validation loss, growing a new rule. Current rules: {anfis_model.n_rules}\")\n",
        "\n",
        "            # 找出当前误差较大的数据点，用于初始化新规则\n",
        "            residuals = (y_val_scaled_tensor.cpu().numpy() - output.cpu().numpy())\n",
        "            high_error_indices = np.argsort(np.abs(residuals))[-int(0.1 * len(residuals)):]  # 选取误差最大的 10% 数据\n",
        "            X_new_rule = X_val_scaled[high_error_indices]\n",
        "\n",
        "            # 添加新规则并进行性能检查\n",
        "            improved = anfis_model.grow_rule_with_performance_check(\n",
        "                X_new_rule,\n",
        "                X_train_scaled,  # 传入训练集特征\n",
        "                y_train_scaled_tensor.cpu().numpy(),  # 传入训练集目标\n",
        "                X_val_scaled,\n",
        "                y_val_scaled_tensor.cpu().numpy(),\n",
        "                best_val_loss,\n",
        "                device,\n",
        "                lr,\n",
        "                grow_epochs=100  # 设定在规则生长后训练的轮次\n",
        "            )\n",
        "\n",
        "            if improved:\n",
        "                # 如果性能有所改善，更新最佳验证损失\n",
        "                prev_val_loss = best_val_loss  # 更新之前的验证损失\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "                print(\"Optimizer and scheduler re-initialized after growing a new rule.\")\n",
        "            else:\n",
        "                # 如果性能未改善，撤销规则生长操作（已在方法内完成）\n",
        "                pass\n",
        "\n",
        "            no_improve_epochs = 0  # 重置计数器\n",
        "\n",
        "        # 设置剪枝停止的 epoch 阈值\n",
        "        pruning_stop_epoch = int(epochs * 0.8)  # 在 80% 的训练过程中进行剪枝\n",
        "\n",
        "        # 每隔 prune_frequency 个 epoch 进行属性剪枝\n",
        "        if (epoch + 1) % prune_frequency == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned_dict = anfis_model.prune_attributes_per_rule(\n",
        "                threshold=prune_threshold,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01,  # 性能下降容忍度，可根据需要调整\n",
        "                best_val_loss=best_val_loss\n",
        "            )\n",
        "            training_info['pruned_attributes'].append(pruned_dict)\n",
        "            if pruned_dict:\n",
        "                print(f\"Epoch {epoch+1}: Pruned attributes per rule: {pruned_dict}\")\n",
        "                # Reinitialize optimizer and scheduler after pruning\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning attributes.\")\n",
        "        elif epoch >= pruning_stop_epoch and (epoch + 1) % prune_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}: Pruning has been stopped to stabilize the model structure.\")\n",
        "\n",
        "        # 每隔若干个 epoch 进行规则剪枝\n",
        "        if (epoch + 1) % 50 == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned = anfis_model.prune_rules(\n",
        "                threshold=attention_threshold_final,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01,\n",
        "                best_val_loss=best_val_loss\n",
        "            )\n",
        "            if pruned:\n",
        "                print(f\"Epoch {epoch+1}: Pruned rules. Total rules: {anfis_model.n_rules}\")\n",
        "                # Reinitialize optimizer and scheduler after pruning\n",
        "                optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch and (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Rule pruning has been stopped to stabilize the model structure.\")\n",
        "\n",
        "    # 在训练结束后，加载最佳模型\n",
        "    if os.path.exists(best_model_path):\n",
        "        model_loaded, scaler_X_loaded, scaler_y_loaded = AttentionDynamicAttributeAndRuleANFIS.load_model_with_architecture(best_model_path, device=device)\n",
        "        anfis_model = model_loaded  # 更新模型为加载的最佳模型\n",
        "        scaler_X = scaler_X_loaded\n",
        "        scaler_y = scaler_y_loaded\n",
        "        print(\"\\nLoaded the best model based on validation loss.\")\n",
        "\n",
        "        # 显示训练后的隶属函数图像（可选）\n",
        "        # anfis_model.plot_membership_functions(feature_names=feature_labels)\n",
        "\n",
        "        # 绘制属性权重的热力图（可选）\n",
        "        # plot_heatmap(anfis_model, feature_labels)\n",
        "\n",
        "        # 提取并保存规则\n",
        "        rules = extract_fuzzy_rules(anfis_model, scaler_X, feature_names=feature_labels)\n",
        "        print(f\"\\n=== ADAR-ANFIS Extracted Fuzzy Rules ===\")\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "            print()\n",
        "        # 保存规则到文件\n",
        "        with open(f'results/rules_lr{lr}_final.txt', 'w') as f:\n",
        "            for rule in rules:\n",
        "                f.write(rule + '\\n')\n",
        "\n",
        "        # 计算总活跃属性数量\n",
        "        attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask_np)\n",
        "        print(f\"Total Number of Attributes Included in All Rules: {total_active_attributes:.2f}\")\n",
        "    else:\n",
        "        print(\"No improvement during training. Using the final model.\")\n",
        "\n",
        "        # 计算总活跃属性数量\n",
        "        attribute_mask_np = anfis_model.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask_np)\n",
        "        print(f\"Total Number of Attributes Included in All Rules: {total_active_attributes:.2f}\")\n",
        "\n",
        "    # 可视化训练过程\n",
        "    # 合并验证损失和规则数量曲线\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Validation Loss', color=color)\n",
        "    ax1.plot(training_info['epoch'], training_info['val_loss'], color=color, label='Validation Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()  # 共享 x 轴\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Number of Rules', color=color)\n",
        "    ax2.plot(training_info['epoch'], training_info['total_rules'], color=color, label='Total Rules', linestyle='--')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.title('Validation Loss and Number of Rules over Epochs')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return anfis_model, scaler_X, scaler_y, total_active_attributes, training_info\n",
        "\n",
        "# ============================\n",
        "# 执行实验\n",
        "# ============================\n",
        "\n",
        "# 将训练集进一步拆分为训练和验证集\n",
        "X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "    X_train_np, y_train_np, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 设置实验参数\n",
        "initial_n_rules = 2  # 初始规则数量\n",
        "epochs = 500\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "prune_frequency = 25\n",
        "prune_threshold = 0.1\n",
        "\n",
        "# 训练模型\n",
        "anfis_model, scaler_X, scaler_y, total_active_attributes, training_info = train_attention_dynamic_attribute_and_rule_anfis(\n",
        "    X_train_sub, y_train_sub, X_val_sub, y_val_sub,\n",
        "    initial_n_rules=initial_n_rules,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    lr=learning_rate,\n",
        "    prune_frequency=prune_frequency,\n",
        "    prune_threshold=prune_threshold,\n",
        "    best_model_path='best_model_dynamic.pth'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5oTtxmUrMRZ"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "\n",
        "# 假设您已经获取了北京 PM2.5 数据集，并将其存储在变量中\n",
        "# 由于无法在线获取数据，请确保您已将数据集加载到 X 和 y 中\n",
        "\n",
        "# 数据（作为 pandas 数据帧）\n",
        "# X = 数据的特征部分\n",
        "# y = 数据的目标变量 'pm2.5'\n",
        "\n",
        "# 从 UCI ML Repo 下载 Appliances Energy Prediction 数据集\n",
        "energy_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv'\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(energy_url)\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'date' 和 'Appliances'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4',\n",
        "    'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9',\n",
        "    'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
        "    'Tdewpoint', 'rv1', 'rv2'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# feature_names = features_to_use\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 定义模型组件\n",
        "# ============================\n",
        "\n",
        "# 定义扩展后的 FuzzyLayer 类，加入属性注意力机制\n",
        "class AttentionFuzzyLayer(nn.Module):\n",
        "    def __init__(self, input_dim, n_rules):\n",
        "        super(AttentionFuzzyLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        # 初始化中心 c 和宽度 sigma\n",
        "        self.c = nn.Parameter(torch.randn(n_rules, input_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(n_rules, input_dim))\n",
        "\n",
        "        # 初始化属性注意力权重\n",
        "        self.attention_weights = nn.Parameter(torch.randn(n_rules, input_dim))\n",
        "\n",
        "        # 初始化属性掩码（1 表示活跃，0 表示被剪除）\n",
        "        self.register_buffer('attribute_mask', torch.ones(n_rules, input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 计算属性注意力权重（使用 sigmoid 激活函数并应用属性掩码）\n",
        "        attention = torch.sigmoid(self.attention_weights) * self.attribute_mask  # (n_rules, input_dim)\n",
        "\n",
        "        # 扩展维度以进行广播\n",
        "        x_expanded = x.unsqueeze(1)  # (batch_size, 1, input_dim)\n",
        "        c_expanded = self.c.unsqueeze(0)  # (1, n_rules, input_dim)\n",
        "        sigma_expanded = self.sigma.unsqueeze(0)  # (1, n_rules, input_dim)\n",
        "\n",
        "        # 确保 sigma 为正数，避免除以零\n",
        "        sigma_expanded = torch.clamp(sigma_expanded, min=1e-3)\n",
        "\n",
        "        # 计算高斯隶属度函数的对数\n",
        "        diff = x_expanded - c_expanded  # (batch_size, n_rules, input_dim)\n",
        "        exponent = -0.5 * ((diff / sigma_expanded) ** 2)\n",
        "\n",
        "        # 使用属性注意力权重并应用属性掩码\n",
        "        exponent_weighted = exponent * attention.unsqueeze(0)  # (batch_size, n_rules, input_dim)\n",
        "\n",
        "        # 对输入属性维度求和\n",
        "        sum_exponent = exponent_weighted.sum(dim=2)  # (batch_size, n_rules)\n",
        "\n",
        "        # 计算规则激活度\n",
        "        phi = torch.exp(sum_exponent)  # (batch_size, n_rules)\n",
        "\n",
        "        return phi, attention\n",
        "\n",
        "# 定义扩展后的 NormalizedLayer 类，加入规则注意力机制\n",
        "class AttentionNormalizedLayer(nn.Module):\n",
        "    def __init__(self, n_rules):\n",
        "        super(AttentionNormalizedLayer, self).__init__()\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        # 初始化规则注意力权重\n",
        "        self.rule_attention_weights = nn.Parameter(torch.ones(n_rules))\n",
        "\n",
        "    def forward(self, phi):\n",
        "        # phi: (batch_size, n_rules)\n",
        "        # 计算规则注意力权重（使用 sigmoid 激活函数）\n",
        "        rule_attention = torch.sigmoid(self.rule_attention_weights)  # (n_rules,)\n",
        "\n",
        "        # 使用规则注意力权重调整规则激活度\n",
        "        phi_weighted = phi * rule_attention.unsqueeze(0)  # (batch_size, n_rules)\n",
        "\n",
        "        # 计算归一化的激活度\n",
        "        phi_sum = phi_weighted.sum(dim=1, keepdim=True) + 1e-8  # 防止除以零\n",
        "        psi = phi_weighted / phi_sum  # (batch_size, n_rules)\n",
        "        return psi, rule_attention\n",
        "\n",
        "# 定义扩展后的 WeightedLayer 类\n",
        "class AttentionWeightedLayer(nn.Module):\n",
        "    def __init__(self, input_dim, n_rules):\n",
        "        super(AttentionWeightedLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        # 初始化后件参数 a，包括偏置项\n",
        "        self.a = nn.Parameter(torch.randn(n_rules, input_dim + 1))\n",
        "\n",
        "    def forward(self, x, psi):\n",
        "        # x: (batch_size, input_dim)\n",
        "        # psi: (batch_size, n_rules)\n",
        "        batch_size = x.size(0)\n",
        "        # 添加偏置项\n",
        "        ones = torch.ones(batch_size, 1).to(x.device)\n",
        "        x_with_bias = torch.cat([ones, x], dim=1)  # (batch_size, input_dim + 1)\n",
        "\n",
        "        # 扩展 x 和 a 的维度以进行广播\n",
        "        x_expanded = x_with_bias.unsqueeze(1)  # (batch_size, 1, input_dim + 1)\n",
        "        a_expanded = self.a.unsqueeze(0)       # (1, n_rules, input_dim + 1)\n",
        "\n",
        "        # 计算每个规则的输出（元素级乘法后在特征维度上求和）\n",
        "        w = (x_expanded * a_expanded).sum(dim=2)  # (batch_size, n_rules)\n",
        "\n",
        "        f = psi * w  # (batch_size, n_rules)\n",
        "        return f\n",
        "\n",
        "# 定义 OutputLayer 类\n",
        "class OutputLayer(nn.Module):\n",
        "    def forward(self, f):\n",
        "        # f: (batch_size, n_rules)\n",
        "        output = f.sum(dim=1)  # (batch_size,)\n",
        "        return output\n",
        "\n",
        "# ============================\n",
        "# 定义完整的 SOFENN 模型，加入规则生长和剪枝功能\n",
        "# ============================\n",
        "class AttentionDynamicAttributeAndRuleSOFENN(nn.Module):\n",
        "    def __init__(self, input_dim, n_rules, attention_threshold=0.1):\n",
        "        super(AttentionDynamicAttributeAndRuleSOFENN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_rules = n_rules\n",
        "        self.attention_threshold = attention_threshold\n",
        "\n",
        "        self.fuzzy_layer = AttentionFuzzyLayer(input_dim, n_rules)\n",
        "        self.normalized_layer = AttentionNormalizedLayer(n_rules)\n",
        "        self.weighted_layer = AttentionWeightedLayer(input_dim, n_rules)\n",
        "        self.output_layer = OutputLayer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        phi, attention = self.fuzzy_layer(x)\n",
        "        psi, rule_attention = self.normalized_layer(phi)\n",
        "        f = self.weighted_layer(x, psi)\n",
        "        output = self.output_layer(f)\n",
        "        return output, phi, attention, rule_attention\n",
        "\n",
        "    def train_step(self, x, target, optimizer, lambda_attention=1e-7, lambda_rule_attention=1e-8, lambda_diversity=1e-4):\n",
        "        \"\"\"\n",
        "        执行一次训练步骤。\n",
        "        \"\"\"\n",
        "        self.train()\n",
        "        optimizer.zero_grad()\n",
        "        output, _, attention, rule_attention = self.forward(x)\n",
        "        # 计算预测损失（均方误差损失）\n",
        "        loss_pred = nn.functional.mse_loss(output, target)\n",
        "\n",
        "        # 添加属性注意力正则化损失（L1 正则化）\n",
        "        loss_attention = lambda_attention * attention.abs().sum()\n",
        "\n",
        "        # 计算规则注意力正则化损失（L1 正则化）\n",
        "        loss_rule_attention = lambda_rule_attention * rule_attention.abs().sum()\n",
        "\n",
        "        # 添加多样性正则化损失（鼓励不同规则的注意力权重不同）\n",
        "        if self.n_rules > 1:\n",
        "            # 计算注意力权重的余弦相似度矩阵\n",
        "            attention_norm = attention / (attention.norm(dim=1, keepdim=True) + 1e-8)\n",
        "            similarity_matrix = torch.matmul(attention_norm, attention_norm.t())\n",
        "            # 计算非对角线的平均相似度\n",
        "            diversity_loss = torch.sum(similarity_matrix) - torch.diag(similarity_matrix).sum()\n",
        "            diversity_loss = diversity_loss / (self.n_rules * (self.n_rules - 1))\n",
        "        else:\n",
        "            diversity_loss = torch.tensor(0.0).to(attention.device)\n",
        "\n",
        "        loss_diversity = lambda_diversity * diversity_loss\n",
        "\n",
        "        # 总损失\n",
        "        loss = loss_pred + loss_attention + loss_rule_attention + loss_diversity\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Loss is NaN. Stopping training.\")\n",
        "            return loss.item(), output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), output\n",
        "\n",
        "    def prune_attributes_per_rule(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除每个规则中注意力权重低于阈值的属性，并冻结其相关参数。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则撤销剪枝。\n",
        "\n",
        "        返回：\n",
        "        - pruned_dict: 字典，键为规则索引，值为被剪除的属性索引列表\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "        if best_val_loss is None:\n",
        "            raise ValueError(\"best_val_loss must be provided for validation performance check.\")\n",
        "\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self)\n",
        "\n",
        "        # 保存剪枝前的模型状态和验证损失\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            output_before = self.infer(X_val)\n",
        "            loss_before = nn.functional.mse_loss(output_before, y_val)\n",
        "\n",
        "        # 执行剪枝操作\n",
        "        pruned_dict = {}\n",
        "        with torch.no_grad():\n",
        "            attention = torch.sigmoid(model_copy.fuzzy_layer.attention_weights)  # (n_rules, input_dim)\n",
        "\n",
        "            for rule_idx in range(model_copy.n_rules):\n",
        "                if torch.all(model_copy.fuzzy_layer.attribute_mask[rule_idx] == 0):\n",
        "                    continue  # 跳过已被完全剪除的规则\n",
        "\n",
        "                prune_indices = torch.where(\n",
        "                    (attention[rule_idx] < threshold) & (model_copy.fuzzy_layer.attribute_mask[rule_idx] == 1)\n",
        "                )[0].tolist()\n",
        "\n",
        "                if prune_indices:\n",
        "                    # 更新属性掩码\n",
        "                    model_copy.fuzzy_layer.attribute_mask[rule_idx, prune_indices] = 0.0\n",
        "\n",
        "                    # 冻结被剪除属性的相关参数\n",
        "                    model_copy.fuzzy_layer.attention_weights[rule_idx, prune_indices].requires_grad = False\n",
        "                    model_copy.fuzzy_layer.c[rule_idx, prune_indices].requires_grad = False\n",
        "                    model_copy.fuzzy_layer.sigma[rule_idx, prune_indices].requires_grad = False\n",
        "                    # 对 prune_indices 中的每个索引加 1，因为偏置项占用了第一个位置\n",
        "                    prune_indices_plus_one = [idx + 1 for idx in prune_indices]\n",
        "                    model_copy.weighted_layer.a[rule_idx, prune_indices_plus_one].requires_grad = False  # +1 是因为有偏置项\n",
        "\n",
        "                    pruned_dict[rule_idx] = prune_indices\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = model_copy.infer(X_val)\n",
        "            loss_after = nn.functional.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - best_val_loss) / best_val_loss\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，撤销剪枝操作\n",
        "            print(f\"Pruning was reverted due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned_dict = {}  # 清空剪枝记录\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能没有显著下降，更新模型参数\n",
        "            self.load_state_dict(model_copy.state_dict())\n",
        "            print(f\"Attribute pruning successful. Performance drop: {performance_drop * 100:.2f}%\")\n",
        "            pruned = True\n",
        "\n",
        "        return pruned_dict\n",
        "\n",
        "    def prune_rules(self, threshold=0.1, X_val=None, y_val=None, performance_drop_tolerance=0.01, best_val_loss=None):\n",
        "        \"\"\"\n",
        "        剪除规则注意力权重低于阈值的规则，并从模型中完全移除这些规则。\n",
        "        如果剪枝后模型在验证集上的性能下降超过容忍度，则撤销剪枝。\n",
        "\n",
        "        返回：\n",
        "        - pruned: 布尔值，指示是否实际移除了规则\n",
        "        \"\"\"\n",
        "        if X_val is None or y_val is None:\n",
        "            raise ValueError(\"X_val and y_val must be provided for validation performance check.\")\n",
        "        if best_val_loss is None:\n",
        "            raise ValueError(\"best_val_loss must be provided for validation performance check.\")\n",
        "\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self)\n",
        "\n",
        "        pruned = False\n",
        "        with torch.no_grad():\n",
        "            # 获取规则注意力权重\n",
        "            rule_attention = torch.sigmoid(model_copy.normalized_layer.rule_attention_weights)\n",
        "            # 找到需要移除的规则索引（rule_attention < threshold）\n",
        "            prune_indices = torch.where(rule_attention < threshold)[0]\n",
        "\n",
        "            if len(prune_indices) == 0:\n",
        "                # 没有需要移除的规则\n",
        "                return pruned\n",
        "\n",
        "            # 保留的规则索引（rule_attention >= threshold）\n",
        "            keep_indices = torch.where(rule_attention >= threshold)[0]\n",
        "\n",
        "            # 更新模型参数，移除低重要性的规则\n",
        "            model_copy.fuzzy_layer.c = nn.Parameter(model_copy.fuzzy_layer.c.data[keep_indices])\n",
        "            model_copy.fuzzy_layer.sigma = nn.Parameter(model_copy.fuzzy_layer.sigma.data[keep_indices])\n",
        "            model_copy.fuzzy_layer.attention_weights = nn.Parameter(model_copy.fuzzy_layer.attention_weights.data[keep_indices])\n",
        "            model_copy.normalized_layer.rule_attention_weights = nn.Parameter(\n",
        "                model_copy.normalized_layer.rule_attention_weights.data[keep_indices]\n",
        "            )\n",
        "            model_copy.weighted_layer.a = nn.Parameter(model_copy.weighted_layer.a.data[keep_indices])\n",
        "\n",
        "            # 更新属性掩码，移除被剪除规则的掩码行\n",
        "            model_copy.fuzzy_layer.attribute_mask = model_copy.fuzzy_layer.attribute_mask.data[keep_indices].clone()\n",
        "\n",
        "            # 更新规则数量\n",
        "            model_copy.n_rules = len(keep_indices)\n",
        "            model_copy.fuzzy_layer.n_rules = model_copy.n_rules\n",
        "            model_copy.normalized_layer.n_rules = model_copy.n_rules\n",
        "            model_copy.weighted_layer.n_rules = model_copy.n_rules\n",
        "\n",
        "            pruned = True  # 标记为已剪枝\n",
        "\n",
        "        # 剪枝后的验证损失\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = model_copy.infer(X_val)\n",
        "            loss_after = nn.functional.mse_loss(output_after, y_val)\n",
        "\n",
        "        # 判断性能是否下降超过容忍度\n",
        "        performance_drop = (loss_after - best_val_loss) / best_val_loss\n",
        "\n",
        "        if performance_drop > performance_drop_tolerance:\n",
        "            # 性能下降超过容忍度，不执行剪枝\n",
        "            print(f\"Rule pruning was not performed due to performance degradation: Loss increased by {performance_drop * 100:.2f}%\")\n",
        "            pruned = False\n",
        "        else:\n",
        "            # 性能未下降，更新原始模型的参数\n",
        "            self.load_state_dict(model_copy.state_dict())\n",
        "            print(f\"Rules pruned successfully. Performance drop: {performance_drop * 100:.2f}%\")\n",
        "            pruned = True\n",
        "\n",
        "        return pruned\n",
        "\n",
        "    def grow_rule(self, X_new):\n",
        "        \"\"\"\n",
        "        添加一个新的规则。\n",
        "        \"\"\"\n",
        "        # 获取设备和数据类型\n",
        "        device = self.fuzzy_layer.c.device\n",
        "        dtype = self.fuzzy_layer.c.dtype\n",
        "\n",
        "        # 使用 X_new 计算新的规则中心和标准差\n",
        "        new_c = torch.tensor(X_new.mean(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, input_dim)\n",
        "        new_sigma = torch.tensor(X_new.std(axis=0), dtype=dtype).unsqueeze(0).to(device)  # (1, input_dim)\n",
        "\n",
        "        # 计算现有规则的属性注意力权重的平均值\n",
        "        if self.n_rules > 0:\n",
        "            existing_attention_weights = torch.sigmoid(self.fuzzy_layer.attention_weights).data  # (n_rules, input_dim)\n",
        "            attention_mean = existing_attention_weights.mean(dim=0, keepdim=True)  # (1, input_dim)\n",
        "        else:\n",
        "            attention_mean = torch.ones(1, self.input_dim, dtype=dtype).to(device)  # 初始化为 1\n",
        "\n",
        "        # 将新规则的属性注意力权重初始化为平均值并加入随机扰动\n",
        "        noise = torch.randn_like(attention_mean) * 0.05  # 调整扰动大小以控制多样性\n",
        "        new_attention_weights = (attention_mean + noise).clamp(0, 1).detach()  # 保持在 [0, 1] 范围内\n",
        "\n",
        "        # 将新规则的规则注意力权重初始化为与现有权重的均值 logit 相同，并加入随机扰动\n",
        "        if self.n_rules > 0:\n",
        "            existing_rule_attention_logits = self.normalized_layer.rule_attention_weights.data  # (n_rules,)\n",
        "            rule_attention_mean_logit = existing_rule_attention_logits.mean().unsqueeze(0)  # (1,)\n",
        "            rule_attention_noise = torch.randn_like(rule_attention_mean_logit) * 0.05  # 调整扰动大小\n",
        "            new_rule_attention_weight = (rule_attention_mean_logit + rule_attention_noise).detach()\n",
        "        else:\n",
        "            rule_attention_mean_logit = torch.tensor([0.0], dtype=dtype).to(device)  # 中性 logit\n",
        "            new_rule_attention_weight = rule_attention_mean_logit.clone().detach()  # (1,)\n",
        "\n",
        "        # 初始化后件参数为小的随机值\n",
        "        new_a = torch.randn(1, self.input_dim + 1, dtype=dtype).to(device) * 0.01  # (1, input_dim + 1)\n",
        "\n",
        "        # 将新的参数添加到模型中\n",
        "        self.fuzzy_layer.c = nn.Parameter(torch.cat([self.fuzzy_layer.c.data, new_c], dim=0))  # (n_rules + 1, input_dim)\n",
        "        self.fuzzy_layer.sigma = nn.Parameter(torch.cat([self.fuzzy_layer.sigma.data, new_sigma], dim=0))  # (n_rules + 1, input_dim)\n",
        "        self.fuzzy_layer.attention_weights = nn.Parameter(torch.cat([self.fuzzy_layer.attention_weights.data, new_attention_weights], dim=0))  # (n_rules + 1, input_dim)\n",
        "        self.normalized_layer.rule_attention_weights = nn.Parameter(torch.cat([self.normalized_layer.rule_attention_weights.data, new_rule_attention_weight], dim=0))  # (n_rules + 1,)\n",
        "        self.weighted_layer.a = nn.Parameter(torch.cat([self.weighted_layer.a.data, new_a], dim=0))  # (n_rules + 1, input_dim + 1)\n",
        "\n",
        "        # 更新属性掩码，添加新规则的掩码行\n",
        "        new_attribute_mask = torch.ones(1, self.input_dim, dtype=self.fuzzy_layer.attribute_mask.dtype).to(device)  # (1, input_dim)\n",
        "        self.fuzzy_layer.attribute_mask = torch.cat([self.fuzzy_layer.attribute_mask, new_attribute_mask], dim=0)  # (n_rules + 1, input_dim)\n",
        "\n",
        "        # 更新规则数量\n",
        "        self.n_rules += 1\n",
        "        self.fuzzy_layer.n_rules = self.n_rules\n",
        "        self.normalized_layer.n_rules = self.n_rules\n",
        "        self.weighted_layer.n_rules = self.n_rules\n",
        "\n",
        "        # 确保 attribute_mask 的维度与 n_rules 一致\n",
        "        assert self.fuzzy_layer.attribute_mask.shape[0] == self.n_rules, \\\n",
        "            f\"After growing, attribute_mask has shape {self.fuzzy_layer.attribute_mask.shape}, but n_rules={self.n_rules}\"\n",
        "\n",
        "        print(f\"New rule added. Total rules: {self.n_rules}\")\n",
        "\n",
        "    def grow_rule_with_performance_check(self, X_new, X_train, y_train, X_val, y_val, best_val_loss, device, lr, grow_epochs=10):\n",
        "        \"\"\"\n",
        "        添加一个新的规则，并进行性能检查。如果性能没有提升，则撤销规则生长。\n",
        "\n",
        "        参数：\n",
        "        - X_new: 新规则的初始数据，形状：(num_samples, input_dim)\n",
        "        \"\"\"\n",
        "        # 创建模型的副本\n",
        "        model_copy = copy.deepcopy(self).to(device)\n",
        "\n",
        "        # 添加新规则到副本\n",
        "        model_copy.grow_rule(X_new)\n",
        "\n",
        "        # 初始化优化器和调度器\n",
        "        optimizer_copy = optim.AdamW(model_copy.parameters(), lr=lr)\n",
        "        scheduler_copy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_copy, T_max=grow_epochs)\n",
        "\n",
        "        # 将训练数据转换为张量\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
        "\n",
        "        # 训练模型副本\n",
        "        for epoch in range(grow_epochs):\n",
        "            model_copy.train()\n",
        "            optimizer_copy.zero_grad()\n",
        "            output, _, _, _ = model_copy.forward(X_train_tensor)\n",
        "            loss = nn.functional.mse_loss(output, y_train_tensor)\n",
        "            loss.backward()\n",
        "            optimizer_copy.step()\n",
        "            scheduler_copy.step()\n",
        "\n",
        "        # 评估副本模型的性能\n",
        "        model_copy.eval()\n",
        "        with torch.no_grad():\n",
        "            output_after = model_copy.infer(X_val_tensor)\n",
        "            loss_after = nn.functional.mse_loss(output_after, y_val_tensor).item()\n",
        "\n",
        "        print(f\"After growing rule and training for {grow_epochs} epochs: Validation Loss = {loss_after:.4f}\")\n",
        "\n",
        "        if loss_after < best_val_loss:\n",
        "            # 性能有所提升，保留新规则\n",
        "            # 更新原始模型的参数\n",
        "            self.load_state_dict(model_copy.state_dict())\n",
        "            self.n_rules = model_copy.n_rules  # 更新规则数量\n",
        "            print(\"Performance improved after growing rule. New rule retained.\")\n",
        "            return True\n",
        "        else:\n",
        "            # 性能未提升，丢弃副本，保持主模型不变\n",
        "            print(\"Performance did not improve after growing rule. Rule growth reverted.\")\n",
        "            return False\n",
        "\n",
        "    def infer(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        执行推理。\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            output, _, _, _ = self.forward(x)\n",
        "            if targets is None:\n",
        "                return output\n",
        "            else:\n",
        "                loss = nn.functional.mse_loss(output, targets)\n",
        "                return output, loss.item()\n",
        "\n",
        "    def extract_rules(self, scaler_X, scaler_y, feature_names=None):\n",
        "        \"\"\"\n",
        "        提取模型的模糊规则。\n",
        "        \"\"\"\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Input {i+1}' for i in range(self.input_dim)]\n",
        "\n",
        "        rules = []\n",
        "        c = self.fuzzy_layer.c.detach().cpu().numpy()  # (n_rules, input_dim)\n",
        "        sigma = self.fuzzy_layer.sigma.detach().cpu().numpy()  # (n_rules, input_dim)\n",
        "        attention_weights = torch.sigmoid(self.fuzzy_layer.attention_weights).detach().cpu().numpy()  # (n_rules, input_dim)\n",
        "        attribute_mask = self.fuzzy_layer.attribute_mask.detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(self.normalized_layer.rule_attention_weights).detach().cpu().numpy()\n",
        "        a = self.weighted_layer.a.detach().cpu().numpy()  # (n_rules, input_dim + 1)\n",
        "\n",
        "        # 反标准化\n",
        "        c_orig = c * scaler_X.scale_.reshape(1, -1) + scaler_X.mean_.reshape(1, -1)\n",
        "        sigma_orig = sigma * scaler_X.scale_.reshape(1, -1)\n",
        "        a_orig = a.copy()\n",
        "        a_orig[:, 1:] = a[:, 1:] / scaler_X.scale_.reshape(1, -1) * scaler_y.scale_[0]\n",
        "        a_orig[:, 0] = scaler_y.scale_[0] * a[:, 0] + scaler_y.mean_[0] - np.sum(\n",
        "            a[:, 1:] * scaler_X.mean_.reshape(1, -1) / scaler_X.scale_.reshape(1, -1) * scaler_y.scale_[0],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        for j in range(self.n_rules):\n",
        "            # 包含规则注意力权重\n",
        "            rule_str = (f\"Rule {j+1} (Rule Attention: {rule_attention_weights[j]:.4f}): IF \")\n",
        "            antecedents = []\n",
        "            for i in range(self.input_dim):\n",
        "                if attribute_mask[j, i] == 0:\n",
        "                    continue  # 忽略被剪枝的属性\n",
        "                attention_value = attention_weights[j, i]\n",
        "                mu = c_orig[j, i]\n",
        "                sigma_i = sigma_orig[j, i]\n",
        "                antecedents.append(\n",
        "                    f\"[{feature_names[i]} (Attn: {attention_value:.4f}) is Gaussian(c={mu:.4f}, σ={sigma_i:.4f})]\"\n",
        "                )\n",
        "            antecedent_str = \" AND \".join(antecedents) if antecedents else \"True\"\n",
        "            rule_str += antecedent_str + \" THEN Output = \"\n",
        "\n",
        "            # 构建后件部分\n",
        "            a_j = a_orig[j, :]  # (input_dim + 1,)\n",
        "            consequent_terms = [f\"{a_j[0]:.4f}\"]\n",
        "            for idx, coef in enumerate(a_j[1:]):\n",
        "                if attribute_mask[j, idx] == 0:\n",
        "                    continue  # 忽略被剪枝的属性\n",
        "                attention_value = attention_weights[j, idx]\n",
        "                if coef >= 0:\n",
        "                    term = f\"+ {coef:.4f} * {feature_names[idx]} (Attn: {attention_value:.4f})\"\n",
        "                else:\n",
        "                    term = f\"- {abs(coef):.4f} * {feature_names[idx]} (Attn: {attention_value:.4f})\"\n",
        "                consequent_terms.append(term)\n",
        "            consequent_str = \" \".join(consequent_terms)\n",
        "            rule_str += consequent_str\n",
        "            rules.append(rule_str)\n",
        "        return rules\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "# ============================\n",
        "# 定义训练函数\n",
        "# ============================\n",
        "def train_attention_dynamic_attribute_and_rule_sofenn(\n",
        "    X_train_np, y_train_np, X_val_np, y_val_np,\n",
        "    initial_n_rules=3, epochs=1500, batch_size=32, lr=0.01,\n",
        "    prune_frequency=190, prune_threshold=0.1,\n",
        "    best_model_path='best_sofenn_model.pth'\n",
        "):\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    input_dim = X_train_scaled_tensor.shape[1]\n",
        "    model = AttentionDynamicAttributeAndRuleSOFENN(\n",
        "        input_dim=input_dim,\n",
        "        n_rules=initial_n_rules,\n",
        "        attention_threshold=prune_threshold\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器和调度器\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'active_rules': [],\n",
        "        'total_rules': [],\n",
        "        'val_rmse': [],\n",
        "        'attribute_weights': [],\n",
        "        'rule_attention_weights': [],\n",
        "        'pruned_attributes': [],\n",
        "        'total_active_attributes': []\n",
        "    }\n",
        "\n",
        "    # 初始化变量以记录最佳验证损失和最佳模型状态\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # 设置规则生长和剪枝的参数\n",
        "    patience = 25  # 等待多少个 epoch 后触发规则生长\n",
        "    grow_threshold = 0.0001  # 训练损失下降低于该阈值，触发规则生长\n",
        "    no_improve_epochs = 0\n",
        "    prev_val_loss = float('inf')\n",
        "\n",
        "    max_rules = 9  # 设置规则数量上限，防止无限生长\n",
        "    attention_threshold = 0.05  # 定义活跃规则的注意力权重阈值\n",
        "\n",
        "    # 设置剪枝停止的 epoch 阈值\n",
        "    pruning_stop_epoch = int(epochs * 0.95)  # 在 95% 的训练过程中进行剪枝\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            loss_train, _ = model.train_step(\n",
        "                batch_x,\n",
        "                batch_y,\n",
        "                optimizer,\n",
        "                lambda_attention=1e-7,\n",
        "                lambda_rule_attention=1e-8,\n",
        "                lambda_diversity=1e-4\n",
        "            )\n",
        "            epoch_loss += loss_train\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, phi, attention, rule_attention = model.forward(X_val_scaled_tensor)\n",
        "            loss_val = nn.functional.mse_loss(output, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(output.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "\n",
        "        # 调整学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 计算当前活跃规则的数量\n",
        "        rule_attention_np = torch.sigmoid(model.normalized_layer.rule_attention_weights).detach().cpu().numpy()\n",
        "        num_active_rules = np.sum(rule_attention_np >= attention_threshold)\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val.item())\n",
        "        training_info['active_rules'].append(num_active_rules)\n",
        "        training_info['total_rules'].append(model.n_rules)\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "\n",
        "        # 提取注意力权重\n",
        "        attention_weights = torch.sigmoid(model.fuzzy_layer.attention_weights).detach().cpu().numpy()\n",
        "        rule_attention_weights = torch.sigmoid(model.normalized_layer.rule_attention_weights).detach().cpu().numpy()\n",
        "\n",
        "        # 计算平均属性权重\n",
        "        avg_attribute_weights = attention_weights.mean(axis=0)  # Average over rules\n",
        "\n",
        "        # 保存注意力权重\n",
        "        training_info['attribute_weights'].append(avg_attribute_weights)\n",
        "        training_info['rule_attention_weights'].append(rule_attention_weights)\n",
        "\n",
        "        # 计算当前所有规则中激活的属性总数\n",
        "        attribute_mask = model.fuzzy_layer.attribute_mask.detach().cpu().numpy()\n",
        "        total_active_attributes = np.sum(attribute_mask)\n",
        "\n",
        "        # 保存激活的属性总数\n",
        "        training_info['total_active_attributes'].append(total_active_attributes)\n",
        "\n",
        "        # 检查是否为最佳验证损失\n",
        "        if loss_val.item() < best_val_loss:\n",
        "            best_val_loss = loss_val.item()\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Epoch {epoch+1}: New best validation loss: {loss_val.item():.4f}. Model saved.\")\n",
        "\n",
        "        # 显示注意力权重信息\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {loss_val.item():.4f} - Val RMSE: {val_rmse:.4f} - Total Rules: {model.n_rules} - Active Rules: {num_active_rules}\")\n",
        "\n",
        "        # 检查验证损失的改进情况\n",
        "        if loss_val.item() < prev_val_loss - grow_threshold:\n",
        "            no_improve_epochs = 0\n",
        "            prev_val_loss = loss_val.item()\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # 如果验证损失在连续若干个 epoch 中没有显著改进，触发规则生长\n",
        "        if no_improve_epochs >= patience and model.n_rules < max_rules:\n",
        "            print(f\"Epoch {epoch+1}: No significant improvement in validation loss, growing a new rule. Current rules: {model.n_rules}\")\n",
        "            # 找出当前误差较大的数据点，用于初始化新规则\n",
        "            residuals = (y_val_scaled_tensor.cpu().numpy() - output.cpu().numpy())\n",
        "            high_error_indices = np.argsort(np.abs(residuals))[-int(0.1 * len(residuals)):]  # 选取误差最大的 10% 数据\n",
        "            X_new_rule = X_val_np[high_error_indices]\n",
        "            # 添加新规则并进行性能检查\n",
        "            improved = model.grow_rule_with_performance_check(\n",
        "                X_new_rule,\n",
        "                X_train_np,  # 传入训练集特征\n",
        "                y_train_scaled_tensor.cpu().numpy(),  # 传入训练集目标\n",
        "                X_val_np,\n",
        "                y_val_scaled_tensor.cpu().numpy(),\n",
        "                best_val_loss,\n",
        "                device,\n",
        "                lr,\n",
        "                grow_epochs=100  # 设定在规则生长后训练的轮次\n",
        "            )\n",
        "            if improved:\n",
        "                # 如果性能有所改善，更新最佳验证损失\n",
        "                prev_val_loss = best_val_loss  # 更新之前的验证损失\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after growing a new rule.\")\n",
        "            else:\n",
        "                # 如果性能未改善，撤销规则生长操作（已在方法内完成）\n",
        "                pass\n",
        "            no_improve_epochs = 0  # 重置计数器\n",
        "\n",
        "        # 每隔 prune_frequency 个 epoch 进行属性剪枝\n",
        "        if (epoch + 1) % prune_frequency == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned_dict = model.prune_attributes_per_rule(\n",
        "                threshold=prune_threshold,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01,  # 性能下降容忍度，可根据需要调整\n",
        "                best_val_loss=best_val_loss\n",
        "            )\n",
        "            training_info['pruned_attributes'].append(pruned_dict)\n",
        "            if pruned_dict:\n",
        "                print(f\"Epoch {epoch+1}: Pruned attributes per rule: {pruned_dict}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch:\n",
        "            pass  # 不再进行剪枝操作\n",
        "\n",
        "        # 每隔若干个 epoch 进行规则剪枝\n",
        "        if (epoch + 1) % 50 == 0 and epoch < pruning_stop_epoch:\n",
        "            pruned = model.prune_rules(\n",
        "                threshold=attention_threshold,\n",
        "                X_val=X_val_scaled_tensor,\n",
        "                y_val=y_val_scaled_tensor,\n",
        "                performance_drop_tolerance=0.01,\n",
        "                best_val_loss=best_val_loss\n",
        "            )\n",
        "            if pruned:\n",
        "                print(f\"Epoch {epoch+1}: Pruned rules. Total rules: {model.n_rules}\")\n",
        "                # 重新初始化优化器和调度器\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - epoch - 1)\n",
        "                print(\"Optimizer and scheduler re-initialized after pruning rules.\")\n",
        "        elif epoch >= pruning_stop_epoch:\n",
        "            pass  # 不再进行规则剪枝\n",
        "\n",
        "    # 在训练结束后，加载最佳模型的状态字典\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(\"Loaded the best model based on validation loss.\")\n",
        "    else:\n",
        "        print(\"No improvement during training. Using the final model.\")\n",
        "\n",
        "    # 返回最佳模型和标准化器\n",
        "    return model, scaler_X, scaler_y\n",
        "\n",
        "# ============================\n",
        "# 使用更新后的模型进行训练和测试\n",
        "# ============================\n",
        "# 定义实验参数\n",
        "initial_n_rules = 3\n",
        "learning_rate = 0.01\n",
        "epochs = 500\n",
        "batch_size = 512\n",
        "prune_frequency = 25\n",
        "prune_threshold = 0.1\n",
        "repeats = 1  # 重复次数\n",
        "\n",
        "# 记录实验结果\n",
        "results_sofenn = []\n",
        "test_rmse_list = []\n",
        "time_list = []\n",
        "for repeat in range(repeats):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 进一步将训练集拆分为训练和验证集\n",
        "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "        X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "    )\n",
        "\n",
        "    # 定义最佳模型保存路径\n",
        "    best_model_path = f'best_sofenn_model_repeat{repeat+1}.pth'\n",
        "\n",
        "    # 训练模型\n",
        "    sofenn_model, scaler_X, scaler_y = train_attention_dynamic_attribute_and_rule_sofenn(\n",
        "        X_train_sub, y_train_sub, X_test_np, y_test_np,\n",
        "        initial_n_rules=initial_n_rules,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        lr=learning_rate,\n",
        "        prune_frequency=prune_frequency,\n",
        "        prune_threshold=prune_threshold,\n",
        "        best_model_path=best_model_path\n",
        "    )\n",
        "\n",
        "    # 在测试集上测试模型\n",
        "    sofenn_model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        y_pred_scaled = sofenn_model.infer(X_test_tensor)\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "        y_true = y_test_np  # 使用原始的 y_test_np\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        test_rmse_list.append(test_rmse)\n",
        "    # 记录时间\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    time_list.append(time_taken)\n",
        "    print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "    # 提取模糊规则\n",
        "    rules = sofenn_model.extract_rules(scaler_X, scaler_y, feature_names=features_to_use)\n",
        "    # 打印模糊规则\n",
        "    print(f\"\\nFuzzy Rules for Repeat={repeat+1}:\")\n",
        "    for rule in rules:\n",
        "        print(rule)\n",
        "        print()\n",
        "\n",
        "    # 保存模型\n",
        "    torch.save(sofenn_model.state_dict(), f'sofenn_model_repeat{repeat+1}.pth')\n",
        "\n",
        "    # 保存结果\n",
        "    result = {\n",
        "        'repeat': repeat + 1,\n",
        "        'test_rmse': test_rmse,\n",
        "        'time_taken': time_taken,\n",
        "        'total_active_attributes': np.sum(sofenn_model.fuzzy_layer.attribute_mask.detach().cpu().numpy())\n",
        "    }\n",
        "    results_sofenn.append(result)\n",
        "\n",
        "# 打印所有实验的结果\n",
        "for res in results_sofenn:\n",
        "    print(f\"Repeat {res['repeat']}: Test RMSE={res['test_rmse']:.4f}, Time={res['time_taken']:.2f}s, Total Active Attributes={res['total_active_attributes']}\")\n",
        "\n",
        "# 计算平均 RMSE 和时间\n",
        "test_rmse_mean = np.mean(test_rmse_list)\n",
        "test_rmse_std = np.std(test_rmse_list)\n",
        "time_mean = np.mean(time_list)\n",
        "time_std = np.std(time_list)\n",
        "\n",
        "# 打印结果\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "\n",
        "# 计算平均的总属性数量\n",
        "total_attributes_list = [res['total_active_attributes'] for res in results_sofenn]\n",
        "average_total_attributes = np.mean(total_attributes_list)\n",
        "print(f\"\\nAverage Total Active Attributes over {repeats} repeats: {average_total_attributes:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oovtQQTxv7J3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R_mVFkmbM0q"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# 从 UCI ML Repo 导入数据集\n",
        "# from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 从 UCI ML Repo 下载 Appliances Energy Prediction 数据集\n",
        "energy_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv'\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(energy_url)\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'date' 和 'Appliances'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4',\n",
        "    'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9',\n",
        "    'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
        "    'Tdewpoint', 'rv1', 'rv2'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# featu\n",
        "# 定义 FuBiNFS 算法的实现\n",
        "def fubinfs(X_train_np, y_train_np, X_test_np, y_test_np, n_clusters=3, max_iter=100, tol=1e-5, m=2):\n",
        "    \"\"\"\n",
        "    实现 FuBiNFS 算法并在测试集上评估性能。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集输入数据，形状：(K, D)\n",
        "    - y_train_np: 训练集目标数据，形状：(K,)\n",
        "    - X_test_np: 测试集输入数据，形状：(N_test, D)\n",
        "    - y_test_np: 测试集目标数据，形状：(N_test,)\n",
        "    - n_clusters: 聚类数目 C\n",
        "    - max_iter: 最大迭代次数\n",
        "    - tol: 收敛阈值\n",
        "    - m: 模糊化系数\n",
        "\n",
        "    返回：\n",
        "    - y_pred_test: 测试集的预测输出\n",
        "    - test_rmse: 测试集上的 RMSE\n",
        "    - 其他中间结果\n",
        "    \"\"\"\n",
        "    K, D = X_train_np.shape\n",
        "    N_test = X_test_np.shape[0]\n",
        "\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()\n",
        "    y_test_scaled = scaler_y.transform(y_test_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 初始化 U\n",
        "    np.random.seed(42)\n",
        "    U = np.random.rand(n_clusters, K, D)\n",
        "    # 归一化 U，使其满足约束条件 (5)\n",
        "    U = U / U.sum(axis=0, keepdims=1)\n",
        "\n",
        "    # 初始化聚类中心 V^(k) 和 V^(d)\n",
        "    V_k = np.random.rand(n_clusters, D)\n",
        "    V_d = np.random.rand(n_clusters, K)\n",
        "\n",
        "    previous_J = np.inf\n",
        "    for iteration in range(max_iter):\n",
        "        # 保存上一轮的 U，用于检查收敛\n",
        "        U_old = U.copy()\n",
        "\n",
        "        # Step 1: 计算聚类中心 V^(k) among objects，公式 (3)\n",
        "        for c in range(n_clusters):\n",
        "            numerator = np.sum((U[c, :, :] ** m) * X_train_scaled, axis=0)  # 修正此处\n",
        "            denominator = np.sum(U[c, :, :] ** m, axis=0)  # 修正此处\n",
        "            V_k[c] = numerator / (denominator + 1e-8)\n",
        "\n",
        "        # Step 2: 计算聚类中心 V^(d) among attributes，公式 (4)\n",
        "        for c in range(n_clusters):\n",
        "            numerator = np.sum((U[c, :, :] ** m) * X_train_scaled, axis=1)  # 修正此处\n",
        "            denominator = np.sum(U[c, :, :] ** m, axis=1)  # 修正此处\n",
        "            V_d[c] = numerator / (denominator + 1e-8)\n",
        "\n",
        "        # Step 3: 更新隶属度矩阵 U，公式 (14)\n",
        "        for c in range(n_clusters):\n",
        "            # 计算距离矩阵\n",
        "            dist_c = (X_train_scaled - V_k[c]) ** 2 + (X_train_scaled - V_d[c][:, np.newaxis]) ** 2  # 形状 (K, D)\n",
        "            # 初始化分母\n",
        "            denom = np.zeros((K, D))\n",
        "            for cc in range(n_clusters):\n",
        "                dist_cc = (X_train_scaled - V_k[cc]) ** 2 + (X_train_scaled - V_d[cc][:, np.newaxis]) ** 2\n",
        "                denom += (dist_c / (dist_cc + 1e-8)) ** (1 / (m - 1))\n",
        "            U[c] = 1 / (denom + 1e-8)\n",
        "\n",
        "        # 归一化 U，使其满足约束条件 (5)\n",
        "        U = U / U.sum(axis=0, keepdims=1)\n",
        "\n",
        "        # Step 4: 计算目标函数 J，公式 (2)\n",
        "        J = 0\n",
        "        for c in range(n_clusters):\n",
        "            J += np.sum((U[c] ** m) * (\n",
        "                (X_train_scaled - V_k[c]) ** 2 + (X_train_scaled - V_d[c][:, np.newaxis]) ** 2\n",
        "            ))\n",
        "\n",
        "        # 检查收敛条件\n",
        "        if abs(J - previous_J) < tol:\n",
        "            print(f\"Converged at iteration {iteration + 1}\")\n",
        "            break\n",
        "        previous_J = J\n",
        "\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "    # Step 5: 生成模糊规则\n",
        "    # 使用高斯隶属函数，标准差 σ 可以设为聚类中心的标准差\n",
        "    sigma_k = np.std(V_k, axis=0) + 1e-8  # 防止为零\n",
        "\n",
        "    # Step 6: 对训练集进行模糊推理并进行后验训练以拟合 y_train_scaled\n",
        "    # 计算规则激活度（训练集）\n",
        "    activation_train = np.zeros((K, n_clusters))\n",
        "    for c in range(n_clusters):\n",
        "        # 计算隶属度\n",
        "        mu_k = np.exp(-0.5 * ((X_train_scaled - V_k[c]) ** 2) / (sigma_k ** 2))\n",
        "        activation_train[:, c] = np.prod(mu_k, axis=1)\n",
        "\n",
        "    # 归一化激活度\n",
        "    total_activation_train = activation_train.sum(axis=1, keepdims=True) + 1e-8\n",
        "    normalized_activation_train = activation_train / total_activation_train\n",
        "\n",
        "    # 使用归一化激活度作为特征，训练线性模型拟合 y_train_scaled\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(normalized_activation_train, y_train_scaled)\n",
        "\n",
        "    # Step 7: 对测试集进行模糊推理并预测输出\n",
        "    activation_test = np.zeros((N_test, n_clusters))\n",
        "    for c in range(n_clusters):\n",
        "        # 计算隶属度\n",
        "        mu_k = np.exp(-0.5 * ((X_test_scaled - V_k[c]) ** 2) / (sigma_k ** 2))\n",
        "        activation_test[:, c] = np.prod(mu_k, axis=1)\n",
        "\n",
        "    # 归一化激活度\n",
        "    total_activation_test = activation_test.sum(axis=1, keepdims=True) + 1e-8\n",
        "    normalized_activation_test = activation_test / total_activation_test\n",
        "\n",
        "    # 使用线性模型预测\n",
        "    y_pred_test_scaled = lin_reg.predict(normalized_activation_test)\n",
        "    # 反标准化\n",
        "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 计算测试集上的 RMSE\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_test))\n",
        "\n",
        "    # 返回结果\n",
        "    # 添加提取的模糊规则\n",
        "    rules = []\n",
        "    V_k_orig = V_k * scaler_X.scale_ + scaler_X.mean_\n",
        "    sigma_k_orig = sigma_k * scaler_X.scale_\n",
        "    for c in range(n_clusters):\n",
        "        antecedent = []\n",
        "        for d in range(len(features_to_use)):\n",
        "            c_val = V_k_orig[c, d]\n",
        "            sigma_val = sigma_k_orig[d]\n",
        "            antecedent.append(f\"{features_to_use[d]} is Gaussian(c={c_val:.4f}, σ={sigma_val:.4f})\")\n",
        "        antecedent_str = \" AND \".join(antecedent)\n",
        "        # 结论部分使用线性模型的系数\n",
        "        coef = lin_reg.coef_[c]\n",
        "        intercept = lin_reg.intercept_\n",
        "        consequent_str = f\"{coef:.4f} * Activation_{c+1} + {intercept:.4f}\"\n",
        "        rule = f\"Rule {c+1}: IF {antecedent_str} THEN Output = {consequent_str}\"\n",
        "        rules.append(rule)\n",
        "\n",
        "    return y_pred_test, test_rmse, {\n",
        "        'V_k': V_k,\n",
        "        'sigma_k': sigma_k,\n",
        "        'lin_reg': lin_reg,\n",
        "        'scaler_X': scaler_X,\n",
        "        'scaler_y': scaler_y,\n",
        "        'activation_test': activation_test,\n",
        "        'rules': rules  # 添加模糊规则到返回结果\n",
        "    }\n",
        "\n",
        "# 实验参数\n",
        "n_clusters_list = [3, 5, 7, 9]  # 可以测试不同的聚类数量\n",
        "max_iters = 100\n",
        "tol = 1e-5\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results_fubinfs', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results_fubinfs = []\n",
        "\n",
        "for n_clusters in n_clusters_list:\n",
        "    test_rmse_list = []\n",
        "    time_list = []\n",
        "    print(f\"\\nStarting experiments for n_clusters={n_clusters}\")\n",
        "    for repeat in range(repeats):\n",
        "        start_time = time.time()\n",
        "        # 调用 FuBiNFS 算法\n",
        "        y_pred_test, test_rmse, intermediate_results = fubinfs(\n",
        "            X_train_np, y_train_np, X_test_np, y_test_np,\n",
        "            n_clusters=n_clusters, max_iter=max_iters, tol=tol, m=2\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        time_list.append(time_taken)\n",
        "        test_rmse_list.append(test_rmse)\n",
        "        print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "        # 提取模糊规则\n",
        "        rules = intermediate_results['rules']\n",
        "\n",
        "        # 打印模糊规则\n",
        "        print(f\"\\nFuzzy Rules for n_clusters={n_clusters}, Repeat={repeat+1}:\")\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "            print()\n",
        "\n",
        "        # 保存规则到文件\n",
        "        with open(f'results_fubinfs/rules_nclusters{n_clusters}_repeat{repeat+1}.txt', 'w') as f:\n",
        "            for rule in rules:\n",
        "                f.write(rule + '\\n')\n",
        "\n",
        "    # 计算平均 RMSE 和时间\n",
        "    test_rmse_mean = np.mean(test_rmse_list)\n",
        "    test_rmse_std = np.std(test_rmse_list)\n",
        "    time_mean = np.mean(time_list)\n",
        "    time_std = np.std(time_list)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\nResults for n_clusters={n_clusters}:\")\n",
        "    print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "    print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "\n",
        "    # 保存结果\n",
        "    result = {\n",
        "        'n_clusters': n_clusters,\n",
        "        'test_rmse_mean': test_rmse_mean,\n",
        "        'test_rmse_std': test_rmse_std,\n",
        "        'time_mean': time_mean,\n",
        "        'time_std': time_std\n",
        "    }\n",
        "    results_fubinfs.append(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WqqiEDsJqvE"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# 从 UCI ML Repo 导入数据集\n",
        "# from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "energy_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv'\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(energy_url)\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'date' 和 'Appliances'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4',\n",
        "    'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9',\n",
        "    'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
        "    'Tdewpoint', 'rv1', 'rv2'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['Appliances']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# 定义 ANFIS 模型\n",
        "# 定义 FuBiNFS 算法的实现\n",
        "def fubinfs(X_train_np, y_train_np, X_test_np, y_test_np, n_clusters=3, max_iter=100, tol=1e-5, m=2):\n",
        "    \"\"\"\n",
        "    实现 FuBiNFS 算法并在测试集上评估性能。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集输入数据，形状：(K, D)\n",
        "    - y_train_np: 训练集目标数据，形状：(K,)\n",
        "    - X_test_np: 测试集输入数据，形状：(N_test, D)\n",
        "    - y_test_np: 测试集目标数据，形状：(N_test,)\n",
        "    - n_clusters: 聚类数目 C\n",
        "    - max_iter: 最大迭代次数\n",
        "    - tol: 收敛阈值\n",
        "    - m: 模糊化系数\n",
        "\n",
        "    返回：\n",
        "    - y_pred_test: 测试集的预测输出\n",
        "    - test_rmse: 测试集上的 RMSE\n",
        "    - 其他中间结果\n",
        "    \"\"\"\n",
        "    K, D = X_train_np.shape\n",
        "    N_test = X_test_np.shape[0]\n",
        "\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()\n",
        "    y_test_scaled = scaler_y.transform(y_test_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 初始化 U\n",
        "    np.random.seed(42)\n",
        "    U = np.random.rand(n_clusters, K, D)\n",
        "    # 归一化 U，使其满足约束条件 (5)\n",
        "    U = U / U.sum(axis=0, keepdims=1)\n",
        "\n",
        "    # 初始化聚类中心 V^(k) 和 V^(d)\n",
        "    V_k = np.random.rand(n_clusters, D)\n",
        "    V_d = np.random.rand(n_clusters, K)\n",
        "\n",
        "    previous_J = np.inf\n",
        "    for iteration in range(max_iter):\n",
        "        # 保存上一轮的 U，用于检查收敛\n",
        "        U_old = U.copy()\n",
        "\n",
        "        # Step 1: 计算聚类中心 V^(k) among objects，公式 (3)\n",
        "        for c in range(n_clusters):\n",
        "            numerator = np.sum((U[c, :, :] ** m) * X_train_scaled, axis=0)  # 修正此处\n",
        "            denominator = np.sum(U[c, :, :] ** m, axis=0)  # 修正此处\n",
        "            V_k[c] = numerator / (denominator + 1e-8)\n",
        "\n",
        "        # Step 2: 计算聚类中心 V^(d) among attributes，公式 (4)\n",
        "        for c in range(n_clusters):\n",
        "            numerator = np.sum((U[c, :, :] ** m) * X_train_scaled, axis=1)  # 修正此处\n",
        "            denominator = np.sum(U[c, :, :] ** m, axis=1)  # 修正此处\n",
        "            V_d[c] = numerator / (denominator + 1e-8)\n",
        "\n",
        "        # Step 3: 更新隶属度矩阵 U，公式 (14)\n",
        "        for c in range(n_clusters):\n",
        "            # 计算距离矩阵\n",
        "            dist_c = (X_train_scaled - V_k[c]) ** 2 + (X_train_scaled - V_d[c][:, np.newaxis]) ** 2  # 形状 (K, D)\n",
        "            # 初始化分母\n",
        "            denom = np.zeros((K, D))\n",
        "            for cc in range(n_clusters):\n",
        "                dist_cc = (X_train_scaled - V_k[cc]) ** 2 + (X_train_scaled - V_d[cc][:, np.newaxis]) ** 2\n",
        "                denom += (dist_c / (dist_cc + 1e-8)) ** (1 / (m - 1))\n",
        "            U[c] = 1 / (denom + 1e-8)\n",
        "\n",
        "        # 归一化 U，使其满足约束条件 (5)\n",
        "        U = U / U.sum(axis=0, keepdims=1)\n",
        "\n",
        "        # Step 4: 计算目标函数 J，公式 (2)\n",
        "        J = 0\n",
        "        for c in range(n_clusters):\n",
        "            J += np.sum((U[c] ** m) * (\n",
        "                (X_train_scaled - V_k[c]) ** 2 + (X_train_scaled - V_d[c][:, np.newaxis]) ** 2\n",
        "            ))\n",
        "\n",
        "        # 检查收敛条件\n",
        "        if abs(J - previous_J) < tol:\n",
        "            print(f\"Converged at iteration {iteration + 1}\")\n",
        "            break\n",
        "        previous_J = J\n",
        "\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "    # Step 5: 生成模糊规则\n",
        "    # 使用高斯隶属函数，标准差 σ 可以设为聚类中心的标准差\n",
        "    sigma_k = np.std(V_k, axis=0) + 1e-8  # 防止为零\n",
        "\n",
        "    # Step 6: 对训练集进行模糊推理并进行后验训练以拟合 y_train_scaled\n",
        "    # 计算规则激活度（训练集）\n",
        "    activation_train = np.zeros((K, n_clusters))\n",
        "    for c in range(n_clusters):\n",
        "        # 计算隶属度\n",
        "        mu_k = np.exp(-0.5 * ((X_train_scaled - V_k[c]) ** 2) / (sigma_k ** 2))\n",
        "        activation_train[:, c] = np.prod(mu_k, axis=1)\n",
        "\n",
        "    # 归一化激活度\n",
        "    total_activation_train = activation_train.sum(axis=1, keepdims=True) + 1e-8\n",
        "    normalized_activation_train = activation_train / total_activation_train\n",
        "\n",
        "    # 使用归一化激活度作为特征，训练线性模型拟合 y_train_scaled\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(normalized_activation_train, y_train_scaled)\n",
        "\n",
        "    # Step 7: 对测试集进行模糊推理并预测输出\n",
        "    activation_test = np.zeros((N_test, n_clusters))\n",
        "    for c in range(n_clusters):\n",
        "        # 计算隶属度\n",
        "        mu_k = np.exp(-0.5 * ((X_test_scaled - V_k[c]) ** 2) / (sigma_k ** 2))\n",
        "        activation_test[:, c] = np.prod(mu_k, axis=1)\n",
        "\n",
        "    # 归一化激活度\n",
        "    total_activation_test = activation_test.sum(axis=1, keepdims=True) + 1e-8\n",
        "    normalized_activation_test = activation_test / total_activation_test\n",
        "\n",
        "    # 使用线性模型预测\n",
        "    y_pred_test_scaled = lin_reg.predict(normalized_activation_test)\n",
        "    # 反标准化\n",
        "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 计算测试集上的 RMSE\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_test))\n",
        "\n",
        "    # 返回结果\n",
        "    # 添加提取的模糊规则\n",
        "    rules = []\n",
        "    V_k_orig = V_k * scaler_X.scale_ + scaler_X.mean_\n",
        "    sigma_k_orig = sigma_k * scaler_X.scale_\n",
        "    for c in range(n_clusters):\n",
        "        antecedent = []\n",
        "        for d in range(len(features_to_use)):\n",
        "            c_val = V_k_orig[c, d]\n",
        "            sigma_val = sigma_k_orig[d]\n",
        "            antecedent.append(f\"{features_to_use[d]} is Gaussian(c={c_val:.4f}, σ={sigma_val:.4f})\")\n",
        "        antecedent_str = \" AND \".join(antecedent)\n",
        "        # 结论部分使用线性模型的系数\n",
        "        coef = lin_reg.coef_[c]\n",
        "        intercept = lin_reg.intercept_\n",
        "        consequent_str = f\"{coef:.4f} * Activation_{c+1} + {intercept:.4f}\"\n",
        "        rule = f\"Rule {c+1}: IF {antecedent_str} THEN Output = {consequent_str}\"\n",
        "        rules.append(rule)\n",
        "\n",
        "    return y_pred_test, test_rmse, {\n",
        "        'V_k': V_k,\n",
        "        'sigma_k': sigma_k,\n",
        "        'lin_reg': lin_reg,\n",
        "        'scaler_X': scaler_X,\n",
        "        'scaler_y': scaler_y,\n",
        "        'activation_test': activation_test,\n",
        "        'rules': rules  # 添加模糊规则到返回结果\n",
        "    }\n",
        "\n",
        "# 实验参数\n",
        "n_clusters_list = [3, 5, 7, 9]  # 可以测试不同的聚类数量\n",
        "max_iters = 100\n",
        "tol = 1e-5\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results_fubinfs', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results_fubinfs = []\n",
        "\n",
        "for n_clusters in n_clusters_list:\n",
        "    test_rmse_list = []\n",
        "    time_list = []\n",
        "    print(f\"\\nStarting experiments for n_clusters={n_clusters}\")\n",
        "    for repeat in range(repeats):\n",
        "        start_time = time.time()\n",
        "        # 调用 FuBiNFS 算法\n",
        "        y_pred_test, test_rmse, intermediate_results = fubinfs(\n",
        "            X_train_np, y_train_np, X_test_np, y_test_np,\n",
        "            n_clusters=n_clusters, max_iter=max_iters, tol=tol, m=2\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        time_list.append(time_taken)\n",
        "        test_rmse_list.append(test_rmse)\n",
        "        print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "        # 提取模糊规则\n",
        "        rules = intermediate_results['rules']\n",
        "\n",
        "        # 打印模糊规则\n",
        "        print(f\"\\nFuzzy Rules for n_clusters={n_clusters}, Repeat={repeat+1}:\")\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "            print()\n",
        "\n",
        "        # # 保存规则到文件\n",
        "        # with open(f'results_fubinfs/rules_nclusters{n_clusters}_repeat{repeat+1}.txt', 'w') as f:\n",
        "        #     for rule in rules:\n",
        "        #         f.write(rule + '\\n')\n",
        "\n",
        "    # 计算平均 RMSE 和时间\n",
        "    test_rmse_mean = np.mean(test_rmse_list)\n",
        "    test_rmse_std = np.std(test_rmse_list)\n",
        "    time_mean = np.mean(time_list)\n",
        "    time_std = np.std(time_list)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\nResults for n_clusters={n_clusters}:\")\n",
        "    print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "    print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "\n",
        "    # 保存结果\n",
        "    result = {\n",
        "        'n_clusters': n_clusters,\n",
        "        'test_rmse_mean': test_rmse_mean,\n",
        "        'test_rmse_std': test_rmse_std,\n",
        "        'time_mean': time_mean,\n",
        "        'time_std': time_std\n",
        "    }\n",
        "    results_fubinfs.append(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78VrMXfgLti"
      },
      "source": [
        "# ANFIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbVGfqKG0JAX"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.stats import norm  # 新增\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# !pip install ucimlrepo\n",
        "\n",
        "# 从 UCI ML Repo 导入 Auto MPG 数据集\n",
        "auto_mpg_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "\n",
        "# 定义列名称\n",
        "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "                'acceleration', 'model_year', 'origin', 'car_name']\n",
        "\n",
        "# 读取数据集，处理缺失值\n",
        "data = pd.read_csv(auto_mpg_url, delim_whitespace=True, names=column_names, na_values='?')\n",
        "\n",
        "# 删除含有缺失值的样本\n",
        "data = data.dropna()\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'mpg' 和 'car_name'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "    'acceleration', 'model_year', 'origin'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['mpg']\n",
        "\n",
        "# 将类别变量 'origin' 进行独热编码（如果需要，可以选择保留为数值型）\n",
        "# 这里保留为数值型，以简化 ANFIS 模型的处理\n",
        "# 如果希望进行独热编码，请取消下方代码的注释\n",
        "# X = pd.get_dummies(X, columns=['origin'], drop_first=True)\n",
        "# features_to_use = X.columns.tolist()\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本（已在读取时完成）\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# 定义 ANFIS 模型\n",
        "class ANFIS(nn.Module):\n",
        "    def __init__(self, n_inputs, n_rules, learning_rate=1e-2, param_count=1):\n",
        "        super(ANFIS, self).__init__()\n",
        "        self.n = n_inputs\n",
        "        self.m = n_rules\n",
        "        self.param_count = param_count\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.mu = nn.ParameterList([nn.Parameter(torch.randn(n_rules, n_inputs)) for _ in range(param_count)])\n",
        "        self.sigma = nn.ParameterList([nn.Parameter(torch.abs(torch.randn(n_rules, n_inputs))) for _ in range(param_count)])\n",
        "        self.y = nn.ParameterList([nn.Parameter(torch.randn(1, n_rules)) for _ in range(param_count)])\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = None  # 优化器将在外部设置\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x_expanded = x.unsqueeze(1).expand(batch_size, self.m, self.n)\n",
        "\n",
        "        rul = torch.ones(batch_size, self.m, self.n).to(x.device)\n",
        "\n",
        "        for i in range(self.param_count):\n",
        "            mu_expanded = self.mu[i].unsqueeze(0).expand(batch_size, self.m, self.n)\n",
        "            sigma_expanded = self.sigma[i].unsqueeze(0).expand(batch_size, self.m, self.n)\n",
        "            rul *= torch.exp(-0.5 * ((x_expanded - mu_expanded) ** 2) / (sigma_expanded ** 2 + 1e-8))\n",
        "\n",
        "        rul = rul.prod(dim=2)\n",
        "\n",
        "        # Compute output\n",
        "        num = sum((rul * self.y[i].expand_as(rul)) for i in range(self.param_count))\n",
        "        den = rul.sum(dim=1, keepdim=True).clamp(min=1e-12)\n",
        "        out = num.sum(dim=1, keepdim=True) / den\n",
        "        return out.squeeze()\n",
        "\n",
        "    def train_step(self, x, target):\n",
        "        if self.optimizer is None:\n",
        "            raise ValueError(\"Optimizer not set. Please set the optimizer before training.\")\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.forward(x)\n",
        "        loss = nn.functional.mse_loss(output, target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item(), output\n",
        "\n",
        "    def infer(self, x, targets=None):\n",
        "        with torch.no_grad():\n",
        "            output = self.forward(x)\n",
        "            if targets is None:\n",
        "                return output\n",
        "            else:\n",
        "                loss = nn.functional.mse_loss(output, targets)\n",
        "                return output, loss.item()\n",
        "\n",
        "    def extract_rules(self, scaler_X, scaler_y, feature_names=None):\n",
        "        \"\"\"\n",
        "        提取 ANFIS 模型的模糊规则。\n",
        "\n",
        "        参数：\n",
        "        - scaler_X: 输入数据的标准化器\n",
        "        - scaler_y: 输出数据的标准化器\n",
        "        - feature_names: 特征名称列表\n",
        "\n",
        "        返回：\n",
        "        - rules: 包含规则字符串的列表\n",
        "        \"\"\"\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Input {i+1}' for i in range(self.n)]\n",
        "\n",
        "        rules = []\n",
        "        for r in range(self.m):\n",
        "            antecedents = []\n",
        "            for j in range(self.n):\n",
        "                # 对 mu 和 sigma 进行反标准化\n",
        "                mu_values = []\n",
        "                sigma_values = []\n",
        "                for i in range(self.param_count):\n",
        "                    mu = self.mu[i][r, j].detach().cpu().numpy()\n",
        "                    sigma = self.sigma[i][r, j].detach().cpu().numpy()\n",
        "                    # 反标准化\n",
        "                    mu_orig = mu * scaler_X.scale_[j] + scaler_X.mean_[j]\n",
        "                    sigma_orig = sigma * scaler_X.scale_[j]\n",
        "                    mu_values.append(mu_orig)\n",
        "                    sigma_values.append(sigma_orig)\n",
        "                # 计算平均值\n",
        "                mu_avg = np.mean(mu_values)\n",
        "                sigma_avg = np.mean(sigma_values)\n",
        "                antecedents.append(f\"{feature_names[j]} is Gaussian(c={mu_avg:.4f}, σ={sigma_avg:.4f})\")\n",
        "            antecedent_str = \" AND \".join(antecedents)\n",
        "            # 后件部分\n",
        "            consequents = []\n",
        "            for i in range(self.param_count):\n",
        "                y_value = self.y[i][0, r].detach().cpu().numpy()\n",
        "                # 反标准化\n",
        "                y_orig = y_value * scaler_y.scale_[0] + scaler_y.mean_[0]\n",
        "                consequents.append(y_orig)  # 直接添加数值而非字符串\n",
        "            consequent_avg = np.mean(consequents)  # 计算数值平均\n",
        "            rule = f\"Rule {r+1}: IF {antecedent_str} THEN Output = {consequent_avg:.4f}\"\n",
        "            rules.append(rule)\n",
        "        return rules\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "# 定义计算 Iov 和 Ifspe 的辅助函数\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    \"\"\"\n",
        "    使用解析解计算两个高斯隶属度函数的重叠面积。\n",
        "\n",
        "    参数：\n",
        "    - c1, sigma1: 第一个高斯函数的中心和标准差\n",
        "    - c2, sigma2: 第二个高斯函数的中心和标准差\n",
        "\n",
        "    返回：\n",
        "    - overlap_area: 两个高斯函数的重叠面积\n",
        "    \"\"\"\n",
        "    denominator = np.sqrt(sigma1**2 + sigma2**2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(mu_orig, sigma_orig):\n",
        "    \"\"\"\n",
        "    计算 Average Overlap Index (Iov)。\n",
        "\n",
        "    参数：\n",
        "    - mu_orig: 反标准化后的 mu 数组，形状为 (n_rules, n_inputs)\n",
        "    - sigma_orig: 反标准化后的 sigma 数组，形状为 (n_rules, n_inputs)\n",
        "\n",
        "    返回：\n",
        "    - average_iov: 平均重叠指数\n",
        "    \"\"\"\n",
        "    n_rules, n_inputs = mu_orig.shape\n",
        "    total_overlap = []\n",
        "    for i in range(n_inputs):\n",
        "        for j in range(n_rules):\n",
        "            for k in range(j+1, n_rules):\n",
        "                c1 = mu_orig[j, i]\n",
        "                sigma1 = sigma_orig[j, i]\n",
        "                c2 = mu_orig[k, i]\n",
        "                sigma2 = sigma_orig[k, i]\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                total_overlap.append(overlap)\n",
        "    if len(total_overlap) == 0:\n",
        "        return 0\n",
        "    average_iov = np.mean(total_overlap)\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(mu_orig, sigma_orig):\n",
        "    \"\"\"\n",
        "    计算 Average Fuzzy Set Position Index (Ifspe)。\n",
        "\n",
        "    参数：\n",
        "    - mu_orig: 反标准化后的 mu 数组，形状为 (n_rules, n_inputs)\n",
        "    - sigma_orig: 反标准化后的 sigma 数组，形状为 (n_rules, n_inputs)\n",
        "\n",
        "    返回：\n",
        "    - average_ifspe: 平均模糊集位置指数（非负数）\n",
        "    \"\"\"\n",
        "    n_rules, n_inputs = mu_orig.shape\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for i in range(n_inputs):\n",
        "        # 仅考虑有两个及以上规则的输入特征\n",
        "        if n_rules < 2:\n",
        "            continue\n",
        "        # 计算每个输入特征的中心值的标准差\n",
        "        std_dev = np.std(mu_orig[:, i])\n",
        "        # 使用绝对值确保 Ifspe_term 为非负数\n",
        "        ifspe_term = std_dev  # 直接使用标准差作为 Ifspe 的度量\n",
        "        total_ifspe += ifspe_term\n",
        "        valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_ifspe = total_ifspe / valid_terms\n",
        "    return average_ifspe\n",
        "\n",
        "# 定义训练 ANFIS 模型的函数\n",
        "def train_anfis(X_train_np, y_train_np, X_val_np, y_val_np, n_rules=3, epochs=500, batch_size=32, lr=0.01):\n",
        "    \"\"\"\n",
        "    训练 ANFIS 模型\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集输入数据，形状：(num_samples, n_inputs)\n",
        "    - y_train_np: 训练集目标数据，形状：(num_samples,)\n",
        "    - X_val_np: 验证集输入数据，形状：(num_samples, n_inputs)\n",
        "    - y_val_np: 验证集目标数据，形状：(num_samples,)\n",
        "    - n_rules: 规则数量\n",
        "    - epochs: 训练轮数\n",
        "    - batch_size: 批次大小\n",
        "    - lr: 学习率\n",
        "\n",
        "    返回：\n",
        "    - anfis_model: 训练好的模型\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - scaler_y: 输出数据的标准化器\n",
        "    \"\"\"\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    n_inputs = X_train_scaled_tensor.shape[1]\n",
        "\n",
        "    anfis_model = ANFIS(\n",
        "        n_inputs=n_inputs,\n",
        "        n_rules=n_rules,\n",
        "        learning_rate=lr,\n",
        "        param_count=1  # 您可以根据需要调整\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器\n",
        "    anfis_model.optimizer = optim.AdamW(anfis_model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        anfis_model.optimizer, mode='min', factor=0.99, patience=100, verbose=True\n",
        "    )\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_rmse': []\n",
        "    }\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in range(epochs):\n",
        "        anfis_model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            loss_train, _ = anfis_model.train_step(\n",
        "                batch_x,\n",
        "                batch_y\n",
        "            )\n",
        "            epoch_loss += loss_train\n",
        "            num_batches += 1\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        anfis_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_val_pred_scaled, loss_val = anfis_model.infer(X_val_scaled_tensor, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "        scheduler.step(loss_val)\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val)  # 修正这里，去掉 .item()\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "\n",
        "        # 显示验证集上的 RMSE\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {loss_val:.4f} - Val RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "    # 可视化训练过程\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_info['epoch'], training_info['train_loss'], label='Train Loss')\n",
        "    plt.plot(training_info['epoch'], training_info['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 可视化验证集上的 RMSE\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_info['epoch'], training_info['val_rmse'], label='Validation RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Validation RMSE over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 返回模型和标准化器\n",
        "    return anfis_model, scaler_X, scaler_y\n",
        "\n",
        "# 定义实验参数\n",
        "n_rules_list = [3]  # 可以测试不同的规则数量\n",
        "learning_rates = [0.01]\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results_anfis', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results_anfis = []\n",
        "\n",
        "for n_rules in n_rules_list:\n",
        "    for lr in learning_rates:\n",
        "        test_rmse_list = []\n",
        "        time_list = []\n",
        "        iov_list = []\n",
        "        ifspe_list = []\n",
        "        total_attributes_list = []  # 新增，用于跟踪总属性数量\n",
        "        print(f\"\\nStarting experiments for n_rules={n_rules}, learning_rate={lr}\")\n",
        "        for repeat in range(repeats):\n",
        "            start_time = time.time()\n",
        "            # 进一步将训练集拆分为训练和验证集\n",
        "            X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "                X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "            )\n",
        "\n",
        "            # 训练模型\n",
        "            anfis_model, scaler_X, scaler_y = train_anfis(\n",
        "                X_train_sub, y_train_sub, X_val_sub, y_val_sub,\n",
        "                n_rules=n_rules,\n",
        "                epochs=1500,\n",
        "                batch_size=512,\n",
        "                lr=lr\n",
        "            )\n",
        "\n",
        "            # 在测试集上测试模型\n",
        "            anfis_model.eval()\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            X_test_scaled = scaler_X.transform(X_test_np)\n",
        "            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "            y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "            with torch.no_grad():\n",
        "                y_pred_scaled = anfis_model.infer(X_test_tensor)\n",
        "                y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                y_true = y_test_np  # Original unstandardized y_test\n",
        "                test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "                test_rmse_list.append(test_rmse)\n",
        "\n",
        "            # 记录时间\n",
        "            end_time = time.time()\n",
        "            time_taken = end_time - start_time\n",
        "            time_list.append(time_taken)\n",
        "\n",
        "            # 提取 mu 和 sigma，计算 Iov 和 Ifspe\n",
        "            mu = anfis_model.mu[0].detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "            sigma = anfis_model.sigma[0].detach().cpu().numpy()  # (n_rules, n_inputs)\n",
        "            # 反标准化\n",
        "            mu_orig = mu * scaler_X.scale_.reshape(1, -1) + scaler_X.mean_.reshape(1, -1)\n",
        "            sigma_orig = sigma * scaler_X.scale_.reshape(1, -1)\n",
        "\n",
        "            # 计算 Iov 和 Ifspe\n",
        "            Iov = compute_iov(mu_orig, sigma_orig)\n",
        "            Ifspe = compute_ifspe(mu_orig, sigma_orig)\n",
        "\n",
        "            iov_list.append(Iov)\n",
        "            ifspe_list.append(Ifspe)\n",
        "\n",
        "            # 计算总属性数量（每个规则每个输入属性都被计算一次）\n",
        "            # 这里假设每个规则都使用所有输入属性（因为原始模型未进行属性剪枝）\n",
        "            total_attributes = n_rules * len(features_to_use)\n",
        "            total_attributes_list.append(total_attributes)\n",
        "\n",
        "            print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "            print(f\"Overlap Index (Iov): {Iov:.4f}, Fuzzy Set Position Index (Ifspe): {Ifspe:.4f}\")\n",
        "\n",
        "            # 提取模糊规则\n",
        "            rules = anfis_model.extract_rules(scaler_X, scaler_y, feature_names=features_to_use)\n",
        "            # 打印模糊规则\n",
        "            print(f\"\\nFuzzy Rules for n_rules={n_rules}, learning_rate={lr}, Repeat={repeat+1}:\")\n",
        "            for rule in rules:\n",
        "                print(rule)\n",
        "                print()\n",
        "\n",
        "            # 保存模型\n",
        "            torch.save(anfis_model.state_dict(), f'results_anfis/anfis_model_nrules{n_rules}_lr{lr}_repeat{repeat+1}.pth')\n",
        "\n",
        "        # 计算平均 RMSE 和时间\n",
        "        test_rmse_mean = np.mean(test_rmse_list)\n",
        "        test_rmse_std = np.std(test_rmse_list)\n",
        "        time_mean = np.mean(time_list)\n",
        "        time_std = np.std(time_list)\n",
        "\n",
        "        # 计算平均 Iov 和 Ifspe\n",
        "        Iov_mean = np.mean(iov_list)\n",
        "        Iov_std = np.std(iov_list)\n",
        "        Ifspe_mean = np.mean(ifspe_list)\n",
        "        Ifspe_std = np.std(ifspe_list)\n",
        "\n",
        "        # 计算平均总属性数量\n",
        "        average_total_attributes = np.mean(total_attributes_list)\n",
        "\n",
        "        # 打印结果\n",
        "        print(f\"\\nResults for n_rules={n_rules}, learning_rate={lr}:\")\n",
        "        print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "        print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "        print(f\"Average Overlap Index (Iov): {Iov_mean:.4f} ± {Iov_std:.4f}\")\n",
        "        print(f\"Average Fuzzy Set Position Index (Ifspe): {Ifspe_mean:.4f} ± {Ifspe_std:.4f}\")\n",
        "        print(f\"Average Total Number of Attributes Included in All Rules: {average_total_attributes:.2f}\")\n",
        "\n",
        "        # 保存结果\n",
        "        result = {\n",
        "            'n_rules': n_rules,\n",
        "            'learning_rate': lr,\n",
        "            'test_rmse_mean': test_rmse_mean,\n",
        "            'test_rmse_std': test_rmse_std,\n",
        "            'time_mean': time_mean,\n",
        "            'time_std': time_std,\n",
        "            'Iov_mean': Iov_mean,\n",
        "            'Iov_std': Iov_std,\n",
        "            'Ifspe_mean': Ifspe_mean,\n",
        "            'Ifspe_std': Ifspe_std,\n",
        "            'average_total_attributes': average_total_attributes\n",
        "        }\n",
        "        results_anfis.append(result)\n",
        "\n",
        "        # 保存结果到 CSV 文件\n",
        "        results_df = pd.DataFrame(results_anfis)\n",
        "        results_df.to_csv('results_anfis/anfis_experiments_results.csv', index=False)\n",
        "        print(\"\\n所有实验的结果已保存到 'results_anfis/anfis_experiments_results.csv'。\")\n",
        "\n",
        "        # 可视化重叠指数和位置指数\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(['Overlap Index (Iov)', 'Fuzzy Set Position Index (Ifspe)'], [Iov_mean, Ifspe_mean],\n",
        "                yerr=[Iov_std, Ifspe_std], capsize=5, color=['skyblue', 'salmon'])\n",
        "        plt.ylabel('Index Value')\n",
        "        plt.title('Average Interpretability Indices')\n",
        "        plt.grid(axis='y')\n",
        "        plt.show()\n",
        "\n",
        "        # 可视化平均总属性数量\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(['Average Total Attributes'], [average_total_attributes], color=['lightgreen'])\n",
        "        plt.ylabel('Number of Attributes')\n",
        "        plt.title('Average Total Number of Attributes Included in All Rules')\n",
        "        plt.grid(axis='y')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PpmTx3mgGLs"
      },
      "source": [
        "# SOFENN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gde2GrvL0cj2"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm  # 确保导入了 norm 函数\n",
        "\n",
        "# 从 UCI ML Repo 导入数据集\n",
        "# 注意：请确保安装了 `ucimlrepo` 包，如果没有，请使用以下命令安装：\n",
        "# !pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "# 从 UCI ML Repo 下载 Appliances Energy Prediction 数据集\n",
        "from sklearn.datasets import fetch_openml\n",
        "# 获取波士顿房价数据集\n",
        "# 从 UCI ML Repo 导入 YearPredictionMSD 数据集\n",
        "# 数据集链接：https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "# 从 UCI ML Repo 导入 YearPredictionMSD 数据集\n",
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip'\n",
        "\n",
        "# 定义列名称\n",
        "col_names = ['Year'] + [f'Feature_{i}' for i in range(1, 91)]\n",
        "\n",
        "# 读取数据集\n",
        "data = pd.read_csv(data_url, header=None, names=col_names)\n",
        "\n",
        "# 特征选择\n",
        "X = data.drop('Year', axis=1)\n",
        "y = data['Year']\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = X.columns.tolist()\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "features_to_use=feature_labels\n",
        "# 更新特征名称以便后续使用\n",
        "# feature_labels = features_to_use\n",
        "# 定义 FuzzyLayer 类（PyTorch 实现）\n",
        "class FuzzyLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(FuzzyLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # 初始化中心 c 和宽度 sigma\n",
        "        self.c = nn.Parameter(torch.randn(input_dim, output_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(input_dim, output_dim))  # 保持原始的初始化方式\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, input_dim)\n",
        "        # 计算高斯隶属度函数\n",
        "        # 扩展维度以进行广播\n",
        "        x_expanded = x.unsqueeze(2)  # (batch_size, input_dim, 1)\n",
        "        c_expanded = self.c.unsqueeze(0)  # (1, input_dim, output_dim)\n",
        "        sigma_expanded = self.sigma.unsqueeze(0) + 1e-8  # (1, input_dim, output_dim) 防止除以零\n",
        "\n",
        "        diff = x_expanded - c_expanded  # (batch_size, input_dim, output_dim)\n",
        "        exponent = -0.5 * (diff / sigma_expanded) ** 2\n",
        "        mu = torch.exp(exponent)  # (batch_size, input_dim, output_dim)\n",
        "\n",
        "        # 对每个神经元，计算所有输入特征的隶属度函数的乘积\n",
        "        phi = mu.prod(dim=1)  # (batch_size, output_dim)\n",
        "        return phi\n",
        "\n",
        "# 定义 NormalizedLayer 类\n",
        "class NormalizedLayer(nn.Module):\n",
        "    def forward(self, phi):\n",
        "        # phi: (batch_size, output_dim)\n",
        "        phi_sum = phi.sum(dim=1, keepdim=True) + 1e-8  # 防止除以零\n",
        "        psi = phi / phi_sum\n",
        "        return psi\n",
        "\n",
        "# 定义 WeightedLayer 类\n",
        "class WeightedLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(WeightedLayer, self).__init__()\n",
        "        # 初始化后件参数 a，包括偏置项\n",
        "        self.a = nn.Parameter(torch.randn(input_dim + 1, output_dim))\n",
        "\n",
        "    def forward(self, x, psi):\n",
        "        # x: (batch_size, input_dim)\n",
        "        # psi: (batch_size, output_dim)\n",
        "        batch_size = x.size(0)\n",
        "        # 添加偏置项\n",
        "        ones = torch.ones(batch_size, 1).to(x.device)\n",
        "        x_with_bias = torch.cat([ones, x], dim=1)  # (batch_size, input_dim + 1)\n",
        "        # 计算 w = x * a\n",
        "        w = torch.matmul(x_with_bias, self.a)  # (batch_size, output_dim)\n",
        "        f = psi * w  # (batch_size, output_dim)\n",
        "        return f\n",
        "\n",
        "# 定义 OutputLayer 类\n",
        "class OutputLayer(nn.Module):\n",
        "    def forward(self, f):\n",
        "        # f: (batch_size, output_dim)\n",
        "        output = f.sum(dim=1)  # (batch_size,)\n",
        "        return output\n",
        "\n",
        "# 定义 SOFENN 模型\n",
        "class SOFENN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, n_rules):\n",
        "        super(SOFENN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_rules = n_rules\n",
        "\n",
        "        self.fuzzy_layer = FuzzyLayer(input_dim, n_rules)\n",
        "        self.normalized_layer = NormalizedLayer()\n",
        "        self.weighted_layer = WeightedLayer(input_dim, n_rules)\n",
        "        self.output_layer = OutputLayer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        phi = self.fuzzy_layer(x)\n",
        "        psi = self.normalized_layer(phi)\n",
        "        f = self.weighted_layer(x, psi)\n",
        "        output = self.output_layer(f)\n",
        "        return output\n",
        "\n",
        "    def extract_rules(self, scaler_X, scaler_y, feature_names):\n",
        "        \"\"\"\n",
        "        提取 SOFENN 模型的模糊规则。\n",
        "\n",
        "        参数：\n",
        "        - scaler_X: 输入数据的标准化器\n",
        "        - scaler_y: 输出数据的标准化器\n",
        "        - feature_names: 特征名称列表\n",
        "\n",
        "        返回：\n",
        "        - rules: 包含规则字符串的列表\n",
        "        \"\"\"\n",
        "        rules = []\n",
        "        c = self.fuzzy_layer.c.detach().cpu().numpy()  # (input_dim, n_rules)\n",
        "        sigma = self.fuzzy_layer.sigma.detach().cpu().numpy()  # (input_dim, n_rules)\n",
        "        a = self.weighted_layer.a.detach().cpu().numpy()  # (input_dim + 1, n_rules)\n",
        "\n",
        "        # 反标准化\n",
        "        c_orig = c * scaler_X.scale_.reshape(-1, 1) + scaler_X.mean_.reshape(-1, 1)\n",
        "        sigma_orig = sigma * scaler_X.scale_.reshape(-1, 1)\n",
        "        a_orig = a.copy()\n",
        "        a_orig[0, :] = a[0, :]  # 偏置项无需反标准化\n",
        "        a_orig[1:, :] = a[1:, :] / scaler_X.scale_.reshape(-1, 1) * scaler_y.scale_[0]\n",
        "        a_orig[0, :] = a_orig[0, :] * scaler_y.scale_[0] + scaler_y.mean_[0] - np.sum(\n",
        "            (a[1:, :] * scaler_X.mean_.reshape(-1, 1) / scaler_X.scale_.reshape(-1, 1)), axis=0\n",
        "        ) * scaler_y.scale_[0]\n",
        "\n",
        "        for j in range(self.n_rules):\n",
        "            antecedents = []\n",
        "            for i in range(self.input_dim):\n",
        "                mu = c_orig[i, j]\n",
        "                sigma_i = sigma_orig[i, j]\n",
        "                antecedents.append(f\"{feature_names[i]} is Gaussian(c={mu:.4f}, σ={sigma_i:.4f})\")\n",
        "            antecedent_str = \" AND \".join(antecedents)\n",
        "            # 构建后件部分\n",
        "            a_j = a_orig[:, j]  # (input_dim + 1,)\n",
        "            consequent_terms = [f\"{a_j[0]:.4f}\"]\n",
        "            for idx, coef in enumerate(a_j[1:]):\n",
        "                coef = coef\n",
        "                if coef >= 0:\n",
        "                    term = f\"+ {coef:.4f} * {feature_names[idx]}\"\n",
        "                else:\n",
        "                    term = f\"- {abs(coef):.4f} * {feature_names[idx]}\"\n",
        "                consequent_terms.append(term)\n",
        "            consequent_str = \" \".join(consequent_terms)\n",
        "            rule = f\"Rule {j+1}: IF {antecedent_str} THEN Output = {consequent_str}\"\n",
        "            rules.append(rule)\n",
        "        return rules\n",
        "\n",
        "# 修改后的 Iov 和 Ifspe 计算函数\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    \"\"\"\n",
        "    使用解析解计算两个高斯隶属度函数的重叠面积。\n",
        "\n",
        "    参数：\n",
        "    - c1, sigma1: 第一个高斯函数的中心和标准差\n",
        "    - c2, sigma2: 第二个高斯函数的中心和标准差\n",
        "\n",
        "    返回：\n",
        "    - overlap_area: 两个高斯函数的重叠面积\n",
        "    \"\"\"\n",
        "    denominator = np.sqrt(sigma1**2 + sigma2**2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(c_orig, sigma_orig):\n",
        "    \"\"\"\n",
        "    计算 Average Overlap Index (Iov)。\n",
        "\n",
        "    参数：\n",
        "    - c_orig: 反标准化后的 c 数组，形状为 (input_dim, n_rules)\n",
        "    - sigma_orig: 反标准化后的 sigma 数组，形状为 (input_dim, n_rules)\n",
        "\n",
        "    返回：\n",
        "    - average_iov: 平均重叠指数\n",
        "    \"\"\"\n",
        "    input_dim, n_rules = c_orig.shape\n",
        "    total_max_overlap = 0\n",
        "    valid_attributes = 0\n",
        "\n",
        "    for i in range(input_dim):\n",
        "        c_i = c_orig[i, :]\n",
        "        sigma_i = sigma_orig[i, :]\n",
        "        # 假设所有规则都是活跃的\n",
        "        active_rules = np.arange(n_rules)\n",
        "\n",
        "        if len(active_rules) < 2:\n",
        "            continue  # 需要至少两个规则才能计算重叠\n",
        "\n",
        "        max_overlap = -np.inf\n",
        "        for j in range(len(active_rules)):\n",
        "            for k in range(j + 1, len(active_rules)):\n",
        "                c1 = c_i[active_rules[j]]\n",
        "                sigma1 = sigma_i[active_rules[j]]\n",
        "                c2 = c_i[active_rules[k]]\n",
        "                sigma2 = sigma_i[active_rules[k]]\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "        if max_overlap != -np.inf:\n",
        "            total_max_overlap += max_overlap\n",
        "            valid_attributes += 1\n",
        "\n",
        "    if valid_attributes == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_iov = total_max_overlap / valid_attributes\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(c_orig, sigma_orig):\n",
        "    \"\"\"\n",
        "    计算 Average Fuzzy Set Position Index (Ifspe)。\n",
        "\n",
        "    参数：\n",
        "    - c_orig: 反标准化后的 c 数组，形状为 (input_dim, n_rules)\n",
        "    - sigma_orig: 反标准化后的 sigma 数组，形状为 (input_dim, n_rules)\n",
        "\n",
        "    返回：\n",
        "    - average_ifspe: 平均模糊集位置指数\n",
        "    \"\"\"\n",
        "    input_dim, n_rules = c_orig.shape\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for i in range(input_dim):\n",
        "        c_i = c_orig[i, :]\n",
        "        sigma_i = sigma_orig[i, :]\n",
        "\n",
        "        # 按中心值排序\n",
        "        sorted_indices = np.argsort(c_i)\n",
        "        sorted_centers = c_i[sorted_indices]\n",
        "        sorted_sigma = sigma_i[sorted_indices]\n",
        "\n",
        "        if len(sorted_centers) < 2:\n",
        "            continue  # 需要至少两个规则才能计算 Ifspe\n",
        "\n",
        "        for l in range(len(sorted_centers) - 1):\n",
        "            v_l = sorted_centers[l]\n",
        "            v_lp1 = sorted_centers[l + 1]\n",
        "            s_l = sorted_sigma[l]\n",
        "            s_lp1 = sorted_sigma[l + 1]\n",
        "\n",
        "            phi_denominator = s_l + s_lp1 + 1e-8  # 防止除以零\n",
        "            phi = np.exp(-0.5 * ((v_l + v_lp1) / phi_denominator) ** 2)\n",
        "\n",
        "            denominator = s_l - s_lp1\n",
        "            if abs(denominator) < 1e-8:\n",
        "                psi = 0\n",
        "            else:\n",
        "                psi = np.exp(-0.5 * ((v_l + v_lp1) / denominator) ** 2)\n",
        "\n",
        "            # 使用绝对值确保 Ifspe_term 为非负数\n",
        "            ifspe_term = 2 * abs(0.5 - phi) + psi\n",
        "\n",
        "            total_ifspe += ifspe_term\n",
        "            valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0  # 避免除以零\n",
        "\n",
        "    average_ifspe = total_ifspe / (input_dim * n_rules)\n",
        "    return average_ifspe\n",
        "\n",
        "# 定义训练 SOFENN 模型的函数\n",
        "def train_sofenn(X_train_np, y_train_np, X_val_np, y_val_np, n_rules=3, epochs=500, batch_size=32, lr=0.01):\n",
        "    \"\"\"\n",
        "    训练 SOFENN 模型\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集输入数据，形状：(num_samples, n_inputs)\n",
        "    - y_train_np: 训练集目标数据，形状：(num_samples,)\n",
        "    - X_val_np: 验证集输入数据，形状：(num_samples, n_inputs)\n",
        "    - y_val_np: 验证集目标数据，形状：(num_samples,)\n",
        "    - n_rules: 规则数量\n",
        "    - epochs: 训练轮数\n",
        "    - batch_size: 批次大小\n",
        "    - lr: 学习率\n",
        "\n",
        "    返回：\n",
        "    - sofenn_model: 训练好的模型\n",
        "    - scaler_X: 输入数据的标准化器\n",
        "    - scaler_y: 输出数据的标准化器\n",
        "    \"\"\"\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)  # 使用相同的缩放器\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()  # 标准化输出并扁平化\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为 PyTorch 张量\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    y_train_scaled_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
        "    X_val_scaled_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "    y_val_scaled_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 初始化模型\n",
        "    input_dim = X_train_scaled_tensor.shape[1]\n",
        "    sofenn_model = SOFENN(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=1,\n",
        "        n_rules=n_rules\n",
        "    ).to(device)\n",
        "\n",
        "    # 初始化优化器\n",
        "    optimizer = optim.AdamW(sofenn_model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.9, patience=10, verbose=True\n",
        "    )\n",
        "\n",
        "    # 初始化列表，保存训练过程中的信息\n",
        "    training_info = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_rmse': []\n",
        "    }\n",
        "\n",
        "    # 定义损失函数\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 训练模型\n",
        "    for epoch in range(epochs):\n",
        "        sofenn_model.train()\n",
        "        # 采用批量训练\n",
        "        permutation = torch.randperm(X_train_scaled_tensor.size()[0])\n",
        "        epoch_loss = 0\n",
        "        for i in range(0, X_train_scaled_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_scaled_tensor[indices], y_train_scaled_tensor[indices]\n",
        "            optimizer.zero_grad()\n",
        "            output = sofenn_model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= (X_train_scaled_tensor.size()[0] // batch_size + 1)\n",
        "\n",
        "        sofenn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_val_pred_scaled = sofenn_model(X_val_scaled_tensor)\n",
        "            loss_val = criterion(y_val_pred_scaled, y_val_scaled_tensor)\n",
        "            # 反标准化预测值和真实值\n",
        "            y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            y_val_true = scaler_y.inverse_transform(y_val_scaled_tensor.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "        scheduler.step(loss_val)\n",
        "\n",
        "        # 保存训练信息\n",
        "        training_info['epoch'].append(epoch + 1)\n",
        "        training_info['train_loss'].append(epoch_loss)\n",
        "        training_info['val_loss'].append(loss_val.item())\n",
        "        training_info['val_rmse'].append(val_rmse)\n",
        "\n",
        "        # 显示验证集上的 RMSE\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {loss_val:.4f} - Val RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "    # 可视化训练过程\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_info['epoch'], training_info['train_loss'], label='Train Loss')\n",
        "    plt.plot(training_info['epoch'], training_info['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 可视化验证集上的 RMSE\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_info['epoch'], training_info['val_rmse'], label='Validation RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Validation RMSE over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # 返回模型和标准化器\n",
        "    return sofenn_model, scaler_X, scaler_y\n",
        "\n",
        "# 实验参数\n",
        "n_rules_list = [3, 5, 7, 9]  # 可以测试不同的规则数量\n",
        "learning_rates = [0.01]\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results_sofenn', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results_sofenn = []\n",
        "\n",
        "for n_rules in n_rules_list:\n",
        "    for lr in learning_rates:\n",
        "        test_rmse_list = []\n",
        "        time_list = []\n",
        "        iov_list = []\n",
        "        ifspe_list = []\n",
        "        print(f\"\\nStarting experiments for n_rules={n_rules}, learning_rate={lr}\")\n",
        "        for repeat in range(repeats):\n",
        "            start_time = time.time()\n",
        "            # 进一步将训练集拆分为训练和验证集\n",
        "            X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "                X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "            )\n",
        "\n",
        "            # 训练模型\n",
        "            sofenn_model, scaler_X, scaler_y = train_sofenn(\n",
        "                X_train_sub, y_train_sub, X_val_sub, y_val_sub,\n",
        "                n_rules=n_rules,\n",
        "                epochs=1500,\n",
        "                batch_size=512,\n",
        "                lr=lr\n",
        "            )\n",
        "\n",
        "            # 在测试集上测试模型\n",
        "            sofenn_model.eval()\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            X_test_scaled = scaler_X.transform(X_test_np)\n",
        "            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "            y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "            with torch.no_grad():\n",
        "                y_pred_scaled = sofenn_model(X_test_tensor)\n",
        "                y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                y_true = y_test_np  # Original unstandardized y_test\n",
        "                test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "                test_rmse_list.append(test_rmse)\n",
        "\n",
        "            # 记录时间\n",
        "            end_time = time.time()\n",
        "            time_taken = end_time - start_time\n",
        "            time_list.append(time_taken)\n",
        "\n",
        "            # 提取 c 和 sigma，计算 Iov 和 Ifspe\n",
        "            c = sofenn_model.fuzzy_layer.c.detach().cpu().numpy()  # (input_dim, n_rules)\n",
        "            sigma = sofenn_model.fuzzy_layer.sigma.detach().cpu().numpy()  # (input_dim, n_rules)\n",
        "            c_orig = c * scaler_X.scale_.reshape(-1, 1) + scaler_X.mean_.reshape(-1, 1)\n",
        "            sigma_orig = sigma * scaler_X.scale_.reshape(-1, 1)\n",
        "\n",
        "            # 计算 Iov 和 Ifspe\n",
        "            Iov = compute_iov(c_orig, sigma_orig)\n",
        "            Ifspe = compute_ifspe(c_orig, sigma_orig)\n",
        "\n",
        "            iov_list.append(Iov)\n",
        "            ifspe_list.append(Ifspe)\n",
        "\n",
        "            print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "            print(f\"Overlap Index (Iov): {Iov:.4f}, Fuzzy Set Position Index (Ifspe): {Ifspe:.4f}\")\n",
        "\n",
        "            # 提取模糊规则\n",
        "            rules = sofenn_model.extract_rules(scaler_X, scaler_y, feature_names=features_to_use)\n",
        "            # 打印模糊规则\n",
        "            print(f\"\\nFuzzy Rules for n_rules={n_rules}, learning_rate={lr}, Repeat={repeat+1}:\")\n",
        "            for rule in rules:\n",
        "                print(rule)\n",
        "                print()\n",
        "\n",
        "            # 保存模型\n",
        "            torch.save(sofenn_model.state_dict(), f'results_sofenn/sofenn_model_nrules{n_rules}_lr{lr}_repeat{repeat+1}.pth')\n",
        "\n",
        "        # 计算平均 RMSE 和时间\n",
        "        test_rmse_mean = np.mean(test_rmse_list)\n",
        "        test_rmse_std = np.std(test_rmse_list)\n",
        "        time_mean = np.mean(time_list)\n",
        "        time_std = np.std(time_list)\n",
        "\n",
        "        # 计算平均 Iov 和 Ifspe\n",
        "        Iov_mean = np.mean(iov_list)\n",
        "        Iov_std = np.std(iov_list)\n",
        "        Ifspe_mean = np.mean(ifspe_list)\n",
        "        Ifspe_std = np.std(ifspe_list)\n",
        "\n",
        "        # 打印结果\n",
        "        print(f\"\\nResults for n_rules={n_rules}, learning_rate={lr}:\")\n",
        "        print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "        print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "        print(f\"Average Overlap Index (Iov): {Iov_mean:.4f} ± {Iov_std:.4f}\")\n",
        "        print(f\"Average Fuzzy Set Position Index (Ifspe): {Ifspe_mean:.4f} ± {Ifspe_std:.4f}\")\n",
        "\n",
        "        # 保存结果\n",
        "        result = {\n",
        "            'n_rules': n_rules,\n",
        "            'learning_rate': lr,\n",
        "            'test_rmse_mean': test_rmse_mean,\n",
        "            'test_rmse_std': test_rmse_std,\n",
        "            'time_mean': time_mean,\n",
        "            'time_std': time_std,\n",
        "            'Iov_mean': Iov_mean,\n",
        "            'Iov_std': Iov_std,\n",
        "            'Ifspe_mean': Ifspe_mean,\n",
        "            'Ifspe_std': Ifspe_std\n",
        "        }\n",
        "        results_sofenn.append(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZiEZJyS8oSG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBaCMgeo81K0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12ZPTzby8onp"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import skfuzzy as fuzz\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import norm\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "!pip install ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# 假设您已经有 fetch_ucirepo 函数来获取数据集\n",
        "# 如果没有，可以直接从 UCI 数据库下载并加载数据集\n",
        "beijing_pm2_5 = fetch_ucirepo(id=381)\n",
        "\n",
        "# 数据（作为 pandas 数据帧）\n",
        "X = beijing_pm2_5.data.features\n",
        "y = beijing_pm2_5.data.targets\n",
        "\n",
        "# 选择需要的特征\n",
        "features_to_use = ['year', 'month', 'day', 'hour', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
        "\n",
        "# 处理目标变量\n",
        "X = X[features_to_use]\n",
        "y = y['pm2.5']\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "X = data[features_to_use]\n",
        "y = data['pm2.5']\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 定义 Neuro-Fuzzy RVFL 模型\n",
        "# ============================\n",
        "\n",
        "class NeuroFuzzyRVFL:\n",
        "    def __init__(self, input_dim, output_dim, NumFuzzyRule, NumHiddenNodes, activation_function, C, cluster_method='fcm'):\n",
        "        # 初始化参数\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.NumFuzzyRule = NumFuzzyRule\n",
        "        self.NumHiddenNodes = NumHiddenNodes\n",
        "        self.activation_function = activation_function\n",
        "        self.C = C\n",
        "        self.cluster_method = cluster_method\n",
        "        # 初始化 Alpha 和 WeightHidden\n",
        "        self.Alpha = np.random.rand(input_dim, NumFuzzyRule)\n",
        "        self.WeightHidden = np.random.rand(NumFuzzyRule + 1, NumHiddenNodes)\n",
        "        # 其他参数将在训练期间设置\n",
        "        self.center = None\n",
        "        self.std = 1  # 与 MATLAB 代码一致\n",
        "        self.beta = None  # 输出层权重\n",
        "\n",
        "    def activation(self, H):\n",
        "        if self.activation_function == 1:\n",
        "            # Sigmoid 函数\n",
        "            return 1 / (1 + np.exp(-H))\n",
        "        elif self.activation_function == 2:\n",
        "            return np.sin(H)\n",
        "        elif self.activation_function == 3:\n",
        "            # Tribas 函数\n",
        "            return np.maximum(1 - np.abs(H), 0)\n",
        "        elif self.activation_function == 4:\n",
        "            # Radbas 函数\n",
        "            return np.exp(-H ** 2)\n",
        "        elif self.activation_function == 5:\n",
        "            # Tansig 函数\n",
        "            return (2 / (1 + np.exp(-2 * H))) - 1\n",
        "        elif self.activation_function == 6:\n",
        "            # ReLU 函数\n",
        "            return np.maximum(0, H)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function\")\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        # 执行聚类以获得中心\n",
        "        if self.cluster_method == 'kmeans':\n",
        "            kmeans = KMeans(n_clusters=self.NumFuzzyRule, random_state=0).fit(train_x)\n",
        "            self.center = kmeans.cluster_centers_\n",
        "        elif self.cluster_method == 'fcm':\n",
        "            # 使用 skfuzzy 进行 FCM 聚类\n",
        "            cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
        "                train_x.T, self.NumFuzzyRule, 2, error=0.005, maxiter=1000, init=None)\n",
        "            self.center = cntr\n",
        "        else:\n",
        "            # 随机中心\n",
        "            indices = np.random.choice(train_x.shape[0], self.NumFuzzyRule, replace=False)\n",
        "            self.center = train_x[indices, :]\n",
        "\n",
        "        # 计算 MF 和 F\n",
        "        diff = train_x[:, np.newaxis, :] - self.center[np.newaxis, :, :]\n",
        "        exp_term = np.exp(-np.square(diff) / self.std)\n",
        "        MF = np.prod(exp_term, axis=2)\n",
        "        MF_sum = np.sum(MF, axis=1, keepdims=True)\n",
        "        MF = MF / MF_sum\n",
        "\n",
        "        train_x_Alpha = train_x @ self.Alpha\n",
        "        F = MF * train_x_Alpha\n",
        "\n",
        "        # 添加偏置项\n",
        "        F1 = np.hstack((F, 0.1 * np.ones((F.shape[0], 1))))\n",
        "        H = F1 @ self.WeightHidden\n",
        "        H = self.activation(H)\n",
        "        # 添加直接链接\n",
        "        H = np.hstack((H, train_x))\n",
        "        M = np.hstack((MF * (train_x @ self.Alpha), H))\n",
        "\n",
        "        # 计算 beta\n",
        "        if M.shape[1] < train_x.shape[0]:\n",
        "            self.beta = np.linalg.inv(M.T @ M + np.eye(M.shape[1]) / self.C) @ (M.T @ train_y)\n",
        "        else:\n",
        "            self.beta = M.T @ np.linalg.inv(np.eye(M.shape[0]) / self.C + M @ M.T) @ train_y\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        # 计算测试数据的 MF 和 F\n",
        "        diff = test_x[:, np.newaxis, :] - self.center[np.newaxis, :, :]\n",
        "        exp_term = np.exp(-np.square(diff) / self.std)\n",
        "        MF = np.prod(exp_term, axis=2)\n",
        "        MF_sum = np.sum(MF, axis=1, keepdims=True)\n",
        "        MF = MF / MF_sum\n",
        "\n",
        "        test_x_Alpha = test_x @ self.Alpha\n",
        "        F = MF * test_x_Alpha\n",
        "\n",
        "        F1 = np.hstack((F, 0.1 * np.ones((F.shape[0], 1))))\n",
        "        H = F1 @ self.WeightHidden\n",
        "        H = self.activation(H)\n",
        "        H = np.hstack((H, test_x))\n",
        "        M1 = np.hstack((MF * (test_x @ self.Alpha), H))\n",
        "\n",
        "        PredictedTestLabel = M1 @ self.beta\n",
        "        return PredictedTestLabel\n",
        "\n",
        "# ============================\n",
        "# 定义训练函数\n",
        "# ============================\n",
        "\n",
        "def train_nf_rvfl(X_train_np, y_train_np, X_val_np, y_val_np, X_test_np, y_test_np,\n",
        "                  NumFuzzyRule=15, NumHiddenNodes=203, activation_function=5, C=0.001,\n",
        "                  cluster_method='fcm'):\n",
        "    # 标准化数据\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "    y_test_scaled = scaler_y.transform(y_test_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 合并训练和验证数据进行训练\n",
        "    X_train_combined = np.vstack((X_train_scaled, X_val_scaled))\n",
        "    y_train_combined = np.hstack((y_train_scaled, y_val_scaled))\n",
        "\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    output_dim = 1\n",
        "\n",
        "    # 初始化并训练模型\n",
        "    nf_rvfl_model = NeuroFuzzyRVFL(input_dim=input_dim, output_dim=output_dim,\n",
        "                                   NumFuzzyRule=NumFuzzyRule, NumHiddenNodes=NumHiddenNodes,\n",
        "                                   activation_function=activation_function, C=C,\n",
        "                                   cluster_method=cluster_method)\n",
        "\n",
        "    start_time = time.time()\n",
        "    nf_rvfl_model.fit(X_train_combined, y_train_combined)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # 测试模型\n",
        "    start_time = time.time()\n",
        "    y_pred_scaled = nf_rvfl_model.predict(X_test_scaled)\n",
        "    testing_time = time.time() - start_time\n",
        "\n",
        "    # 反标准化预测值\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred))\n",
        "\n",
        "    return nf_rvfl_model, test_rmse, training_time + testing_time, scaler_X, scaler_y\n",
        "\n",
        "# ============================\n",
        "# 定义计算指标的函数\n",
        "# ============================\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    denominator = np.sqrt(sigma1 ** 2 + sigma2 ** 2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(model):\n",
        "    c = model.center  # (n_rules, input_dim)\n",
        "    sigma = model.std  # 标量\n",
        "    n_rules, input_dim = c.shape\n",
        "\n",
        "    total_max_overlap = 0\n",
        "    valid_attributes = 0\n",
        "\n",
        "    for attr in range(input_dim):\n",
        "        max_overlap = -np.inf\n",
        "        for i in range(n_rules):\n",
        "            for j in range(i + 1, n_rules):\n",
        "                c1 = c[i, attr]\n",
        "                c2 = c[j, attr]\n",
        "                sigma1 = sigma\n",
        "                sigma2 = sigma\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "        if max_overlap != -np.inf:\n",
        "            total_max_overlap += max_overlap\n",
        "            valid_attributes += 1\n",
        "\n",
        "    if valid_attributes == 0:\n",
        "        return 0\n",
        "\n",
        "    average_iov = total_max_overlap / valid_attributes\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(model):\n",
        "    c = model.center  # (n_rules, input_dim)\n",
        "    sigma = model.std  # 标量\n",
        "    n_rules, input_dim = c.shape\n",
        "\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for attr in range(input_dim):\n",
        "        # 获取该属性的中心\n",
        "        centers = c[:, attr]\n",
        "        # 排序中心\n",
        "        sorted_indices = np.argsort(centers)\n",
        "        sorted_centers = centers[sorted_indices]\n",
        "\n",
        "        for l in range(n_rules - 1):\n",
        "            v_l = sorted_centers[l]\n",
        "            v_lp1 = sorted_centers[l + 1]\n",
        "            s_l = sigma\n",
        "            s_lp1 = sigma\n",
        "\n",
        "            phi = np.exp(-0.5 * ((v_l + v_lp1) / (s_l + s_lp1)) ** 2)\n",
        "            denominator = s_l - s_lp1\n",
        "            if denominator == 0:\n",
        "                psi = 0\n",
        "            else:\n",
        "                psi = np.exp(-0.5 * ((v_l + v_lp1) / denominator) ** 2)\n",
        "\n",
        "            ifspe_term = 2 * abs(0.5 - phi) + psi\n",
        "\n",
        "            total_ifspe += ifspe_term\n",
        "            valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0\n",
        "\n",
        "    average_ifspe = total_ifspe / (n_rules * input_dim)\n",
        "    return average_ifspe\n",
        "\n",
        "# ============================\n",
        "# 实验设置与运行\n",
        "# ============================\n",
        "\n",
        "# 设置超参数\n",
        "NumFuzzyRule = [3,5,7,9]\n",
        "NumHiddenNodes = 203\n",
        "activation_function = 5  # Tansig\n",
        "C = 0.001\n",
        "cluster_method = 'fcm'\n",
        "repeats = 5\n",
        "for i in NumFuzzyRule:\n",
        "  test_rmse_list = []\n",
        "  time_list = []\n",
        "  results_nf_rvfl = []\n",
        "\n",
        "  for repeat in range(repeats):\n",
        "      start_time = time.time()\n",
        "\n",
        "      # 进一步将训练集拆分为训练和验证集\n",
        "      X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "          X_train_np, y_train_np, test_size=0.2, random_state=repeat)\n",
        "\n",
        "      nf_rvfl_model, test_rmse, time_taken, scaler_X, scaler_y = train_nf_rvfl(\n",
        "          X_train_sub, y_train_sub, X_val_sub, y_val_sub, X_test_np, y_test_np,\n",
        "          NumFuzzyRule=NumFuzzyRule, NumHiddenNodes=NumHiddenNodes,\n",
        "          activation_function=activation_function, C=C,\n",
        "          cluster_method=cluster_method)\n",
        "\n",
        "      test_rmse_list.append(test_rmse)\n",
        "      time_list.append(time_taken)\n",
        "\n",
        "      # 计算 Iov 和 Ifspe\n",
        "      average_iov = compute_iov(nf_rvfl_model)\n",
        "      average_ifspe = compute_ifspe(nf_rvfl_model)\n",
        "\n",
        "      print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s, Iov={average_iov:.4f}, Ifspe={average_ifspe:.4f}\")\n",
        "\n",
        "      # 保存结果\n",
        "      result = {\n",
        "          'repeat': repeat + 1,\n",
        "          'test_rmse': test_rmse,\n",
        "          'time_taken': time_taken,\n",
        "          'average_iov': average_iov,\n",
        "          'average_ifspe': average_ifspe\n",
        "      }\n",
        "      results_nf_rvfl.append(result)\n",
        "\n",
        "  # 计算平均值和标准差\n",
        "  test_rmse_mean = np.mean(test_rmse_list)\n",
        "  test_rmse_std = np.std(test_rmse_list)\n",
        "  time_mean = np.mean(time_list)\n",
        "  time_std = np.std(time_list)\n",
        "\n",
        "  average_iov_list = [res['average_iov'] for res in results_nf_rvfl]\n",
        "  average_ifspe_list = [res['average_ifspe'] for res in results_nf_rvfl]\n",
        "  average_iov_mean = np.mean(average_iov_list)\n",
        "  average_iov_std = np.std(average_iov_list)\n",
        "  average_ifspe_mean = np.mean(average_ifspe_list)\n",
        "  average_ifspe_std = np.std(average_ifspe_list)\n",
        "\n",
        "  # 打印结果\n",
        "  print(f\"\\nResults:\",'rule=',i)\n",
        "  print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "  print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "  print(f\"Average Overlap Index (Iov): {average_iov_mean:.4f} ± {average_iov_std:.4f}\")\n",
        "  print(f\"Average Fuzzy Set Position Index (Ifspe): {average_ifspe_mean:.4f} ± {average_ifspe_std:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOMi-iWz8K-0"
      },
      "source": [
        "# RVFL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta7oxZSpCtDf"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llcj_usy-eeW"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 导入必要的库\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import skfuzzy as fuzz\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import norm\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.datasets import fetch_openml\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "# 安装 ucimlrepo 库（仅在第一次运行时需要）\n",
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "except ImportError:\n",
        "    !pip install ucimlrepo\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# 从 UCI ML Repo 导入数据集\n",
        "# 注意：请确保安装了 `ucimlrepo` 包，如果没有，请使用以下命令安装：\n",
        "!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "# 从 UCI ML Repo 下载 Appliances Energy Prediction 数据集\n",
        "\n",
        "# 获取数据集\n",
        "\n",
        "\n",
        "# 获取波士顿房价数据集\n",
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "\n",
        "# 数据（作为 pandas 数据帧）\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 选择需要的特征\n",
        "features_to_use = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
        "                  'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
        "                  'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "# 处理目标变量\n",
        "y = y.astype(float)  # 确保目标变量为浮点数\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本\n",
        "data = pd.concat([X[features_to_use], y.rename('MEDV')], axis=1).dropna()\n",
        "X = data[features_to_use].astype(float).values  # 确保所有特征为浮点数\n",
        "y = data['MEDV'].astype(float).values  # 确保目标变量为浮点数\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# ============================\n",
        "# 定义 Neuro-Fuzzy RVFL 模型\n",
        "# ============================\n",
        "\n",
        "class NeuroFuzzyRVFL:\n",
        "    def __init__(self, input_dim, output_dim, NumFuzzyRule, NumHiddenNodes, activation_function, C, cluster_method='fcm'):\n",
        "        # 初始化参数\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.NumFuzzyRule = NumFuzzyRule\n",
        "        self.NumHiddenNodes = NumHiddenNodes\n",
        "        self.activation_function = activation_function\n",
        "        self.C = C\n",
        "        self.cluster_method = cluster_method\n",
        "        # 初始化 Alpha 和 WeightHidden\n",
        "        self.Alpha = np.random.rand(input_dim, NumFuzzyRule)\n",
        "        self.WeightHidden = np.random.rand(NumFuzzyRule + 1, NumHiddenNodes)\n",
        "        # 其他参数将在训练期间设置\n",
        "        self.center = None\n",
        "        self.std = 1  # 与 MATLAB 代码一致\n",
        "        self.beta = None  # 输出层权重\n",
        "\n",
        "    def activation(self, H):\n",
        "        # 限制 H 的范围，防止数值溢出\n",
        "        H = np.clip(H, -500, 500)\n",
        "        if self.activation_function == 1:\n",
        "            # Sigmoid 函数\n",
        "            return 1 / (1 + np.exp(-H))\n",
        "        elif self.activation_function == 2:\n",
        "            return np.sin(H)\n",
        "        elif self.activation_function == 3:\n",
        "            # Tribas 函数\n",
        "            return np.maximum(1 - np.abs(H), 0)\n",
        "        elif self.activation_function == 4:\n",
        "            # Radbas 函数\n",
        "            return np.exp(-H ** 2)\n",
        "        elif self.activation_function == 5:\n",
        "            # Tansig 函数\n",
        "            return (2 / (1 + np.exp(-2 * H))) - 1\n",
        "        elif self.activation_function == 6:\n",
        "            # ReLU 函数\n",
        "            return np.maximum(0, H)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function\")\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        # 执行聚类以获得中心\n",
        "        if self.cluster_method == 'kmeans':\n",
        "            kmeans = KMeans(n_clusters=self.NumFuzzyRule, random_state=0).fit(train_x)\n",
        "            self.center = kmeans.cluster_centers_\n",
        "        elif self.cluster_method == 'fcm':\n",
        "            # 使用 skfuzzy 进行 FCM 聚类\n",
        "            cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
        "                train_x.T, self.NumFuzzyRule, 2, error=0.005, maxiter=1000, init=None)\n",
        "            self.center = cntr\n",
        "        else:\n",
        "            # 随机中心\n",
        "            indices = np.random.choice(train_x.shape[0], self.NumFuzzyRule, replace=False)\n",
        "            self.center = train_x[indices, :]\n",
        "\n",
        "        # 计算 MF 和 F\n",
        "        diff = train_x[:, np.newaxis, :] - self.center[np.newaxis, :, :]\n",
        "        exp_term = np.exp(-np.square(diff) / self.std)\n",
        "        MF = np.prod(exp_term, axis=2)\n",
        "        MF_sum = np.sum(MF, axis=1, keepdims=True) + 1e-10  # 防止除以零\n",
        "        MF = MF / MF_sum\n",
        "\n",
        "        train_x_Alpha = train_x @ self.Alpha\n",
        "        F = MF * train_x_Alpha\n",
        "\n",
        "        # 添加偏置项\n",
        "        F1 = np.hstack((F, 0.1 * np.ones((F.shape[0], 1))))\n",
        "        H = F1 @ self.WeightHidden\n",
        "        H = self.activation(H)\n",
        "        # 添加直接链接\n",
        "        H = np.hstack((H, train_x))\n",
        "        M = np.hstack((MF * (train_x @ self.Alpha), H))\n",
        "\n",
        "        # 计算 beta 使用伪逆\n",
        "        self.beta = np.linalg.pinv(M) @ train_y\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        # 计算测试数据的 MF 和 F\n",
        "        diff = test_x[:, np.newaxis, :] - self.center[np.newaxis, :, :]\n",
        "        exp_term = np.exp(-np.square(diff) / self.std)\n",
        "        MF = np.prod(exp_term, axis=2)\n",
        "        MF_sum = np.sum(MF, axis=1, keepdims=True) + 1e-10  # 防止除以零\n",
        "        MF = MF / MF_sum\n",
        "\n",
        "        test_x_Alpha = test_x @ self.Alpha\n",
        "        F = MF * test_x_Alpha\n",
        "\n",
        "        F1 = np.hstack((F, 0.1 * np.ones((F.shape[0], 1))))\n",
        "        H = F1 @ self.WeightHidden\n",
        "        H = self.activation(H)\n",
        "        H = np.hstack((H, test_x))\n",
        "        M1 = np.hstack((MF * (test_x @ self.Alpha), H))\n",
        "\n",
        "        PredictedTestLabel = M1 @ self.beta\n",
        "\n",
        "        # 检查是否有 NaN 并处理\n",
        "        if np.isnan(PredictedTestLabel).any():\n",
        "            print(\"Warning: Predicted labels contain NaN. Replacing NaN with zero.\")\n",
        "            PredictedTestLabel = np.nan_to_num(PredictedTestLabel)\n",
        "\n",
        "        return PredictedTestLabel\n",
        "\n",
        "# ============================\n",
        "# 定义训练函数\n",
        "# ============================\n",
        "\n",
        "def train_nf_rvfl(X_train_np, y_train_np, X_val_np, y_val_np, X_test_np, y_test_np,\n",
        "                  NumFuzzyRule=15, NumHiddenNodes=203, activation_function=5, C=0.001,\n",
        "                  cluster_method='fcm'):\n",
        "    # 标准化数据\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled = scaler_X.transform(X_val_np)\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()\n",
        "    y_val_scaled = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "    y_test_scaled = scaler_y.transform(y_test_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 合并训练和验证数据进行训练\n",
        "    X_train_combined = np.vstack((X_train_scaled, X_val_scaled))\n",
        "    y_train_combined = np.hstack((y_train_scaled, y_val_scaled))\n",
        "\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    output_dim = 1\n",
        "\n",
        "    # 初始化并训练模型\n",
        "    nf_rvfl_model = NeuroFuzzyRVFL(input_dim=input_dim, output_dim=output_dim,\n",
        "                                   NumFuzzyRule=NumFuzzyRule, NumHiddenNodes=NumHiddenNodes,\n",
        "                                   activation_function=activation_function, C=C,\n",
        "                                   cluster_method=cluster_method)\n",
        "\n",
        "    start_time = time.time()\n",
        "    nf_rvfl_model.fit(X_train_combined, y_train_combined)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # 测试模型\n",
        "    start_time = time.time()\n",
        "    y_pred_scaled = nf_rvfl_model.predict(X_test_scaled)\n",
        "    testing_time = time.time() - start_time\n",
        "\n",
        "    # 反标准化预测值\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 检查 y_pred 是否包含 NaN\n",
        "    if np.isnan(y_pred).any():\n",
        "        print(\"Error: Predicted values contain NaN. Please check the model implementation.\")\n",
        "        # 这里我们可以选择替换 NaN，或者引发异常\n",
        "        y_pred = np.nan_to_num(y_pred)\n",
        "\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred))\n",
        "\n",
        "    return nf_rvfl_model, test_rmse, training_time + testing_time, scaler_X, scaler_y\n",
        "\n",
        "# ============================\n",
        "# 定义计算指标的函数\n",
        "# ============================\n",
        "\n",
        "def compute_overlap_analytic(c1, sigma1, c2, sigma2):\n",
        "    denominator = np.sqrt(sigma1 ** 2 + sigma2 ** 2)\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    d = np.abs(c1 - c2) / denominator\n",
        "    overlap_area = 2 * norm.cdf(-d)\n",
        "    return overlap_area\n",
        "\n",
        "def compute_iov(model):\n",
        "    c = model.center  # (n_rules, input_dim)\n",
        "    sigma = model.std  # 标量\n",
        "    n_rules, input_dim = c.shape\n",
        "\n",
        "    total_max_overlap = 0\n",
        "    valid_attributes = 0\n",
        "\n",
        "    for attr in range(input_dim):\n",
        "        max_overlap = -np.inf\n",
        "        for i in range(n_rules):\n",
        "            for j in range(i + 1, n_rules):\n",
        "                c1 = c[i, attr]\n",
        "                c2 = c[j, attr]\n",
        "                sigma1 = sigma\n",
        "                sigma2 = sigma\n",
        "                overlap = compute_overlap_analytic(c1, sigma1, c2, sigma2)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "        if max_overlap != -np.inf:\n",
        "            total_max_overlap += max_overlap\n",
        "            valid_attributes += 1\n",
        "\n",
        "    if valid_attributes == 0:\n",
        "        return 0\n",
        "\n",
        "    average_iov = total_max_overlap / valid_attributes\n",
        "    return average_iov\n",
        "\n",
        "def compute_ifspe(model):\n",
        "    c = model.center  # (n_rules, input_dim)\n",
        "    sigma = model.std  # 标量\n",
        "    n_rules, input_dim = c.shape\n",
        "\n",
        "    total_ifspe = 0\n",
        "    valid_terms = 0\n",
        "\n",
        "    for attr in range(input_dim):\n",
        "        # 获取该属性的中心\n",
        "        centers = c[:, attr]\n",
        "        # 排序中心\n",
        "        sorted_indices = np.argsort(centers)\n",
        "        sorted_centers = centers[sorted_indices]\n",
        "\n",
        "        for l in range(n_rules - 1):\n",
        "            v_l = sorted_centers[l]\n",
        "            v_lp1 = sorted_centers[l + 1]\n",
        "            s_l = sigma\n",
        "            s_lp1 = sigma\n",
        "\n",
        "            phi = np.exp(-0.5 * ((v_l + v_lp1) / (s_l + s_lp1)) ** 2)\n",
        "            denominator = s_l - s_lp1\n",
        "            if denominator == 0:\n",
        "                psi = 0\n",
        "            else:\n",
        "                psi = np.exp(-0.5 * ((v_l + v_lp1) / denominator) ** 2)\n",
        "\n",
        "            ifspe_term = 2 * abs(0.5 - phi) + psi\n",
        "\n",
        "            total_ifspe += ifspe_term\n",
        "            valid_terms += 1\n",
        "\n",
        "    if valid_terms == 0:\n",
        "        return 0\n",
        "\n",
        "    average_ifspe = total_ifspe / (n_rules * input_dim)\n",
        "    return average_ifspe\n",
        "\n",
        "# ============================\n",
        "# 实验设置与运行\n",
        "# ============================\n",
        "\n",
        "# 设置超参数\n",
        "NumFuzzyRules = [3,5,7,9]  # 推荐使用与原始 MATLAB 代码一致的值\n",
        "NumHiddenNodes = 21\n",
        "activation_function =  5 # Tansig\n",
        "C = 0.001\n",
        "cluster_method = 'fcm'\n",
        "repeats = 5\n",
        "for NumFuzzyRule in NumFuzzyRules:\n",
        "  test_rmse_list = []\n",
        "  time_list = []\n",
        "  results_nf_rvfl = []\n",
        "\n",
        "  for repeat in range(repeats):\n",
        "      start_time = time.time()\n",
        "\n",
        "      # 进一步将训练集拆分为训练和验证集\n",
        "      X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "          X_train_np, y_train_np, test_size=0.2, random_state=repeat)\n",
        "\n",
        "      nf_rvfl_model, test_rmse, time_taken, scaler_X, scaler_y = train_nf_rvfl(\n",
        "          X_train_sub, y_train_sub, X_val_sub, y_val_sub, X_test_np, y_test_np,\n",
        "          NumFuzzyRule=NumFuzzyRule, NumHiddenNodes=NumHiddenNodes,\n",
        "          activation_function=activation_function, C=C,\n",
        "          cluster_method=cluster_method)\n",
        "\n",
        "      test_rmse_list.append(test_rmse)\n",
        "      time_list.append(time_taken)\n",
        "\n",
        "      # 计算 Iov 和 Ifspe\n",
        "      average_iov = compute_iov(nf_rvfl_model)\n",
        "      average_ifspe = compute_ifspe(nf_rvfl_model)\n",
        "\n",
        "      print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s, Iov={average_iov:.4f}, Ifspe={average_ifspe:.4f}\")\n",
        "\n",
        "      # 保存结果\n",
        "      result = {\n",
        "          'repeat': repeat + 1,\n",
        "          'test_rmse': test_rmse,\n",
        "          'time_taken': time_taken,\n",
        "          'average_iov': average_iov,\n",
        "          'average_ifspe': average_ifspe\n",
        "      }\n",
        "      results_nf_rvfl.append(result)\n",
        "\n",
        "  # 计算平均值和标准差\n",
        "  test_rmse_mean = np.mean(test_rmse_list)\n",
        "  test_rmse_std = np.std(test_rmse_list)\n",
        "  time_mean = np.mean(time_list)\n",
        "  time_std = np.std(time_list)\n",
        "\n",
        "  average_iov_list = [res['average_iov'] for res in results_nf_rvfl]\n",
        "  average_ifspe_list = [res['average_ifspe'] for res in results_nf_rvfl]\n",
        "  average_iov_mean = np.mean(average_iov_list)\n",
        "  average_iov_std = np.std(average_iov_list)\n",
        "  average_ifspe_mean = np.mean(average_ifspe_list)\n",
        "  average_ifspe_std = np.std(average_ifspe_list)\n",
        "\n",
        "  # 打印结果\n",
        "  print(f\"\\nResults:Rule=\",NumFuzzyRule)\n",
        "  print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "  print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "  print(f\"Average Overlap Index (Iov): {average_iov_mean:.4f} ± {average_iov_std:.4f}\")\n",
        "  print(f\"Average Fuzzy Set Position Index (Ifspe): {average_ifspe_mean:.4f} ± {average_ifspe_std:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjC6qD1JAiB5"
      },
      "source": [
        "# ANFIS-PSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIh7B935Aia3"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "# !pip install ucimlrepo\n",
        "# 从 UCI ML Repo 导入数据集\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.datasets import fetch_openml\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================\n",
        "# 数据加载与预处理\n",
        "# ============================\n",
        "# 安装 ucimlrepo 库（仅在第一次运行时需要）\n",
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "except ImportError:\n",
        "    !pip install ucimlrepo\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "\n",
        "# 从 UCI ML Repo 导入 Auto MPG 数据集\n",
        "auto_mpg_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "\n",
        "# 定义列名称\n",
        "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "                'acceleration', 'model_year', 'origin', 'car_name']\n",
        "\n",
        "# 读取数据集，处理缺失值\n",
        "data = pd.read_csv(auto_mpg_url, delim_whitespace=True, names=column_names, na_values='?')\n",
        "\n",
        "# 删除含有缺失值的样本\n",
        "data = data.dropna()\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'mpg' 和 'car_name'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "    'acceleration', 'model_year', 'origin'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['mpg']\n",
        "\n",
        "# 将类别变量 'origin' 进行独热编码（如果需要，可以选择保留为数值型）\n",
        "# 这里保留为数值型，以简化 ANFIS 模型的处理\n",
        "# 如果希望进行独热编码，请取消下方代码的注释\n",
        "# X = pd.get_dummies(X, columns=['origin'], drop_first=True)\n",
        "# features_to_use = X.columns.tolist()\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本（已在读取时完成）\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "# feature_labels = features_to_use\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# 定义高斯隶属函数\n",
        "def gmf(x, c, sigma):\n",
        "    return torch.exp(-0.5 * ((x - c) / sigma) ** 2)\n",
        "\n",
        "# 定义 ANFIS 模型的前向传播\n",
        "def anfis_get_output(params, inputs, num_of_mf_terms):\n",
        "    num_of_inputs = inputs.shape[1]\n",
        "    num_of_rules = num_of_mf_terms ** num_of_inputs\n",
        "\n",
        "    # 输入变量\n",
        "    fis_input = []\n",
        "    weight_index = 0\n",
        "    for i in range(num_of_inputs):\n",
        "        input_dict = {}\n",
        "        input_dict['value'] = inputs[:, i]\n",
        "        input_dict['mf'] = []\n",
        "        for j in range(num_of_mf_terms):\n",
        "            c = params[weight_index]\n",
        "            sigma = params[weight_index + 1]\n",
        "            md = gmf(input_dict['value'], c, sigma)\n",
        "            input_dict['mf'].append({'params': [c, sigma], 'MD': md})\n",
        "            weight_index += 2\n",
        "        fis_input.append(input_dict)\n",
        "\n",
        "    # 生成规则列表\n",
        "    from itertools import product\n",
        "    mf_combinations = list(product(*[input_dict['mf'] for input_dict in fis_input]))\n",
        "    rule_list = []\n",
        "    for mf_tuple in mf_combinations:\n",
        "        rule = {'antecedent': [], 'prod': None, 'norm': None, 'consequent': None}\n",
        "        antecedent_mds = [mf['MD'] for mf in mf_tuple]\n",
        "        # 计算规则的前件部分的乘积\n",
        "        rule['antecedent'] = antecedent_mds\n",
        "        rule['prod'] = torch.prod(torch.stack(antecedent_mds), dim=0)\n",
        "        rule_list.append(rule)\n",
        "\n",
        "    # 计算所有规则的前件乘积之和\n",
        "    sum_of_all_rules = torch.sum(torch.stack([rule['prod'] for rule in rule_list]), dim=0) + 1e-8  # 防止除以零\n",
        "\n",
        "    # 归一化每个规则\n",
        "    for rule in rule_list:\n",
        "        rule['norm'] = rule['prod'] / sum_of_all_rules\n",
        "\n",
        "    # 计算规则的输出\n",
        "    outputs = torch.zeros(inputs.shape[0], device=device)\n",
        "    for rule in rule_list:\n",
        "        # 线性函数的参数\n",
        "        a_params = params[weight_index:weight_index + num_of_inputs + 1]\n",
        "        weight_index += num_of_inputs + 1\n",
        "        # 计算规则的后件部分\n",
        "        f = torch.matmul(torch.cat((inputs, torch.ones(inputs.shape[0], 1, device=device)), dim=1), a_params)\n",
        "        rule_output = rule['norm'] * f\n",
        "        outputs += rule_output\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# 初始化粒子群\n",
        "def init_swarm(n_particles, Lb, Ub):\n",
        "    return Lb + torch.rand(n_particles, len(Lb), device=device) * (Ub - Lb)\n",
        "\n",
        "# 初始化速度\n",
        "def init_velocity(n_particles, ndim, vel_clamping_factor, Ub):\n",
        "    v_max = Ub * vel_clamping_factor\n",
        "    v_min = -v_max\n",
        "    return v_min + (v_max - v_min) * torch.rand(n_particles, ndim, device=device)\n",
        "\n",
        "# 更新速度\n",
        "def pso_velocity(velocity, gbest, pbest, particles, w, social_const, cognitive_const, vel_clamping_factor, Ub):\n",
        "    n_particles, ndim = particles.shape\n",
        "    v_max = Ub * vel_clamping_factor\n",
        "    v_min = -v_max\n",
        "    r1 = torch.rand(n_particles, ndim, device=device)\n",
        "    r2 = torch.rand(n_particles, ndim, device=device)\n",
        "    velocity = (w * velocity +\n",
        "                cognitive_const * r1 * (pbest - particles) +\n",
        "                social_const * r2 * (gbest - particles))\n",
        "    # 速度限制\n",
        "    velocity = torch.clamp(velocity, v_min, v_max)\n",
        "    return velocity\n",
        "\n",
        "# 更新粒子位置\n",
        "def pso_move(particles, velocity, Lb, Ub):\n",
        "    particles = particles + velocity\n",
        "    particles = torch.clamp(particles, Lb, Ub)\n",
        "    return particles\n",
        "\n",
        "# PSO 训练函数\n",
        "def pso_train(X_train, y_train, X_val, y_val, total_param, num_of_mf_terms, max_iterations=20, err_tolerance=0.05):\n",
        "    itr = 1\n",
        "    # 使用float32的最大值替代1.0e+100\n",
        "    fbest = torch.finfo(torch.float32).max  # 全局最优值\n",
        "    total_particles = 15  # 粒子数量\n",
        "    Ub = torch.ones(total_param, device=device)\n",
        "    Lb = -torch.ones(total_param, device=device)\n",
        "\n",
        "    min_inertia_weight = 0.4\n",
        "    max_inertia_weight = 0.9\n",
        "    social_const = 2\n",
        "    cognitive_const = 2\n",
        "    vel_clamping_factor = 2\n",
        "    # 使用float32的最大值替代1.0e+100\n",
        "    pbestval = torch.full((total_particles,), torch.finfo(torch.float32).max, device=device)  # 个体最优值\n",
        "    pbest = torch.zeros((total_particles, total_param), device=device)  # 个体最优位置\n",
        "\n",
        "    particles = init_swarm(total_particles, Lb, Ub)\n",
        "    velocity = init_velocity(total_particles, total_param, vel_clamping_factor, Ub)\n",
        "\n",
        "    # 存储训练和验证损失\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    print(\"Starting PSO training...\")\n",
        "    while (fbest > err_tolerance) and (itr <= max_iterations):\n",
        "        for i in range(total_particles):\n",
        "            outputs = anfis_get_output(particles[i], X_train, num_of_mf_terms)\n",
        "            mse = torch.mean((y_train - outputs) ** 2).item()\n",
        "\n",
        "            # 更新个体最优\n",
        "            if mse <= pbestval[i].item():\n",
        "                pbestval[i] = torch.tensor(mse, device=device)\n",
        "                pbest[i] = particles[i].clone()\n",
        "\n",
        "            # 更新全局最优\n",
        "            if mse <= fbest:\n",
        "                fbest = mse\n",
        "                gbest = particles[i].clone()\n",
        "\n",
        "        # 更新速度和位置\n",
        "        w = ((max_iterations - itr) * (max_inertia_weight - min_inertia_weight)) / (max_iterations - 1) + min_inertia_weight\n",
        "        velocity = pso_velocity(velocity, gbest, pbest, particles, w, social_const, cognitive_const, vel_clamping_factor, Ub)\n",
        "        particles = pso_move(particles, velocity, Lb, Ub)\n",
        "\n",
        "        # 计算验证损失\n",
        "        outputs_val = anfis_get_output(gbest, X_val, num_of_mf_terms)\n",
        "        val_mse = torch.mean((y_val - outputs_val) ** 2).item()\n",
        "\n",
        "        # 存储损失\n",
        "        training_losses.append(fbest)\n",
        "        validation_losses.append(val_mse)\n",
        "\n",
        "        # 打印训练进度\n",
        "        print(f\"Iteration {itr}/{max_iterations} - Training MSE: {fbest:.6f} - Validation MSE: {val_mse:.6f}\")\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "    TrainingMSE = fbest\n",
        "    bestParams = gbest.cpu().numpy()\n",
        "    Iterations = itr - 1\n",
        "    return TrainingMSE, bestParams, Iterations, training_losses, validation_losses\n",
        "\n",
        "# 训练 ANFIS 模型\n",
        "def train_anfis_pso(X_train_np, y_train_np, X_val_np, y_val_np, num_of_mf_terms=2, max_iterations=20):\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled_np = scaler_X.fit_transform(X_train_np)\n",
        "    X_val_scaled_np = scaler_X.transform(X_val_np)\n",
        "    y_train_scaled_np = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()\n",
        "    y_val_scaled_np = scaler_y.transform(y_val_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 转换为PyTorch张量并移动到GPU\n",
        "    X_train_scaled = torch.tensor(X_train_scaled_np, dtype=torch.float32).to(device)\n",
        "    X_val_scaled = torch.tensor(X_val_scaled_np, dtype=torch.float32).to(device)\n",
        "    y_train_scaled = torch.tensor(y_train_scaled_np, dtype=torch.float32).to(device)\n",
        "    y_val_scaled = torch.tensor(y_val_scaled_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    num_of_inputs = X_train_scaled.shape[1]\n",
        "    num_of_rules = num_of_mf_terms ** num_of_inputs\n",
        "    total_param = num_of_inputs * num_of_mf_terms * 2 + num_of_rules * (num_of_inputs + 1)\n",
        "\n",
        "    # 训练模型\n",
        "    TrainingMSE, bestParams, Iterations, training_losses, validation_losses = pso_train(\n",
        "        X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled,\n",
        "        total_param, num_of_mf_terms, max_iterations\n",
        "    )\n",
        "\n",
        "    # 记录训练信息\n",
        "    training_info = {\n",
        "        'TrainingMSE': TrainingMSE,\n",
        "        'Iterations': Iterations,\n",
        "        'training_losses': training_losses,\n",
        "        'validation_losses': validation_losses\n",
        "    }\n",
        "\n",
        "    return TrainingMSE, bestParams, training_info, scaler_X, scaler_y\n",
        "\n",
        "# 实验参数\n",
        "num_of_mf_terms_list = [2]  # 与 SOFENN 实验中 n_rules_list 对应\n",
        "repeats = 5  # 每种配置重复次数\n",
        "max_iterations = 1500  # 最大迭代次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results_anfis_pso', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results_anfis_pso = []\n",
        "\n",
        "for num_of_mf_terms in num_of_mf_terms_list:\n",
        "    test_rmse_list = []\n",
        "    time_list = []\n",
        "    print(f\"\\nStarting experiments for num_of_mf_terms={num_of_mf_terms}\")\n",
        "    for repeat in range(repeats):\n",
        "        start_time = time.time()\n",
        "        # 进一步将训练集拆分为训练和验证集\n",
        "        X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "            X_train_np, y_train_np, test_size=0.2, random_state=repeat\n",
        "        )\n",
        "\n",
        "        # 训练模型\n",
        "        TrainingMSE, bestParams, training_info, scaler_X, scaler_y = train_anfis_pso(\n",
        "            X_train_sub, y_train_sub, X_val_sub, y_val_sub,\n",
        "            num_of_mf_terms=num_of_mf_terms,\n",
        "            max_iterations=max_iterations\n",
        "        )\n",
        "\n",
        "        # 在测试集上测试模型\n",
        "        X_test_scaled_np = scaler_X.transform(X_test_np)\n",
        "        X_test_scaled = torch.tensor(X_test_scaled_np, dtype=torch.float32).to(device)\n",
        "        # 转换bestParams为PyTorch张量并移动到GPU\n",
        "        bestParams_tensor = torch.tensor(bestParams, dtype=torch.float32).to(device)\n",
        "        outputs_test_scaled = anfis_get_output(bestParams_tensor, X_test_scaled, num_of_mf_terms)\n",
        "        y_test_pred_scaled = outputs_test_scaled.cpu().detach().numpy()\n",
        "        y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
        "        y_test_true = y_test_np  # 原始未标准化的测试集目标值\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
        "        test_rmse_list.append(test_rmse)\n",
        "\n",
        "        # 记录时间\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        time_list.append(time_taken)\n",
        "\n",
        "        print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "        # 保存模型参数\n",
        "        np.savez(f'results_anfis_pso/anfis_pso_params_nmf{num_of_mf_terms}_repeat{repeat+1}.npz', bestParams=bestParams)\n",
        "\n",
        "        # 可视化训练和验证损失\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(training_info['training_losses']) + 1), training_info['training_losses'], label='Training MSE')\n",
        "        plt.plot(range(1, len(training_info['validation_losses']) + 1), training_info['validation_losses'], label='Validation MSE')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.title(f'Training and Validation MSE (num_of_mf_terms={num_of_mf_terms}, Repeat={repeat+1})')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # 计算平均 RMSE 和时间\n",
        "    test_rmse_mean = np.mean(test_rmse_list)\n",
        "    test_rmse_std = np.std(test_rmse_list)\n",
        "    time_mean = np.mean(time_list)\n",
        "    time_std = np.std(time_list)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\nResults for num_of_mf_terms={num_of_mf_terms}:\")\n",
        "    print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "    print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "\n",
        "    # 保存结果\n",
        "    result = {\n",
        "        'num_of_mf_terms': num_of_mf_terms,\n",
        "        'test_rmse_mean': test_rmse_mean,\n",
        "        'test_rmse_std': test_rmse_std,\n",
        "        'time_mean': time_mean,\n",
        "        'time_std': time_std\n",
        "    }\n",
        "    results_anfis_pso.append(result)\n",
        "\n",
        "# 可视化测试集上的 RMSE 随隶属函数数量的变化\n",
        "num_of_mf_terms_values = [result['num_of_mf_terms'] for result in results_anfis_pso]\n",
        "test_rmse_means = [result['test_rmse_mean'] for result in results_anfis_pso]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(num_of_mf_terms_values, test_rmse_means, marker='o')\n",
        "plt.xlabel('Number of Membership Functions per Input')\n",
        "plt.ylabel('Test RMSE')\n",
        "plt.title('Test RMSE vs. Number of Membership Functions per Input')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MzyPQKhXrqy"
      },
      "source": [
        "# FuBiNFIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wzjBpP0XsDQ"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# 从 UCI ML Repo 导入数据集\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# 禁用不必要的警告\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 从 UCI ML Repo 导入 Auto MPG 数据集\n",
        "auto_mpg_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "\n",
        "# 定义列名称\n",
        "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "                'acceleration', 'model_year', 'origin', 'car_name']\n",
        "\n",
        "# 读取数据集，处理缺失值\n",
        "data = pd.read_csv(auto_mpg_url, delim_whitespace=True, names=column_names, na_values='?')\n",
        "\n",
        "# 删除含有缺失值的样本\n",
        "data = data.dropna()\n",
        "\n",
        "# 特征选择\n",
        "# 排除 'mpg' 和 'car_name'，将其余作为输入特征\n",
        "features_to_use = [\n",
        "    'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "    'acceleration', 'model_year', 'origin'\n",
        "]\n",
        "\n",
        "# 处理目标变量\n",
        "X = data[features_to_use]\n",
        "y = data['mpg']\n",
        "\n",
        "# 将类别变量 'origin' 进行独热编码（如果需要，可以选择保留为数值型）\n",
        "# 这里保留为数值型，以简化 ANFIS 模型的处理\n",
        "# 如果希望进行独热编码，请取消下方代码的注释\n",
        "# X = pd.get_dummies(X, columns=['origin'], drop_first=True)\n",
        "# features_to_use = X.columns.tolist()\n",
        "\n",
        "# 检查缺失值并删除含有缺失值的样本（已在读取时完成）\n",
        "\n",
        "# 将数据拆分为训练集和测试集\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X.values, y.values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 更新特征名称以便后续使用\n",
        "feature_labels = features_to_use\n",
        "# 定义 FuBiNFS 算法的实现\n",
        "def fubinfs(X_train_np, y_train_np, X_test_np, y_test_np, n_clusters=3, max_iter=100, tol=1e-5, m=2):\n",
        "    \"\"\"\n",
        "    实现 FuBiNFS 算法并在测试集上评估性能。\n",
        "\n",
        "    参数：\n",
        "    - X_train_np: 训练集输入数据，形状：(K, D)\n",
        "    - y_train_np: 训练集目标数据，形状：(K,)\n",
        "    - X_test_np: 测试集输入数据，形状：(N_test, D)\n",
        "    - y_test_np: 测试集目标数据，形状：(N_test,)\n",
        "    - n_clusters: 聚类数目 C\n",
        "    - max_iter: 最大迭代次数\n",
        "    - tol: 收敛阈值\n",
        "    - m: 模糊化系数\n",
        "\n",
        "    返回：\n",
        "    - y_pred_test: 测试集的预测输出\n",
        "    - test_rmse: 测试集上的 RMSE\n",
        "    - 其他中间结果\n",
        "    \"\"\"\n",
        "    K, D = X_train_np.shape\n",
        "    N_test = X_test_np.shape[0]\n",
        "\n",
        "    # 标准化输入和输出\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_np)\n",
        "    X_test_scaled = scaler_X.transform(X_test_np)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_np.reshape(-1, 1)).flatten()\n",
        "    y_test_scaled = scaler_y.transform(y_test_np.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 初始化 U\n",
        "    np.random.seed(42)\n",
        "    U = np.random.rand(n_clusters, K, D)\n",
        "    # 归一化 U，使其满足约束条件 (5)\n",
        "    U = U / U.sum(axis=0, keepdims=1)\n",
        "\n",
        "    # 初始化聚类中心 V^(k) 和 V^(d)\n",
        "    V_k = np.random.rand(n_clusters, D)\n",
        "    V_d = np.random.rand(n_clusters, K)\n",
        "\n",
        "    previous_J = np.inf\n",
        "    for iteration in range(max_iter):\n",
        "        # 保存上一轮的 U，用于检查收敛\n",
        "        U_old = U.copy()\n",
        "\n",
        "        # Step 1: 计算聚类中心 V^(k) among objects，公式 (3)\n",
        "        for c in range(n_clusters):\n",
        "            numerator = np.sum((U[c, :, :] ** m) * X_train_scaled, axis=0)  # 修正此处\n",
        "            denominator = np.sum(U[c, :, :] ** m, axis=0)  # 修正此处\n",
        "            V_k[c] = numerator / (denominator + 1e-8)\n",
        "\n",
        "        # Step 2: 计算聚类中心 V^(d) among attributes，公式 (4)\n",
        "        for c in range(n_clusters):\n",
        "            numerator = np.sum((U[c, :, :] ** m) * X_train_scaled, axis=1)  # 修正此处\n",
        "            denominator = np.sum(U[c, :, :] ** m, axis=1)  # 修正此处\n",
        "            V_d[c] = numerator / (denominator + 1e-8)\n",
        "\n",
        "        # Step 3: 更新隶属度矩阵 U，公式 (14)\n",
        "        for c in range(n_clusters):\n",
        "            # 计算距离矩阵\n",
        "            dist_c = (X_train_scaled - V_k[c]) ** 2 + (X_train_scaled - V_d[c][:, np.newaxis]) ** 2  # 形状 (K, D)\n",
        "            # 初始化分母\n",
        "            denom = np.zeros((K, D))\n",
        "            for cc in range(n_clusters):\n",
        "                dist_cc = (X_train_scaled - V_k[cc]) ** 2 + (X_train_scaled - V_d[cc][:, np.newaxis]) ** 2\n",
        "                denom += (dist_c / (dist_cc + 1e-8)) ** (1 / (m - 1))\n",
        "            U[c] = 1 / (denom + 1e-8)\n",
        "\n",
        "        # 归一化 U，使其满足约束条件 (5)\n",
        "        U = U / U.sum(axis=0, keepdims=1)\n",
        "\n",
        "        # Step 4: 计算目标函数 J，公式 (2)\n",
        "        J = 0\n",
        "        for c in range(n_clusters):\n",
        "            J += np.sum((U[c] ** m) * (\n",
        "                (X_train_scaled - V_k[c]) ** 2 + (X_train_scaled - V_d[c][:, np.newaxis]) ** 2\n",
        "            ))\n",
        "\n",
        "        # 检查收敛条件\n",
        "        if abs(J - previous_J) < tol:\n",
        "            print(f\"Converged at iteration {iteration + 1}\")\n",
        "            break\n",
        "        previous_J = J\n",
        "\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "    # Step 5: 生成模糊规则\n",
        "    # 使用高斯隶属函数，标准差 σ 可以设为聚类中心的标准差\n",
        "    sigma_k = np.std(V_k, axis=0) + 1e-8  # 防止为零\n",
        "\n",
        "    # Step 6: 对训练集进行模糊推理并进行后验训练以拟合 y_train_scaled\n",
        "    # 计算规则激活度（训练集）\n",
        "    activation_train = np.zeros((K, n_clusters))\n",
        "    for c in range(n_clusters):\n",
        "        # 计算隶属度\n",
        "        mu_k = np.exp(-0.5 * ((X_train_scaled - V_k[c]) ** 2) / (sigma_k ** 2))\n",
        "        activation_train[:, c] = np.prod(mu_k, axis=1)\n",
        "\n",
        "    # 归一化激活度\n",
        "    total_activation_train = activation_train.sum(axis=1, keepdims=True) + 1e-8\n",
        "    normalized_activation_train = activation_train / total_activation_train\n",
        "\n",
        "    # 使用归一化激活度作为特征，训练线性模型拟合 y_train_scaled\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(normalized_activation_train, y_train_scaled)\n",
        "\n",
        "    # Step 7: 对测试集进行模糊推理并预测输出\n",
        "    activation_test = np.zeros((N_test, n_clusters))\n",
        "    for c in range(n_clusters):\n",
        "        # 计算隶属度\n",
        "        mu_k = np.exp(-0.5 * ((X_test_scaled - V_k[c]) ** 2) / (sigma_k ** 2))\n",
        "        activation_test[:, c] = np.prod(mu_k, axis=1)\n",
        "\n",
        "    # 归一化激活度\n",
        "    total_activation_test = activation_test.sum(axis=1, keepdims=True) + 1e-8\n",
        "    normalized_activation_test = activation_test / total_activation_test\n",
        "\n",
        "    # 使用线性模型预测\n",
        "    y_pred_test_scaled = lin_reg.predict(normalized_activation_test)\n",
        "    # 反标准化\n",
        "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 计算测试集上的 RMSE\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_test))\n",
        "\n",
        "    # 返回结果\n",
        "    # 添加提取的模糊规则\n",
        "    rules = []\n",
        "    V_k_orig = V_k * scaler_X.scale_ + scaler_X.mean_\n",
        "    sigma_k_orig = sigma_k * scaler_X.scale_\n",
        "    for c in range(n_clusters):\n",
        "        antecedent = []\n",
        "        for d in range(len(features_to_use)):\n",
        "            c_val = V_k_orig[c, d]\n",
        "            sigma_val = sigma_k_orig[d]\n",
        "            antecedent.append(f\"{features_to_use[d]} is Gaussian(c={c_val:.4f}, σ={sigma_val:.4f})\")\n",
        "        antecedent_str = \" AND \".join(antecedent)\n",
        "        # 结论部分使用线性模型的系数\n",
        "        coef = lin_reg.coef_[c]\n",
        "        intercept = lin_reg.intercept_\n",
        "        consequent_str = f\"{coef:.4f} * Activation_{c+1} + {intercept:.4f}\"\n",
        "        rule = f\"Rule {c+1}: IF {antecedent_str} THEN Output = {consequent_str}\"\n",
        "        rules.append(rule)\n",
        "\n",
        "    return y_pred_test, test_rmse, {\n",
        "        'V_k': V_k,\n",
        "        'sigma_k': sigma_k,\n",
        "        'lin_reg': lin_reg,\n",
        "        'scaler_X': scaler_X,\n",
        "        'scaler_y': scaler_y,\n",
        "        'activation_test': activation_test,\n",
        "        'rules': rules  # 添加模糊规则到返回结果\n",
        "    }\n",
        "\n",
        "# 实验参数\n",
        "n_clusters_list = [3, 5, 7, 9]  # 可以测试不同的聚类数量\n",
        "max_iters = 100\n",
        "tol = 1e-5\n",
        "repeats = 5  # 每种配置重复次数\n",
        "\n",
        "# 创建结果保存的目录\n",
        "os.makedirs('results_fubinfs', exist_ok=True)\n",
        "\n",
        "# 记录实验结果\n",
        "results_fubinfs = []\n",
        "\n",
        "for n_clusters in n_clusters_list:\n",
        "    test_rmse_list = []\n",
        "    time_list = []\n",
        "    print(f\"\\nStarting experiments for n_clusters={n_clusters}\")\n",
        "    for repeat in range(repeats):\n",
        "        start_time = time.time()\n",
        "        # 调用 FuBiNFS 算法\n",
        "        y_pred_test, test_rmse, intermediate_results = fubinfs(\n",
        "            X_train_np, y_train_np, X_test_np, y_test_np,\n",
        "            n_clusters=n_clusters, max_iter=max_iters, tol=tol, m=2\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        time_list.append(time_taken)\n",
        "        test_rmse_list.append(test_rmse)\n",
        "        print(f\"Repeat {repeat+1}/{repeats}: Test RMSE={test_rmse:.4f}, Time={time_taken:.2f}s\")\n",
        "\n",
        "        # 提取模糊规则\n",
        "        rules = intermediate_results['rules']\n",
        "\n",
        "        # 打印模糊规则\n",
        "        print(f\"\\nFuzzy Rules for n_clusters={n_clusters}, Repeat={repeat+1}:\")\n",
        "        for rule in rules:\n",
        "            print(rule)\n",
        "            print()\n",
        "\n",
        "        # # 保存规则到文件\n",
        "        # with open(f'results_fubinfs/rules_nclusters{n_clusters}_repeat{repeat+1}.txt', 'w') as f:\n",
        "        #     for rule in rules:\n",
        "        #         f.write(rule + '\\n')\n",
        "\n",
        "    # 计算平均 RMSE 和时间\n",
        "    test_rmse_mean = np.mean(test_rmse_list)\n",
        "    test_rmse_std = np.std(test_rmse_list)\n",
        "    time_mean = np.mean(time_list)\n",
        "    time_std = np.std(time_list)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\nResults for n_clusters={n_clusters}:\")\n",
        "    print(f\"Test RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
        "    print(f\"Time: {time_mean:.2f}s ± {time_std:.2f}s\")\n",
        "\n",
        "    # 保存结果\n",
        "    result = {\n",
        "        'n_clusters': n_clusters,\n",
        "        'test_rmse_mean': test_rmse_mean,\n",
        "        'test_rmse_std': test_rmse_std,\n",
        "        'time_mean': time_mean,\n",
        "        'time_std': time_std\n",
        "    }\n",
        "    results_fubinfs.append(result)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}